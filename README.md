### üìÑ `README.md`

```markdown
# üîç Amplify Gen 2 RAG Assistant (Offline AI)

This project is a fully local **Retrieval-Augmented Generation (RAG)** system built to answer natural language questions about **AWS Amplify Gen 2** documentation. It uses **FAISS for vector search**, **sentence-transformers for embeddings**, and **Ollama** to run local large language models like **Mistral** or **Gemma** ‚Äî all completely offline.

---

## üöÄ Why I Built This

I recently started working on AWS Amplify Gen 2 and realized that most AI models don‚Äôt have updated knowledge about it.  
At the same time, I was curious about **local LLMs** and **RAG architecture** ‚Äî so I decided to combine the two and build this project from scratch.

---

## üß© Tech Stack

- **Python** ‚Äì FastAPI, Streamlit, BeautifulSoup
- **FAISS** ‚Äì Fast local vector search
- **Sentence Transformers** ‚Äì MiniLM-L6-v2 for embeddings
- **Ollama** ‚Äì Run local LLMs like `mistral` or `gemma`
- **No external API required** ‚Äì Fully private and free

---

## üì¶ Project Structure
```

amplify-gen-2-rag/
‚îú‚îÄ‚îÄ scraper/ # Scrapes Amplify Gen 2 docs
‚îú‚îÄ‚îÄ utils/ # Cleans and chunks scraped content
‚îú‚îÄ‚îÄ embeddings/ # Embeds and indexes chunks using FAISS
‚îú‚îÄ‚îÄ data/ # Stores all data (raw, chunks, FAISS)
‚îú‚îÄ‚îÄ api/ # FastAPI backend for querying
‚îú‚îÄ‚îÄ ui/ # Streamlit interface for user input
‚îú‚îÄ‚îÄ main.py # Orchestrates scraping, chunking, and embedding
‚îú‚îÄ‚îÄ requirements.txt # Python dependencies

````

---

## ‚öôÔ∏è Setup Instructions

### üñ•Ô∏è 1. Clone the Repository

```bash
git clone https://github.com/your-username/amplify-gen-2-rag.git
cd amplify-gen-2-rag
````

### üêç 2. Create a Virtual Environment

```bash
python -m venv .venv
# Activate venv
# On Windows:
.venv\Scripts\activate
# On macOS/Linux:
source .venv/bin/activate
```

### üì¶ 3. Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

---

## üõ†Ô∏è How to Run the Project

### üîé 1. Scrape and Process Docs

```bash
python main.py
```

This will:

- Scrape Amplify Gen 2 docs
- Clean + chunk the content
- Generate vector embeddings
- Store vectors in FAISS

### üöÄ 2. Start the API Server

```bash
uvicorn api.app:app --reload
```

Visit the Swagger UI at:  
üëâ [http://localhost:8000/docs](http://localhost:8000/docs)

Use the `/ask` endpoint to send questions.

### üåê 3. (Optional) Run the Streamlit UI

```bash
streamlit run ui/app.py
```

Visit: [http://localhost:8501](http://localhost:8501)

---

## ü§ñ Running the LLM with Ollama

### ‚úÖ Step 1: Install Ollama

- Download from: https://ollama.com/download
- Works on macOS, Windows (via native or WSL), and Linux

### ‚úÖ Step 2: Start Ollama

```bash
ollama serve
```

Then run your model (e.g. Mistral or Gemma):

```bash
ollama run mistral
```

> You can switch models by changing the `model=` name in `api/app.py`

---

## ‚ö†Ô∏è Known Limitations

- ‚ùó On low-spec systems, each query may take **5‚Äì6 minutes** due to large model size and no GPU acceleration.
- ‚ÑπÔ∏è Reduce context size or switch to smaller models like `gemma:2b` for better performance.

---

## üåü Future Improvements

- Add GitHub issue scraping
- Add query history saving
- Add model selector toggle in UI
- Optimize chunk length + token budget

---

## üì¨ Let's Connect

If you're working with local LLMs, self-hosted AI, or RAG systems ‚Äî feel free to reach out or fork the project!
