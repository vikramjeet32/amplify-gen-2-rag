[{"source": "data/raw_pages/react_build-a-backend_auth_manage-users_with-admin-actions.txt", "text": "With admin actions Amplify Auth can be managed with the AWS SDK's @aws-sdk/client-cognito-identity-provider package . This package is intended to use server-side, and can be used within a Function. This example focuses on the addUserToGroup action and will be defined as a custom mutation . To get started, create an \"ADMINS\" group that will be used to authorize the mutation: Next, create the Function resource: Then, in your auth resources, grant access for the function to perform the addUserToGroup action. Learn more about granting access to auth resources . You're now ready to define the custom mutation. Here you will use the newly-created addUserToGroup function resource to handle the addUserToGroup mutation. This mutation can only be called by a user in the \"ADMINS\" group. Lastly, create the function's handler using the exported client schema to type the handler function, and the generated env to specify the user pool ID you'd like to interact with: In your frontend, use the generated client to call your mutation using the group name and the user's ID."}, {"source": "data/raw_pages/react_ai_generation_data-extraction.txt", "text": "Data Extraction Data extraction allows you to parse unstructured text and extract structured data using AI. This is useful for converting free-form text into typed objects that can be used in your application. The following example shows how to extract product details from an unstructured product description. The AI model will analyze the text and return a structured object containing the product name, summary, price, and category."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_maps.txt", "text": "Work with maps First, ensure you've provisioned an Amazon Location Service Map resource and configured your app using the instructions in either Set up map or Use existing resources guide. To render a map, the MapLibre GL and the maplibre-gl-js-amplify libraries are required. MapLibre GL is an open source map rendering library and maplibre-gl-js-amplify library makes it easy to integrate MapLibre with Amplify Geo and handles Authentication. Add the dependencies to your app: Verify the following: maplibre-gl-js-amplify version 4.0.0 or above is installed Any package bundlers (webpack, rollup, etc) are configured to handle css files. Check out the webpack documentation here . Import the library into your application: Next, create and render the Map with the help of createMap . Note: There must be a div with an id=\"map\" on the DOM before making the call to createMap in this way. To render a map using a className or something other than the ID you can pass in a reference to the HTML Element itself. The MapLibre canvas requires a defined height to display properly, otherwise you may end up with a blank screen where the map is supposed to be. The amplify-map.css file has a few commonly used methods for setting the height of the map component. You can add some of the examples listed to your own styles or directly import amplify-map.css like so: To render a map using percentage based height you need to ensure that all ancestor elements to the map container have a height: To display markers on a map, use the drawPoints function. drawPoints expects: sourceName - specifies the layer on which the markers are rendered on. You can edit existing markers by passing the same sourceName coordinate data - (longitude, latitude) the coordinate data of the markers to be displayed a maplibre-gl-js Map - the map object on which to render the markers First, import the drawPoints method in your app. Your import section should include look like this The drawPoints method returns ids of the source and layers used to display the markers on the map. These ids can be used for further customization through maplibre-gl-js source , paint , and layer options. For more information about the parameters and options that can be used with drawPoints check the documentation here . Next, use the following code snippet when you want to display the markers on the map. Add it to the initializeMap() function if you want the markers to show up on map load. The getAvailableMaps API fetches information for all maps that are available to be displayed. This is useful if you would like to give your users a variety of maps styles to choose from. The available maps are returned as an array with the following contents: You can resize and customize a map with the resize and setStyle functions: When it's time to remove the map from the DOM, you can use the .remove method of the generated map. This will clean up and release all resources associated"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_maps.txt", "text": "with the map (DOM elements, event bindings, web workers, and WebGL resources). After calling .remove() , you must not call any other methods on the map. For React users: Not removing the map on component unmount can cause memory leaks in your application. It's recommended to call .remove() in either the return function of a React useEffect hook or the componentWillUnmount lifecycle hook of a class component. To display a map on your html website, add the following scripts to your html webpage. Next, add a div element with id map anywhere in your webpage where you want to render the map. Include the following code snippet to configure Amplify (update the amplify_outputs.json file path accordingly) and instantiate the map. If you want more information about the maps you currently have configured or want a way to switch between maps programmatically, the @aws-amplify/geo package provides API's that return more information about your currently provisioned maps. First, you need to import Geo from the @aws-amplify/geo package. getAvailableMaps will return the map resources you currently have provisioned in your Amplify project. You can switch between any of these different maps and display their different map styles. API Parameters Return The return from getAvailableMaps is a Promise that resolves to AmazonLocationServiceMapStyle[] which is an array of mapName , style , and region . Each object has the following properties: mapName - name of the map you created. style - the Amazon Location Service style used to create the map. region - the AWS region the map is hosted in. Note: When changing a map with Amplify and MapLibre the setStyle function should be called with the name of the Location Service map NOT the style. This is because the transformRequest function uses the Location Service map name to make a new request for map tile data. Example getDefaultMap is used to get a the default map object. API Parameters Return The return from getDefaultMap is a Promise that resolves to a AmazonLocationServiceMapStyle object. The object has the following properties: mapName - name of the map you created. style - the Amazon Location Service style used to create the map. region - the AWS region the map is hosted in. Example"}, {"source": "data/raw_pages/react_build-ui_formbuilder_lifecycle.txt", "text": "Manage form lifecycle Hook into the form's lifecycle events to customize user input before submission, run validations, or handle errors. Initial state - The inputs are either empty or pre-populated based on a default value provided by you. Use case: If your user clicks on the Clear or Reset button, they'll be brought back to this state. onChange - Event when form data is changed by the user. Use case: Use this to get the form data after every user input. onValidate - Event hook for custom validations. This event triggers after onChange . Use case: Use this to extend validation rules via code. onValidate also supports asynchronous validation rules, which enable you to validate the form input against external APIs. onSubmit - Event when your user clicks the Submit button. Use case: If your form is not connected to a data model , use set this event handler to retrieve the form data. If your form is connected to a data model , use this to customize the provided form data before they are saved to the cloud. onSuccess - Event when saving form data to the cloud succeeds. Use case: Use this to dismiss the form or reroute your user after a successful form submission. Only use this if your form is connected to a data model. onError - Event when saving form data to the cloud fails. Use case: Use this to log the error and investigate further if your validation rules need to be enhanced to catch input formatting issues. Only use this if your form is connected to a data model. onCancel - Event when your user clicks on the Cancel button. Use case: Use this to dismiss the form without saving the form data. In some cases, you want to get the form data in real-time as the user is filling the form. The onChange event provides you the form data in the fields parameter. With the onValidate event, you can extend the validation rules in code. Learn more about How to add validation rules . onSubmit should be your default way to handle form submission. It is triggered every time the user clicks on the Submit action button. You can use the onSubmit handler to customize the form data before they are saved to the cloud. The form data that's returned from the onSubmit handler will be saved to the cloud. For example, if you want to trim all the string data before saving it: You can use the onSuccess handler to take an action after the form data has been successfully submitted. The example below hides the form after it has been successfully submitted. You might encounter additional errors during the submit process. You can log these errors and present an alert to customers by using the onError handler. If the user clicks on the Cancel action button, you can use the onCancel event to hide the form or route the customer to another page."}, {"source": "data/raw_pages/react_build-a-backend_server-side-rendering.txt", "text": "Server-Side Rendering This guide walks through how to use Amplify Auth and Data APIs from Next.js server-side runtimes. Note: Amplify JS v6 supports Next.js with the version range: >=13.5.0 <16.0.0 . Ensure you have the correct version to integrate with Amplify. To use Amplify APIs server-side, you need to install the Amplify Next.js adapter in addition to the Amplify libraries: Configure Amplify for server-side usage Configure Amplify for client-side usage You will need to create a runWithAmplifyServerContext function to use Amplify APIs on the server-side of your Next.js app. You can create an amplifyServerUtils.ts file under a utils folder in your codebase. In this file, you will import the Amplify backend outputs from the amplify_outputs.json file that is generated by the Amplify CLI, and use the createServerRunner function to create the runWithAmplifyServerContext function. For example, the utils/amplifyServerUtils.ts file may contain the following content: You can use the exported runWithAmplifyServerContext function to call Amplify APIs within isolated request contexts. You can review examples under the Calling Amplify category APIs on the server side section. Tip: You only need to call the createServerRunner function once and reuse the runWithAmplifyServerContext function throughout. Tip : You only need do this step if you are using Amplify APIs on the client side of your Next.js app, for example, calling Amplify Auth signIn API to sign in a user, or use GraphQL subscriptions on the client side. When you use the Amplify library on the client-side of your Next.js app, you will need to configure Amplify by calling the Amplify.configure as you would to use Amplify in a single-page application. Note: To use the Amplify library on the client side in a Next.js app, you will need to set ssr to true when calling Amplify.configure . This instructs the Amplify library to store tokens in the cookie store of a browser. Cookies will be sent along with requests to your Next.js server for authentication. Make sure you call Amplify.configure as early as possible in your application\u00e2\u0080\u0099s life-cycle. A missing configuration or NoCredentials error is thrown if Amplify.configure has not been called before other Amplify JavaScript APIs. Review the Library Not Configured Troubleshooting guide for possible causes of this issue. To avoid repetitive calls to Amplify.configure , you can call it once in a top-level client-side rendered layout component. Configure Amplify in a Next.js App Router application If you're using the Next.js App Router, you can create a client component to configure Amplify and import it into your root layout. ConfigureAmplifyClientSide.ts : layout.tsx : Close accordion Warning: This feature is experimental and may change in future releases. Once you enable the server-side sign-in feature, auth tokens are stored in HttpOnly cookies and you may not change the HttpOnly attribute. Since these cookies are inaccessible from client-side scripts, you won\u00e2\u0080\u0099t be able to use any Amplify JS APIs on the client side. Therefore, you don\u00e2\u0080\u0099t need to configure Amplify on the client side. You can keep using these Amplify JS server-side APIs on the server side. Additional setup"}, {"source": "data/raw_pages/react_build-a-backend_server-side-rendering.txt", "text": "is required to enable server-side authentication flows in your Next.js app. Step 1 - Specify the origin of your app in environment variables Add the following environment variable to your Next.js app. For example in a .env file: Ensure this environment variable is accessible in your Next.js app's server runtime. Note: Token cookies are transmitted via server-side authentication flows. In production environments, it is recommended to use HTTPS as the origin for enhanced security. Step 2 - Export the createAuthRouteHandlers function The createAuthRouteHandlers function is created by the createServerRunner function call when you configure Amplify for server-side usage. You can export this function from your amplifyServerUtils.ts file. You can also configure cookie attributes with the runtimeOptions parameter. Step 3 - Set up the Auth API routes Create an API route using the createAuthRouteHandlers function. For example: With the above example, Amplify generates the following API routes: To customize the language of the Amazon Cognito Managed Login pages, you can add the lang query parameter to the /api/auth/sign-in and /api/auth/sign-up routes. For example, /api/auth/sign-in?lang=fr . Refer to the Managed login localization documentation for more information on the supported languages. Note: A signing-out call involves multiple steps, including signing out from Amazon Cognito Managed Login, revoking tokens, and removing cookies. If the user closes the browser during the process, the following may occur: auth token have not been revoked - user remains signed in auth token have been revoked but cookies have not been removed - cookies will be removed when the user visits the app again Step 4 - Provide the redirect URLs to the Auth Resource in Amplify You can provide the callback API routes as the redirect URLs in the Auth resource configuration. For example: This enables Amazon Cognito Hosted UI to support the server-side authentication flows. You may upgrade to the latest Amazon Cognito Managed Login Branding to customize the sign-in and sign-up pages. See Amazon Cognito user pool managed login for more information. Step 5 - Use Anchor link for initiating server-side authentication flows Use HTML anchor links to navigate users to the sign-in and sign-up routes. For example: Sign in button Sign in with Google button Sign up button Sign out button When an end user clicks on the buttons above, a corresponding server-side authentication flow will be initiated. You can use the fetchAuthSession API to check the auth sessions that are attached to the incoming requests in the middleware of your Next.js app to protect your routes. For example: In this example, if the incoming request is not associated with a valid user session the request will be redirected to the /sign-in route. Note: When calling fetchAuthSession with a response context, it will send the refreshed tokens (if any) back to the client via the Set-Cookie header in the response. For the Auth categories to use Amplify APIs on the server in your Next.js app, you will need to: Import the API from the /server sub path. Use the runWithAmplifyServerContext helper function created by calling"}, {"source": "data/raw_pages/react_build-a-backend_server-side-rendering.txt", "text": "the createServerRunner function exported from @aws-amplify/adapter-nextjs to call the Amplify API in an isolated server context. For the GraphQL API category, review Connect to data from Server-side Runtimes . Note: A subset of Amplify APIs can now be called on the server side of a Next.js app. These APIs are exported from the /server sub paths. See the full list of supported APIs. Note: If you use the Amplify server-side APIs in a server action and encounter the following error running next build : ./node_modules/@aws-amplify/core/node_modules/@aws-crypto/sha256-js/build/module/index.js + 12 modules Cannot get final name for export 'fromUtf8' of ./node_modules/@smithy/util-utf8/dist-es/index.js You can add the following to your next.config.js : See Next.js documentation on serverComponentsPackages for more details. Dynamic rendering in React server component Dynamic rendering is based on a user session extracted from an incoming request. Static rendering in React server component Static rendering does not require a user session, so you can specify the nextServerContext parameter as null . This is useful for some use cases; for example, when you are using the Storage API with guest access (if you have enabled it in your backend). Note: The URL returned by the getUrl API expires in the above example. You may want to specify the revalidate parameter to rerender the page as required to ensure the URL gets regenerated. In Route Handlers In route handlers require implementing an API route that enables GET /apis/get-current-user . When you call fetch('/apis/get-current-user') it returns a payload that contains the user data for the current signed-in user. In getServerSideProps The following example extracts current user data from the request and provides them to a page react component via its props. In getStaticProps Similar to static rendering with the App Router, you can pass null as the value of the nextServerContext parameter to use the Amplify Storage API with guest access. All APIs that support use on the server are exported from the aws-amplify/<category>/server sub paths. You must use these APIs for any server-side use cases. Have a server-side use case that isn't currently supported in Amplify JS? Consider using the AWS SDK for JavaScript ."}, {"source": "data/raw_pages/react_build-a-backend_storage_lambda-triggers.txt", "text": "Listen to storage events Function triggers can be configured to enable event-based workflows when files are uploaded or deleted. To add a function trigger, modify the defineStorage configuration. First, in your storage definition, add the following: Then create the function definitions at amplify/storage/on-upload-handler.ts and amplify/storage/on-delete-handler.ts . Note: The S3Handler type comes from the @types/aws-lambda npm package. This package contains types for different kinds of Lambda handlers, events, and responses. Now, when you deploy your backend, these functions will be invoked whenever an object is uploaded or deleted from the bucket. The example listed above demonstrates what is exposed directly in your storage definition. Specifically, the use of the triggers option when you use defineStorage . This method is for simple triggers that always execute on file uploads or file deletions. There are no additional modifications you can make to the triggers defined in this way. If you want the ability to do something more than simply handle the events onUpload and onDelete you will have to use .addEventNotification in your backend.ts . If you use this method, the triggers section in your storage/resource.ts file should be removed. Here is an example of how you can add a Lambda trigger for an S3 object PUT event. This trigger will execute when a file that has been uploaded to the bucket defined in your storage/resource.ts has a matching prefix and suffix as that listed in the function input of addEventNotification . It's important to note that using this methodology does not require any changes your lambda function. This modification on your backend.ts file will create a new AWS CloudFormation handler for \"Custom::S3BucketNotifications\" resources (@aws-cdk/aws-s3) that specifically handles checking the prefix and suffix."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_rest-api_delete-data.txt", "text": "Delete data To delete an item via the API endpoint: import { del } from 'aws-amplify/api' ; async function deleteItem ( ) { const restOperation = del ( { await restOperation . response ; console . log ( 'DELETE call succeeded' ) ; console . log ( 'DELETE call failed: ' , JSON . parse ( e . response . body ) ) ;"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_amazon-location-sdk.txt", "text": "Use Amazon Location Service SDK Amplify Geo provides solutions for common use cases with Amazon Location Service but for any functionality that is not currently supported by Amplify Geo you can access the Amazon Location Service SDK directly. Follow this guide to get started with the aws-sdk for Amazon Location Service using Amplify Auth credentials. In this tutorial, we\u00e2\u0080\u0099ll go over the following: Setting up the AWS SDK JavaScript v3 package for the Amazon Location Service SDK calls with Amplify auth. Code examples using the Amazon Location Service SDK. The first step to using the SDKs in the client is to install the necessary dependencies with the following command: In the following procedure, you\u00e2\u0080\u0099ll connect your app to the Amazon Location Service APIs. To connect your app to the Amazon Location Service In your React App, open src/App.js file, and call the following function to initialize the Amazon Location Service client: You\u00e2\u0080\u0099ve now successfully connected your app to the Amazon Location Service. In order to access Amazon Location Service APIs, ensure you've provisioned resources and configured your app using the instructions in either Amplify Geo Maps docs or the Amazon Location Service console . You can check out the Amazon Location API Reference documentation for a complete list of supported features. This example requires you to have first provisioned a Tracker resource using the Amazon Location Service console . The following code details how to use the Amazon Location Service APIs to update a device position and get a device position using the tracker you just created:"}, {"source": "data/raw_pages/react_build-a-backend_functions_examples_google-recaptcha-challenge.txt", "text": "Google reCAPTCHA challenge You can use defineAuth and defineFunction to create an auth experience that requires a reCAPTCHA v3 token. This can be accomplished by leveraging Amazon Cognito's feature to define a custom auth challenge and 3 triggers: Create auth challenge Define auth challenge Verify auth challenge response To get started, create the first of the three triggers, create-auth-challenge . This is the trigger responsible for creating the reCAPTCHA challenge after a password is verified. After creating the resource file, create the handler with the following contents: Next, you will want to create the trigger responsible for defining the auth challenge flow, define-auth-challenge . After creating the resource file, create the handler with the following contents: Lastly, create the trigger responsible for verifying the challenge response, which in this case is the reCAPTCHA token verification. If you have not done so already, you will need to register your application and retrieve a reCAPTCHA secret key. This can then be configured for use with your cloud sandbox using: After creating the resource file, create the handler with the following contents: Finally, import and set the three triggers on your auth resource:"}, {"source": "data/raw_pages/react_ai_conversation.txt", "text": "Conversation The conversation route simplifies the creation of AI-powered conversation interfaces in your application. It automatically sets up the necessary AppSync API components and Lambda functions to handle streaming multi-turn interactions with Amazon Bedrock foundation models. AppSync API : Gateway to the conversation route. Create new conversation route instance. Send messages to conversation route instance. Subscribe to real-time updates for assistant responses. Lambda Function : Bridge between AppSync and Amazon Bedrock. Retrieve conversation instance history. Invokes Bedrock's /converse endpoint. Handles tool use responses by invoking AppSync queries. DynamoDB : Stores conversation and message data Conversations are scoped to a specific application user. The user's OIDC access token is passed from the client to AppSync AppSync forwards this token to the Lambda function The Lambda function uses the token to authenticate requests back to AppSync Each of the following scenarios have safeguards in place to mitigate risks associated with invoking tools on behalf of the user, including: Amazon CloudWatch log group redacting OIDC access tokens for logs from the Lambda function. IAM policies that limit the Lambda function's ability to access other resources. User sends a message via the AppSync mutation AppSync triggers the Lambda function (default or custom) Lambda processes the message and invokes Bedrock's /converse endpoint a. If response is a tool use, Lambda function invokes applicable AppSync query. Lambda sends assistant response back to AppSync AppSync sends the response to subscribed clients This design allows for real-time, scalable conversations while ensuring that the Lambda function's data access matches that of the application user."}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_email.txt", "text": "Email By default Amplify Auth is scaffolded with email as the default method for user sign-in. This will configure an email attribute that is required for sign-up and cannot be changed."}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_using-the-authenticator.txt", "text": "Using the Authenticator The quickest way to get started with Amplify Auth in your frontend application is with the Authenticator component , which provides a customizable UI and complete authentication flows. The Authenticator component is automatically configured based on the outputs generated from your backend. To learn more about the Authenticator and how to customize its appearance, visit the Amplify UI documentation . Conversely, you can bring your own UI and leverage the library from aws-amplify to handle authentication flows manually."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_tagging-resources.txt", "text": "Tagging resources Tags are a key-value pair that are applied to AWS resources to hold metadata. Tags are often used to decorate resources with metadata that helps categorize resources for billing or viewing purposes. Learn more about tags by visiting the AWS documentation for best practices for tagging resources . Amplify applies the following tags by default: In your Amplify backend you can use the Tags class from the AWS Cloud Development Kit (CDK) to apply tags at the root level, which then cascades to child resources."}, {"source": "data/raw_pages/react_build-a-backend_data_customize-authz_public-data-access.txt", "text": "Public data access The public authorization strategy grants everyone access to the API, which is protected behind the scenes with an API key. You can also override the authorization provider to use an unauthenticated IAM role from Cognito instead of an API key for public access. To grant everyone access, use the .public() authorization strategy. Behind the scenes, the API will be protected with an API key. In your application, you can perform CRUD operations against the model using client.models.<model-name> by specifying the apiKey auth mode. import { generateClient } from 'aws-amplify/data' ; import type { Schema } from '../amplify/data/resource' ; const client = generateClient < Schema > ( ) ; const { errors , data : newTodo } = await client . models . Todo . create ( Copy highlighted code example If the API key has not expired, you can extend the expiration date by deploying your app again. The API key expiration date will be set to expiresInDays days from the date when the app is deployed. In the example below, the API key will expire 7 days from the latest deployment. You can rotate an API key if it was expired, compromised, or deleted. To rotate an API key, you can override the logical ID of the API key resource in the amplify/backend.ts file. This will create a new API key with a new logical ID. Deploy your app. After the deploy has finished, remove the override to the logical ID and deploy your app again to use the default logical ID. A new API key will be created for your app. You can also override the authorization provider. In the example below, identityPool is specified as the provider which allows you to use an \"Unauthenticated Role\" from the Cognito identity pool for public access instead of an API key. Your Auth resources defined in amplify/auth/resource.ts generates scoped down IAM policies for the \"Unauthenticated role\" in the Cognito identity pool automatically. In your application, you can perform CRUD operations against the model using client.models.<model-name> with the identityPool auth mode. If you're not using the auto-generated amplify_outputs.json file, then you must set the Amplify Library resource configuration's allowGuestAccess flag to true . This lets the Amplify Library use the unauthenticated role from your Cognito identity pool when your user isn't logged in. Close accordion"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging_clear-messages.txt", "text": "Clear messages Once messages have been synced to your user's device, clearMessages() can be used to clear the synced messages. Note: If your app has authentication implemented, we recommend calling clearMessages() in between user log-ins to remove messages targeted for specific user segments. This is especially important if you anticipate your application will be used in shared device scenarios."}, {"source": "data/raw_pages/android_start_quickstart.txt", "text": "Quickstart \u00f0\u009f\u0091\u008b Welcome to AWS Amplify! In this quickstart guide, you will: deploy an Amplify backend database and authentication connect to the backend from an Android app make backend updates To get started faster, we've created a starter \"To-do\" Amplify backend. First, create a repository in your GitHub account using our starter template. Use our starter template to create a repository in your GitHub account. This template scaffolds an Amplify Backend with Auth and Data capabilities. Create repository from template Now that the repository has been created, deploy this to Amplify's CI/CD pipeline. Deploy to AWS Select GitHub , pick the starter repository, and hit \"Save and Deploy\". While you are waiting for your app to deploy (~5 mins) Learn about the project structure Let's take a tour of the project structure in this starter repository. The starter application already has pre-written code to give you a real-time database with a feed of all to-do items and the ability to add new items. Close accordion When the build completes, visit the newly deployed branch by selecting the branch name and then looking at the Deployed backend resources section under deployments. Let's learn how to enhance the app functionality by creating a delete flow for to-do list items. If you do not have an existing Android app. Setup Android project Open Android Studio. Select + Create New Project. In Select a Project Template , select Empty Activity or Empty Compose Activity . Press Next . Enter MyAmplifyApp in the Name field Select either Java or Kotlin from the Language dropdown menu Select API 24: Android 7.0 (Nougat) from the Minimum SDK dropdown menu Press Finish Close accordion On the Deployed backend resources , choose Download outputs file to download the amplify_outputs.json file that contains identifiers for all the deployed backend resources. Now move the amplify_outputs.json file you downloaded above to app/src/main/res/raw in your Android project. You will now be able to connect to this backend. The amplify_outputs.json file contains backend endpoint information, publicly-viewable API keys, authentication flow information, and more. The Amplify client library uses this outputs file to connect to your Amplify Backend. Close accordion Amplify uses some modern Java APIs that require desugaring to be added for earlier versions of Android. In your app/build.gradle.kts add the following lines: The deployed backend application already has a pre-configured auth backend defined in the amplify/auth/resource.ts file. The fastest way to get your login experience up and running is to use our Authenticator UI component. To use the Authenticator UI component, you need to add the following dependencies to your app/build.gradle.kts file: Be sure to have compileSdk version as 34 or higher. Afterwards create a MyAmplifyApp class that extends Application and add the following code: Next call this class in your AndroidManifest.xml file: Update MainActivity.kt to use the Android Authenticator component. Now if you run the application on the Android emulator, you should see the authentication flow working. The initial scaffolding already has a pre-configured data backend defined in the amplify/data/resource.ts file. The"}, {"source": "data/raw_pages/android_start_quickstart.txt", "text": "default example will create a Todo model with content field. Amplify can automatically generate code for interacting with the backend API. The command below generates model classes from the Data schema: Find your App ID in the Amplify Console Looking for your App ID? In the command below, replace the APP-ID with your Amplify app ID. Find this in the Amplify Console. Close accordion Once you are done, add the following dependencies to your project: After adding the dependencies, open the MyAmplifyApp class and add the following line before the configure call: Update the MainActivity class with the following code to create new to-do items. The onClick function will create a new Todo item. Now it is time to add a logic to view the added items. Now call TodoList() from the onCreate() function: If you build and rerun the application, you should see the todo that was created in the previous build. But notice how when you click on the \"create Todo\" button, it doesn't add any new todos to the list below until the next time your app relaunches. To solve this, let's add real-time updates to the todo list. To add real-time updates, you can use the subscription feature of Amplify Data. It allows to subscribe to onCreate , onUpdate , and onDelete events of the application. In our example, let's append the list every time a new todo is added. Let's update our backend to implement per-user authorization rules, allowing each user to only access their own to-dos. First, clone the deployed repository. The backend to-do model is configured to share data across all users, but, in most cases, you want data to be isolated on a per-user basis. To isolate the data on a per-user basis, you can use an \"owner-based authorization rule\". Let's apply the owner-based authorization rule to your to-do items: Commit this change to your git repository. Amplify's CI/CD system will automatically pick up the changes and build and deploy the updates. Now, let's go back to your Android application and test out the user isolation of the to-do items. Fetch the latest amplify_outputs.json and model files by re-running the following command in your Android Studio terminal. Find your App ID in the Amplify Console Looking for your App ID? In the command below, replace the APP-ID with your Amplify app ID. Find this in the Amplify Console. Close accordion Also update your amplify_outputs file with the latest outputs information. That's it! You have successfully built a fullstack app on AWS Amplify. If you want to learn more about how to work with Amplify, here's the conceptual guide for how Amplify works ."}, {"source": "data/raw_pages/react_build-a-backend_data_data-modeling_identifiers.txt", "text": "Customize data model identifiers Identifiers are defined using the .identifier() method on a model definition. Usage of the .identifier() method is optional; when it's not present, the model will automatically have a field called id of type ID that is automatically generated unless manually specified. If you want, you can use Amplify Data to define single-field and composite identifiers: Single-field identifier with a consumer-provided value (type: id or string , and must be marked required ) Composite identifier with a set of consumer-provided values (type: id or string , and must be marked required ) If the default id identifier field needs to be customized, you can do so by passing the name of another field. For cases where items are uniquely identified by more than a single field, you can pass an array of the field names to the identifier() function:"}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_storage_remove-files.txt", "text": "Remove files Files can be removed from a storage bucket using the remove API. If a file is protected by an identity Id, only the user who owns the file will be able to remove it. You can also perform a remove operation from a specific bucket by providing the target bucket's assigned name from Amplify Backend in bucket option. Alternatively, you can also pass in an object by specifying the bucket name and region from the console."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_analytics_set-up-analytics.txt", "text": "Set up Amplify Analytics Amplify enables you to collect analytics data for your app. In order to use Analytics, you will enable Amazon Kinesis or Amazon Pinpoint using the AWS Cloud Development Kit (AWS CDK). The Analytics category uses Amazon Cognito identity pools to identify users in your app. Cognito allows you to receive data from authenticated, and unauthenticated users in your app. Use the AWS CDK to create an analytics resource powered by Amazon Pinpoint . First, install the aws-amplify library: Import and load the configuration file in your app. It's recommended you add the Amplify configuration step to your app's root entry point. Next Steps: Congratulations! Now that you have Analytics' backend provisioned and Analytics library installed. Check out the following links to see Amplify Analytics use cases: Amazon Pinpoint Construct Library"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-to-existing-data-sources_connect-external-ddb-table.txt", "text": "Connect to external Amazon DynamoDB data sources The a.model() data model allows you to define a GraphQL schema for an AWS AppSync API where models are backed by DynamoDB Tables managed by Amplify. The generated schema also provides queries and mutations to the Amplify Data client. However, you may want to connect to an external DynamoDB table and execute custom business logic against it instead. Using an external DynamoDB table as a data source may be useful if you need to leverage patterns such as single table design. In the following sections, we walk through the steps to add and use an external DynamoDB table as a data source for your API: Set up your Amazon DynamoDB table Add your Amazon DynamoDB table as a data source Define custom queries and mutations Configure custom business logic handler code Invoke custom queries or mutations For the purpose of this guide we will define a Post type and create an external DynamoDB table that will store records for it. In Amplify Gen 2, customType adds a type to the schema that is not backed by an Amplify-generated DynamoDB table. With the Post type defined, it can then be referenced as the return type when defining your custom queries and mutations. First, add the Post custom type to your schema: NOTE: To comply with the GraphQL spec, at least one query is required for a schema to be valid. Otherwise, deployments will fail with a schema error. The Amplify Data schema is auto-generated with a Todo model and corresponding queries under the hood. You can leave the Todo model in the schema until you add the first custom query to the schema in the next steps. Once the deployment successfully completes, navigate to the AppSync console and select your Amplify-generated API. Follow these steps to create a new DynamoDB table: On the Schema page, choose Create Resources . Choose Use existing type , then choose the Post type. Set the Primary key to id and the Sort key to None . Disable Automatically generate GraphQL . In this example, we'll create the resolver ourselves. Choose Create . You now have a new DynamoDB table named PostTable , which you can see by visiting Data sources in the side tab. You will use this table as the data source for your custom queries and mutations to your Amazon DynamoDB table. In your amplify/backend.ts file, add your DynamoDB table as a data source for your API: Now that your DynamoDB table has been added as a data source, you can reference it in custom queries and mutations using the a.handler.custom() modifier which accepts the name of the data source and an entry point for your resolvers. Use the following code examples to add addPost , getPost , updatePost , and deletePost as custom queries and mutations to your schema: addPost getPost updatePost deletePost Next, create the following files in your amplify/data folder and use the code examples to define custom resolvers for the custom queries"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-to-existing-data-sources_connect-external-ddb-table.txt", "text": "and mutations added to your schema from the previous step. These are AppSync JavaScript resolvers addPost getPost updatePost deletePost From your generated Data client, you can find all your custom queries and mutations under the client.queries. and client.mutations. APIs respectively. addPost getPost updatePost deletePost In this guide, you\u00e2\u0080\u0099ve added an external DynamoDB table as a data source to an Amplify GraphQL API and defined custom queries and mutations, handled by AppSync JS resolvers, to manipulate Post items in an external DynamoDB table using the Amplify Gen 2 Data client. To clean up, you can delete your sandbox by accepting the prompt when terminating the sandbox process in your terminal. Alternatively, you can also use the AWS Amplify console to manage and delete sandbox environments. To delete your external DynamoDB table, you can navigate to the AppSync console and click on the name of the table in the data sources list. This takes you to the DynamoDB console where you can delete the table. Reference - The GetItem request lets you tell the AWS AppSync DynamoDB function to make a GetItem request to DynamoDB, and enables you to specify: The key of the item in DynamoDB Whether to use a consistent read or not Example: PutItem - The PutItem request mapping document lets you tell the AWS AppSync DynamoDB function to make a PutItem request to DynamoDB, and enables you to specify the following: The key of the item in DynamoDB The full contents of the item (composed of key and attributeValues) Conditions for the operation to succeed Example: UpdateItem - The UpdateItem request enables you to tell the AWS AppSync DynamoDB function to make a UpdateItem request to DynamoDB and allows you to specify the following: The key of the item in DynamoDB An update expression describing how to update the item in DynamoDB Conditions for the operation to succeed Example: DeleteItem - The DeleteItem request lets you tell the AWS AppSync DynamoDB function to make a DeleteItem request to DynamoDB, and enables you to specify the following: The key of the item in DynamoDB Conditions for the operation to succeed Example: Query - The Query request object lets you tell the AWS AppSync DynamoDB resolver to make a Query request to DynamoDB, and enables you to specify the following: Key expression Which index to use Any additional filter How many items to return Whether to use consistent reads query direction (forward or backward) Pagination token Example: Scan - The Scan request lets you tell the AWS AppSync DynamoDB function to make a Scan request to DynamoDB, and enables you to specify the following: A filter to exclude results Which index to use How many items to return Whether to use consistent reads Pagination token Parallel scans Example: Sync - The Sync request object lets you retrieve all the results from a DynamoDB table and then receive only the data altered since your last query (the delta updates). Sync requests can only be made to versioned DynamoDB data sources. You"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-to-existing-data-sources_connect-external-ddb-table.txt", "text": "can specify the following: Example: BatchGetItem - The BatchGetItem request object lets you tell the AWS AppSync DynamoDB function to make a BatchGetItem request to DynamoDB to retrieve multiple items, potentially across multiple tables. For this request object, you must specify the following: The DynamoDB BatchGetItem limits apply and no condition expression can be provided. Example: BatchDeleteItem - The BatchDeleteItem request object lets you tell the AWS AppSync DynamoDB function to make a BatchWriteItem request to DynamoDB to delete multiple items, potentially across multiple tables. For this request object, you must specify the following: The DynamoDB BatchWriteItem limits apply and no condition expression can be provided. Example: BatchPutItem - The BatchPutItem request object lets you tell the AWS AppSync DynamoDB function to make a BatchWriteItem request to DynamoDB to put multiple items, potentially across multiple tables. For this request object, you must specify the following: The DynamoDB BatchWriteItem limits apply and no condition expression can be provided. Example: TransactGetItems - The TransactGetItems request object lets you to tell the AWS AppSync DynamoDB function to make a TransactGetItems request to DynamoDB to retrieve multiple items, potentially across multiple tables. For this request object, you must specify the following: The DynamoDB TransactGetItems limits apply and no condition expression can be provided. Example: TransactWriteItems - The TransactWriteItems request object lets you tell the AWS AppSync DynamoDB function to make a TransactWriteItems request to DynamoDB to write multiple items, potentially to multiple tables. For this request object, you must specify the following: The destination table name of each request item The operation of each request item to perform. There are four types of operations that are supported: PutItem , UpdateItem , DeleteItem , and ConditionCheck The key of each request item to write The DynamoDB TransactWriteItems limits apply. Example:"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_pubsub_set-up-pubsub.txt", "text": "Set up Amplify PubSub The AWS Amplify PubSub category provides connectivity with cloud-based message-oriented middleware. You can use PubSub to pass messages between your app instances and your app's backend creating real-time interactive experiences. PubSub is available with AWS IoT and Generic MQTT Over WebSocket Providers . With AWS IoT, AWS Amplify's PubSub automatically signs your HTTP requests when sending your messages. The default export for PubSub will sign requests according to Signature Version 4 . Make sure that the @aws-amplify/pubsub package has the same version number as the aws-amplify package in your package.json file. To use in your app, import PubSub from the root export path: Create a new instance for your endpoint and region in your configuration: Find your aws_pubsub_endpoint by logging onto your AWS Console , choosing IoT Core from the list of services and then choosing Settings from the left navigation pane. To use PubSub with AWS IoT, you will need to create the necessary IAM policies in the AWS IoT Console, and attach them to your Amazon Cognito Identity. Go to IoT Core and choose Security from the left navigation pane, and then Policies from the dropdown menu. Next, click Create . The following myIoTPolicy policy will allow full access to all the topics. The next step is attaching the policy to your Cognito Identity . You can retrieve the Cognito Identity Id of a logged in user with Auth Module: Then, you need to send your Cognito Identity Id to the AWS backend and attach myIoTPolicy . You can do this with the following AWS CLI command: For your Cognito Authenticated Role to be able to interact with AWS IoT it may be necessary to update its permissions, if you haven't done this before. One way of doing this is to log to your AWS Console , select CloudFormation from the available services. Locate the parent stack of your solution: it is usually named <SERVICE-NAME>-<CREATION_TIMESTAMP> . Select the Resources tab and tap on AuthRole Physical ID . The IAM console will be opened in a new tab. Once there, tap on the button Attach Policies , then search AWSIoTDataAccess and AWSIoTConfigAccess , select them and tap on Attach policy . If you are using Cognito Groups, the IAM role associated with that group also need the AWSIoTDataAccess and AWSIoTConfigAccess policies attached to it. Failing to grant IoT related permissions to the Cognito Authenticated Role will result in errors similar to the following in your browser console: errorCode: 8, errorMessage: AMQJS0008I Socket closed. In a real-world application, the code that sets up a pubsub instance ( const pubsub = new PubSub(...) ) will be used in multiple places. This means that the configuration will be separate from where your application publishes ( pubsub.publish(...) ) or subscribes ( pubsub.subscribe(...) ). If you already know all the connections when deploying your application, you can export singleton instances for other parts of your application to easily import and use. ./src/utils/pubsub.ts : ./src/components/LatestMessage.tsx : This means you will maintain"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_pubsub_set-up-pubsub.txt", "text": "a single connection to the target endpoint without needing to pass the pubsub instance as a property through layers of components. Import PubSub from the mqtt specific export path Create a new instance for your endpoint and region in your configuration: You can integrate any MQTT Over WebSocket provider with your app. Click here to learn more about MQTT Over WebSocket. Only JSON serializable message payloads are currently supported for MQTT providers within PubSub. If you are attempting to use message payloads that are non-JSON serializable, consider transforming the payload into a format that aligns with the input type expected by MQTT ."}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic.txt", "text": "Add custom queries and mutations The a.model() data model provides a solid foundation for querying, mutating, and fetching data. However, you may need additional customizations to meet specific requirements around custom API requests, response formatting, and/or fetching from external data sources. In the following sections, we walk through the three steps to create a custom query or mutation: Define a custom query or mutation Configure custom business logic handler code Invoke the custom query or mutation For every custom query or mutation, you need to set a return type and, optionally, arguments. Use a.query() or a.mutation() to define your custom query or mutation in your amplify/data/resource.ts file: Custom query Custom mutation After your query or mutation is defined, you need to author your custom business logic. You can either define it in a function or using a custom resolver powered by AppSync JavaScript resolver . Function Custom resolver powered by AppSync JavaScript resolvers In your amplify/data/echo-handler/ folder, create a handler.ts file. You can import a utility type for your function handler via the Schema type from your backend resource. This gives you type-safe handler parameters and return values. In your amplify/data/resource.ts file, define the function using defineFunction and then reference the function with your query or mutation using a.handler.function() as a handler. If you want to use an existing Lambda function, you can reference it by its name: a.handler.function('name-of-existing-lambda-fn') . Note that Amplify will not update this external Lambda function or its dependencies. Custom resolvers work on a \"request/response\" basis. You choose a data source, map your request to the data source's input parameters, and then map the data source's response back to the query/mutation's return type. Custom resolvers provide the benefit of no cold starts, less infrastructure to manage, and no additional charge for Lambda function invocations. Review Choosing between custom resolver and function . In your amplify/data/resource.ts file, define a custom handler using a.handler.custom . By default, you'll be able to access any existing database tables (powered by Amazon DynamoDB) using a.ref('MODEL_NAME') . But you can also reference any other external data source from within your AWS account, by adding them to your backend definition. The supported data sources are: Amazon DynamoDB AWS Lambda Amazon RDS databases with Data API Amazon EventBridge OpenSearch HTTP endpoints You can add these additional data sources via our amplify/backend.ts file: In your schema you can then reference these additional data sources based on their name: All handlers must be of the same type. For example, you can't mix and match a.handler.function with a.handler.custom within a single .handler() modifier. From your generated Data client, you can find all your custom queries and mutations under the client.queries. and client.mutations. APIs respectively. Custom query Custom mutation Custom operations can accept different types of arguments. Understanding these options helps define flexible and well-structured APIs. When defining a custom operation, you can specify arguments using different types: Scalar Fields : Basic types such as string , integer , float , etc Custom Types : Define inline customType"}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic.txt", "text": "Reference Types : Use a.ref() to reference enums and custom types Async function handlers allow you to execute long-running operations asynchronously, improving the responsiveness of your API. This is particularly useful for tasks that don't require an immediate response, such as batch processing, putting messages in a queue, and initiating a generative AI model inference. To define an async function handler, use the .async() method when defining your handler: Single Return Type : Async handlers return a static type EventInvocationResponse and don't support specifying a return type. The .returns() method is not available for operations using async handlers. Fire and Forget : The client is informed whether the invocation was successfully queued, but doesn't receive data from the Lambda function execution. Pipeline Support : Async handlers can be used in function pipelines. If the final handler is an async function, the return type of the query or mutation is EventInvocationResponse ."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging_display-messages.txt", "text": "Display messages In-app messages are displayed when an In-App Messaging or analytics event is sent and matches the criteria defined by your active In-App Messaging campaigns. Now that messages have been synced to your users' devices, Amplify In-App Messaging will allow you to start displaying them with Amplify Analytics events with no additional integration steps. Any events you record or are already recording using the Analytics' record API are automatically picked up and processed by In-App Messaging. If the event matches the attributes and criteria defined in an in-app message, that message will be displayed. If the event name, attributes, and metrics match those set forth by one of your In-App Messaging campaigns, you should see the in-app message displayed in your app. In addition to or instead of Amplify Analytics events, you can also dispatch In-App Messaging events to trigger an in-app message display programmatically. If the event name, attributes, and metrics match those set forth by one of your In-App Messaging campaigns, you should see the in-app message displayed in your app."}, {"source": "data/raw_pages/react_deploy-and-host_sandbox-environments.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_passwordless.txt", "text": "Passwordless Amplify supports the use of passwordless authentication flows using the following methods: Passwordless authentication removes the security risks and user friction associated with traditional passwords. Learn how to implement passwordless sign-in flows by overriding the Cognito UserPool to enable the sign-in methods below . SMS-based authentication uses phone numbers as the identifier and text messages as the verification channel. At a high level end users will perform the following steps to authenticate: User enters their phone number to sign up/sign in They receive a text message with a time-limited code After the user enters their code they are authenticated Learn more about using SMS OTP in your application code . Email-based authentication uses email addresses for identification and verification. At a high level end users will perform the following steps to authenticate: User enters their email address to sign up/sign in They receive an email message with a time-limited code After the users enters their code they are authenticated Learn more about using email OTP in your application code . WebAuthn uses biometrics or security keys for authentication, leveraging device-specific security features. At a high level end users will perform the following steps to authenticate: User chooses to register a passkey Their device prompts for biometric/security key verification For future logins, they'll authenticate using the same method Learn more about using WebAuthn passkeys in your application code . Learn more about managing WebAuthn credentials ."}, {"source": "data/raw_pages/react_start_account-setup.txt", "text": "Configure AWS for local development Note : If you already have an AWS account and profile configured locally, you do not need to follow this guide. Please add the AmplifyBackendDeployFullAccess IAM role to your configured AWS profile. This guide will help you set up Temporary credentials with IAM Identity Center and AWS Organizations , which will enable you to define Single-sign on (SSO), users, groups, permission sets, and more for your team. AWS Organizations can grow to house multiple AWS accounts. Users within the organization can traverse the AWS account(s) as their permission set allows. Amplify leverages the standard local credentials chain provider to simplify access to AWS services. While this guide highlights IAM Identity Center, you can explore additional methods for authenticating with AWS locally . IAM Identity Center terminology IAM Identity Center enables users to sign in using a single user identity to access all their assigned AWS accounts, business applications, and custom applications in the AWS Cloud. This single sign-on capability reduces the complexity of managing multiple credentials and improves security by centralizing user authentication. Users refers to the location where user identities and group information are stored and managed. IAM Identity Center can integrate with external identity sources like Microsoft Active Directory or use a built-in identity store provided by AWS. A collection of permissions that can be assigned to users or groups. Permission sets define what actions users are allowed to perform in your AWS accounts. They are similar to IAM policies but are used within the context of IAM Identity Center to manage access across multiple accounts. AWS Organizations and IAM Identity Center work together to streamline management across multiple AWS accounts. AWS Organizations manages account structures and policies, while IAM Identity Center integrates with it to enable single sign-on and align permissions with organizational roles. This synergy ensures secure and consistent access control, simplifying user and permission management. Credentials are typically resolved through the use of AWS profiles . Profiles can contain permanent credentials or SSO metadata, and can be set for use with Amplify by using the same techniques as the AWS CLI: with the --profile flag with the AWS_PROFILE environment variable An alternative to permanent credentials, enable you to define permissions for a session . Sessions are created when you assume an IAM role or sign in using AWS IAM Identity Center. These sessions come with an additional \"session token\" that is used to validate the temporary credentials and must be included on requests to AWS. As you are working locally, this will be presented as an additional environment variable. You can use temporary security credentials to make programmatic requests for AWS resources using the AWS CLI or AWS API (through the AWS SDKs). The temporary credentials provide the same permissions as long-term security credentials, such as IAM user credentials. However, there are a few differences, which are covered in the AWS Identity and Access Management documentation . Close accordion Follow the steps below if you have never set up AWS"}, {"source": "data/raw_pages/react_start_account-setup.txt", "text": "profiles before . If you already have a profile, attach the AmplifyBackendDeployFullAccess managed policy to your IAM user . Sign in to the AWS Console to access IAM Identity Center page and choose Enable . A dialog will open, prompting you to \"Choose how to configure IAM Identity Center in your AWS environment.\" Select Enable with AWS Organizations and choose Continue . Next, we are going to automate a number of steps that simulate the operations of setting up a user in the IAM Identity Center console. To get started open CloudShell, located in the console footer. Paste the following command in the CloudShell terminal and enter an email address you would like to associate with this AWS account: Enter email address: <your-email-address> Now, run the following command To validate that this worked, run the following command in the CloudShell. If something failed in this process, please report an issue . Keep this information readily available for the next step . A step-by-step walkthrough in the console Prefer a manual set up? After the AWS Organization is created and IAM Identity Center is enabled, you are presented with a dashboard. In the navigation pane, select Permission sets . Select Create permission set . When prompted for the permission set type, choose Custom permission set . Then choose Next . Expand AWS Managed Policies (set) and search for amplify . Select AmplifyBackendDeployFullAccess and choose Next . Name the permission set amplify-policy and optionally change the session duration. Choose Next . Review the permission set and choose Create . Once the permission set is created, you will return to the IAM Identity Center dashboard. You are now ready to create your first user. Using the navigation pane, select Users . Enter the user details, then choose Next . Optionally create and add the user to a group, and choose Next . Review the user information and select Add user . The user will then need to verify their email using the email specified during user creation. Once the new user is created, you will return to the IAM Identity Center dashboard. The next step is to grant the user access to an AWS account. For this demo, we will use the AWS account we used to create the Organization, but you can create a new AWS account under your organization for use with Amplify. Select the checkbox next to the management account and choose Assign users or groups . When prompted to assign a user or group, select the Users tab, select the user created in step 13, and choose Next . Assign the permission set created in step 9 and choose Next . Review the assignment information and choose Submit . Now you are ready to sign in to the access portal. Navigate back to the IAM Identity Center dashboard. Within the Settings summary pane, copy the URL for your AWS access portal URL . Navigate to the copied URL and sign in as your user, amplify-admin . After signing in,"}, {"source": "data/raw_pages/react_start_account-setup.txt", "text": "you should have access to an AWS account. Close accordion Now create a password for the user that we need for the next step. In the IdC console, navigate to Users > amplify_admin > Reset password > Send an email to the user with instructions for resetting the password . Check your email (make sure you also check your spam folder). Click on the Reset password link and choose a password of your choice. When signing in make sure to use amplify-admin as the Username . Now, set up an AWS profile that is linked to the user you just created on your local machine. There are a few options for getting IAM Identity Center user credentials , but we will use the AWS CLI configuration wizard. Install the AWS CLI . Mac Windows Linux In your browser, Download and run the AWS CLI MSI installer for Windows (64-bit): Install on Windows Open your terminal, you are ready to configure an AWS profile that uses the SSO user. Use the information from CloudShell to populate the information below. After you provide this information, the browser will automatically open asking you to sign in with the username and password you just created and configure a multi-factor device to authenticate. Now return to the terminal and enter the following information: Make sure to set the profile name to default . Alternatively, remember the auto-generated profile name; you will need this later. If you inspect ~/.aws/config , you should now see the SSO profile: Now you are ready to use this AWS profile with AWS Amplify. Open your Amplify project and start the sandbox. If you have multiple local profiles or named your profile something other than default , you can specify a profile with --profile . Before you can start deploying resources in the cloud sandbox environment, Amplify will need to complete a one-time bootstrap setup for the account and AWS Region before it can start deploying resources. Bootstrapping is the process of provisioning resources for the AWS CDK before you can deploy AWS CDK apps into an AWS environment. These resources include an Amazon S3 bucket for storing files and IAM roles that grant permissions needed to perform deployments. The required resources are defined in an AWS CloudFormation stack, called the bootstrap stack, which is usually named CDKToolkit . Like any AWS CloudFormation stack, it appears in the AWS CloudFormation console once it is deployed. You can learn more about this process in the CDK documentation . Close accordion During the first-time setup, npx ampx sandbox will ask you to sign in to the AWS Management Console. You must sign in as the account root user or as a user that has AdministratorAccess permissions. Once signed in, you will be redirected to the Amplify console. On the Create new app page, choose Initialize setup now . It may take a few minutes for the bootstrapping process to complete. You have successfully completed the bootstrapping process and you can now return to"}, {"source": "data/raw_pages/react_start_account-setup.txt", "text": "the terminal to create a new Amplify sandbox environment:"}, {"source": "data/raw_pages/react_build-a-backend_troubleshooting_library-not-configured.txt", "text": "Troubleshoot configuration errors If you are running into a missing configuration or NoCredentials error message and have called Amplify.configure in your project, your Amplify API is most likely being called before Amplify.configure . This can happen in a few different ways. Below are three possibilities you can check to troubleshoot this issue. Make sure you are calling Amplify.configure in the root file of your project. The root file of your app may be different depending on your frontend framework. The current default for some common frameworks are listed below (if you are not using TypeScript the ts and tsx extensions would be js and jsx ): Vue.js: src/main.ts React: src/main.tsx Angular: src/main.ts Next.js Page Router: pages/_app.tsx or src/pages/_app.tsx Nuxt: app.vue (Or in a plugins file, as recommended here .) If you are using the Next.js App Router, you can follow the suggestions in our Next.js documentation for root-level configuration. Keep in mind that if you are calling any APIs at the module-level (i.e. at the top of your file) in any of the Child components, you may still run into this issue. Continue on the Check 2 if this is the case. When Amplify APIs are used outside of your application lifecycle, there is a risk that a JavaScript bundler may place that API call before Amplify.configure . Module-level function calls (calls at the top-level of a file), are generally evaluated in the order that they are imported. Below is an example of code that will likely result in a missing configuration or NoCredentials error message: This error can also happen when using Next.js Layouts and calling Amplify APIs in child components at the module-level (at the top of your file/module). See below for an example of this issue: To fix this, we suggest moving all Amplify API calls to within the application lifecycle. For instance, if you are using React , you can use the useEffect hook for functions that should run before the app is loaded: If you are working in a multi-page app, you need to call Amplify.configure() for each page/route of your application. We recommend calling Amplify.configure in a common source file and importing it into each page."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_pubsub_publish.txt", "text": "Publish To send a message to a topic, use publish() method with your topic name and the message: If multiple providers are defined in your app you can pass the message to a specific provider: You can also publish a message to multiple topics:"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_rest-api_customize-authz.txt", "text": "Define authorization rules When determining the authorization mode for your REST endpoint, there are a few customizations you can do. By default, the API will be using IAM authorization and the requests will be signed for you automatically. IAM authorization has two modes: one using an unauthenticated role, and one using an authenticated role. When the user has not signed in, the unauthenticated role is used by default. Once the user has signed in, the authenticate role is used, instead. If you want to configure a public REST API, you can set an API key in Amazon API Gateway or create one using the CDK construct . Then, you can set the API key header in the API configuration which will be applied to all requests. You can use the access token from configured Cognito User Pool to authenticate against REST endpoint. The JWT token can be retrieved from the Auth category. Then you need to set the Authorization header in the API category configuration. The following example shows how to set the Authorization header for all requests. For more details on how to configure the API Gateway with the custom authorization, see this The ID Token contains claims about the identity of the authenticated user such as name, email, and phone_number. On the Amplify Authentication category you can retrieve the Id Token using: The Access Token contains scopes and groups and is used to grant access to authorized resources. This is a tutorial for enabling custom scopes . You can retrieve the Access Token using If you want to use a custom authorization token, you can set the token in the API category configuration. The custom authorization token will be applied to all requests. Alternatively, you can set the authorization headers per request. For example, if you want to use a custom header named Authorization for a specific REST request, you can set the following configuration:"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_location-search.txt", "text": "Work with location search First, make sure you've provisioned a search index resource and configured your app using the instructions in either Configure Location Search or Use existing Amazon Location Service resources and you have already setup displaying a map in your application. To add a location search UI component to your map, you can use the maplibre-gl-geocoder library. maplibre-gl-js-amplify package makes it easy to integrate maplibre-gl-geocoder with Amplify Geo by exporting a utility function createAmplifyGeocoder() that returns an instance of maplibre-gl-geocoder with some pre-defined settings and supports all the options for customizing the UI component Install the necessary dependencies with the following command: Note: Make sure that maplibre-gl-js-amplify version 4.0.0 or above is installed. First, create a map onto which you want to add the location search UI component. See the guide on creating and displaying maps . Then, use createAmplifyGeocoder() to get a new instance of MaplibreGeocoder and add the location search UI component to the map. Note: Ensure that your package bundler (webpack, rollup, etc) is configured to handle css files. Check out the webpack documentation here . You can also use maplibre-gl-geocoder to display the location search UI component anywhere in your application, even outside the map. To do so, extract the html element using function onAdd() and attach it anywhere in your DOM instead of adding it via the map's addControl() function. You can customize the search icons used by the maplibre-gl-geocoder to use any image of your choosing. MapLibre markers require an HTMLElement when passing in custom images. The following example puts an existing SVG icon into an HTMLElement before being passed to createAmplifyGeocoder which creates a maplibre-gl-geocoder . Amplify Geo enables you to search for locations by text, addresses, or geo-coordinates. The Geo.searchByText() API enables you to search for places or points of interest by free-form text, such as an address, name, city, or region. Customize your search results further by providing: countries - to limit the search results to given countries (specified in ISO Alpha-3 country codes ) maxResults - to limit the maximum result set biasPosition - to act as the search origination location searchAreaConstraints - to limit the area to search inside of searchIndexName - to use a different Location Service search index resource than the default Note: Providing both biasPosition and searchAreaConstraints parameters simultaneously returns an error. This returns places and their coordinates that match the search constraints. A place can also have additional metadata as shown in the example below. The Geo.searchByCoordinates() API is a reverse Geocoder that takes a coordinate point and returns information about what it finds at that point on the map. The returned object is the same shape as searchByText() API above. You can optionally limit your result set with the maxResults parameter or override the default search index with the searchIndexName parameter. The Geo.searchForSuggestions() API enables you to search for suggestions by free-form text, such as a place, address, city, or region. Similar to Geo.searchByText() API, customize your search results further by providing:"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_location-search.txt", "text": "countries - to limit the search results to given countries (specified in ISO Alpha-3 country codes ) maxResults - to limit the maximum result set biasPosition - to act as the search origination location searchAreaConstraints - to limit the area to search inside of searchIndexName - to use a different Location Service search index resource than the default Note: Providing both biasPosition and searchAreaConstraints parameters simultaneously returns an error. This returns a list of suggestions (places and their respective placeId if available) that match the search constraints. In cases where placeId is not available on the list of suggestions as below, use searchByText to search for the selected place by text. This returns places and their coordinates that match the search text. The Geo.searchByPlaceId() API enables you to search for a place by a placeId , which is a unique opaque token for a place returned by the provider. You can optionally override the default search index with the searchIndexName parameter. This returns a place with metadata as shown in the example below."}, {"source": "data/raw_pages/react_ai.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_switching-authentication-flows.txt", "text": "Switching authentication flows For client side authentication there are four different flows: USER_SRP_AUTH : The USER_SRP_AUTH flow uses the SRP protocol (Secure Remote Password) where the password never leaves the client and is unknown to the server. This is the recommended flow and is used by default. USER_PASSWORD_AUTH : The USER_PASSWORD_AUTH flow will send user credentials to the backend without applying SRP encryption. If you want to migrate users to Cognito using the \"Migration\" trigger and avoid forcing users to reset their passwords, you will need to use this authentication type because the Lambda function invoked by the trigger needs to verify the supplied credentials. CUSTOM_WITH_SRP & CUSTOM_WITHOUT_SRP : Allows for a series of challenge and response cycles that can be customized to meet different requirements. USER_AUTH : The USER_AUTH flow is a choice-based authentication flow that allows the user to choose from the list of available authentication methods. This flow is useful when you want to provide the user with the option to choose the authentication method. The choices that may be available to the user are EMAIL_OTP , SMS_OTP , WEB_AUTHN , PASSWORD or PASSWORD_SRP . The Auth flow can be customized when calling signIn , for example: For more information about authentication flows, please visit AWS Cognito developer documentation The USER_AUTH sign in flow supports the following methods as first factors for authentication: WEB_AUTHN , EMAIL_OTP , SMS_OTP , PASSWORD , and PASSWORD_SRP . If the desired first factor is known when authentication is initiated, it can be passed to the signIn API as the preferredChallenge to initiate the corresponding authentication flow. If the desired first factor is not known or you would like to provide users with the available options, preferredChallenge can be omitted from the initial signIn API call. This allows you to discover which authentication first factors are available for a user via the CONTINUE_SIGN_IN_WITH_FIRST_FACTOR_SELECTION step. You can then present the available options to the user and use the confirmSignIn API to respond with the user's selection. Also, note that if the preferredChallenge passed to the initial signIn API call is unavailable for the user, Amplify will also respond with the CONTINUE_SIGN_IN_WITH_FIRST_FACTOR_SELECTION next step. For more information about determining a first factor, and signing in with passwordless authentication factors, please visit the Passwordless concepts page. A use case for the USER_PASSWORD_AUTH authentication flow is migrating users into Amazon Cognito In order to use the authentication flow USER_PASSWORD_AUTH , your Cognito app client has to be configured to allow it. In the AWS Console, this is done by ticking the checkbox at General settings > App clients > Show Details (for the affected client) > Enable username-password (non-SRP) flow. If you're using the AWS CLI or CloudFormation, update your app client by adding USER_PASSWORD_AUTH to the list of \"Explicit Auth Flows\". Amazon Cognito provides a trigger to migrate users from your existing user directory seamlessly into Cognito. You achieve this by configuring your User Pool's \"Migration\" trigger which invokes a Lambda function whenever a user"}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_switching-authentication-flows.txt", "text": "that does not already exist in the user pool authenticates, or resets their password. In short, the Lambda function will validate the user credentials against your existing user directory and return a response object containing the user attributes and status on success. An error message will be returned if an error occurs. Visit Amazon Cognito user pools import guide for migration flow and more detailed instruction, and Amazon Cognito Lambda trigger guide on how to set up lambda to handle request and response objects. Amazon Cognito user pools supports customizing the authentication flow to enable custom challenge types, in addition to a password in order to verify the identity of users. These challenge types may include CAPTCHAs or dynamic challenge questions. The CUSTOM_WITH_SRP flow requires a password when calling signIn . Both of these flows map to the CUSTOM_AUTH flow in Cognito. To initiate a custom authentication flow in your app, call signIn without a password. A custom challenge needs to be answered using the confirmSignIn API: To create a CAPTCHA challenge with a Lambda Trigger, please visit AWS Amplify Google reCAPTCHA challenge example for detailed examples."}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_connect-http-datasource.txt", "text": "Connect to an external HTTP endpoint The HTTP Datasource allows you to quickly configure HTTP resolvers within your Data API. This guide will demonstrate how to establish a connection to an external REST API using an HTTP data source and use Amplify Data's custom mutations and queries to interact with the REST API. For the purpose of this guide we will define a Post type and use an existing external REST API that will store records for it. In Amplify Gen 2, customType adds a type to the schema that is not backed by an Amplify-generated DynamoDB table. With the Post type defined, it can then be referenced as the return type when defining your custom queries and mutations. First, add the Post custom type to your schema: To integrate the external REST API or HTTP API, you'll need to set it up as the HTTP Datasource. Add the following code in your amplify/backend.ts file. Now that your REST API has been added as a data source, you can reference it in custom queries and mutations using the a.handler.custom() modifier which accepts the name of the data source and an entry point for your resolvers. Use the following code examples to add addPost , getPost , updatePost , and deletePost as custom queries and mutations to your schema: addPost getPost updatePost deletePost Next, create the following files in your amplify/data folder and use the code examples to define custom resolvers for the custom queries and mutations added to your schema from the previous step. These are AppSync JavaScript resolvers. addPost getPost updatePost deletePost From your generated Data client, you can find all your custom queries and mutations under the client.queries. and client.mutations. APIs respectively. addPost getPost updatePost deletePost In this guide, you\u00e2\u0080\u0099ve added an external REST API as a HTTP data source to an Amplify Data API and defined custom queries and mutations, handled by AppSync JS resolvers, to manipulate Post items in an external REST API using the Amplify Gen 2 Data client. To clean up, you can delete your sandbox by accepting the prompt when terminating the sandbox process in your terminal. Alternatively, you can also use the AWS Amplify console to manage and delete sandbox environments."}, {"source": "data/raw_pages/react_build-a-backend_storage_set-up-storage.txt", "text": "Set up Storage In this guide, you will learn how to set up storage in your Amplify app. You will set up your backend resources, and enable listing, uploading, and downloading files. If you have not yet created an Amplify app, visit the quickstart guide . Amplify Storage seamlessly integrates file storage and management capabilities into frontend web and mobile apps, built on top of Amazon Simple Storage Service (Amazon S3). It provides intuitive APIs and UI components for core file operations, enabling developers to build scalable and secure file storage solutions without dealing with cloud service complexities. First, create a file amplify/storage/resource.ts . This file will be the location where you configure your storage backend. Instantiate storage using the defineStorage function and providing a name for your storage bucket. This name is a friendly name to identify your bucket in your backend configuration. Amplify will generate a unique identifier for your app using a UUID, the name attribute is just for use in your app. Import your storage definition in your amplify/backend.ts file that contains your backend definition. Add storage to defineBackend . Now when you run npx ampx sandbox or deploy your app on Amplify, it will configure an Amazon S3 bucket where your files will be stored. Before files can be accessed in your application, you must configure storage access rules. To deploy these changes, commit them to git and push the changes upstream. Amplify's CI/CD system will automatically pick up the changes and build and deploy the updates. By default, no users or other project resources have access to any files in the storage bucket. Access must be explicitly granted within defineStorage using the access callback. The access callback returns an object where each key in the object is a file path and each value in the object is an array of access rules that apply to that path. The following example shows you how you can set up your file storage structure for a generic photo sharing app. Here, Guests have access to see all profile pictures and only the users that uploaded the profile picture can replace or delete them. Users are identified by their Identity Pool ID in this case i.e. identityID. There's also a general pool where all users can submit pictures. Learn more about customizing access to file path . Amplify Storage gives you the flexibility to configure your backend to automatically provision and manage multiple storage resources. You can define additional storage buckets by using the same defineStorage function and providing a unique, descriptive name to identify the storage bucket. You can pass this name to the storage APIs to specify the bucket you want to perform the action to. Ensure that this name attribute is unique across the defined storage buckets in order to reliably identify the correct bucket and prevent conflicts. It's important to note that if additional storage buckets are defined one of them must be marked as default with the isDefault flag. Add additional storage resources"}, {"source": "data/raw_pages/react_build-a-backend_storage_set-up-storage.txt", "text": "to the backend definition. Additional storage buckets can be referenced from application code by passing the bucket option to Amplify Storage APIs. You can provide a target bucket's name assigned in Amplify Backend. import { downloadData } from 'aws-amplify/storage' ; const result = downloadData ( { path : \"album/2024/1.jpg\" , Copy highlighted code example console . log ( ` Error: ${ error } ` ) Alternatively, you can also pass in an object by specifying the bucket name and region from the console. See each Amplify Storage API page for additional usage examples. import { downloadData } from 'aws-amplify/storage' ; const result = downloadData ( { path : 'album/2024/1.jpg' , Copy highlighted code example bucketName : 'second-bucket-name-from-console' , console . log ( ` Error: ${ error } ` ) ; The Amplify Storage library provides client APIs that connect to the backend resources you defined. Import and load the configuration file in your app. It's recommended you add the Amplify configuration step to your app's root entry point. For example index.js in React or main.ts in Angular. Make sure you call Amplify.configure as early as possible in your application\u00e2\u0080\u0099s life-cycle. A missing configuration or NoCredentials error is thrown if Amplify.configure has not been called before other Amplify JavaScript APIs. Next, let's a photo to the picture-submissions/ path. After successfully publishing your storage backend and connecting your project with client APIs, you can manage files and folders in the Amplify console . You can perform on-demand actions like upload, download, copy, and more under the Storage tab in the console. Refer to Manage files in Amplify Console guide for additional information. Congratulations! You finished the Set up Amplify Storage guide. In this guide, you set up and connected to backend resources, customized your file paths and access definitions, and connected your application to the backend to implement features like file uploads and downloads. Now that you have completed setting up storage in your Amplify app, you can proceed to add file management features to your app. You can use the following guides to implement upload and download functionality, or you can access more capabilities from the side navigation."}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_connect-eventbridge-datasource.txt", "text": "Connect to Amazon EventBridge to send and receive events Amazon EventBridge is a serverless event bus that simplifies how applications communicate with each other. It acts as a central hub for events generated by various sources, including AWS services, custom applications, and third-party SaaS providers. EventBridge delivers this event data in real-time, allowing you to build applications that react swiftly to changes. You define rules to filter and route these events to specific destinations, known as targets. Targets can include services like AWS Lambda, Amazon SQS Queues, Amazon SNS Topics. For the purpose of this guide, we will use AWS AppSync as the target for events. By adopting an event-driven architecture with EventBridge, you can achieve: Loose Coupling : Applications become independent and communicate through events, improving scalability and maintainability. Increased Resilience : System failures are isolated as events are delivered asynchronously, ensuring overall application availability. Simplified Integration : EventBridge provides a unified interface for integrating diverse event sources, streamlining development. This section will guide you through adding an event bus as a datasource to your API, defining routing rules, and configuring targets to build robust event-driven applications with AWS Amplify Gen 2 and Amazon EventBridge. Set up your API Add your Amazon EventBridge event bus as a data source Define custom queries and mutations Configure custom business logic handler code Invoke custom mutations to send events to EventBridge Subscribe to mutations invoked by EventBridge Invoke mutations and trigger subscriptions from EventBridge For the purpose of this guide, we will define an OrderStatusChange custom type that represents an order status change event. This type includes fields for the order ID, status, and message. In your amplify/data/resource.ts file, use the following code to define an OrderStatusChange custom type and an OrderStatus enum, adding them to your schema: NOTE: At least one query is required for a schema to be valid. Otherwise, deployments will fail a schema error. The Amplify Data schema is auto-generated with a Todo model and corresponding queries under the hood. You can leave the Todo model in the schema until you add the first custom query to the schema in the next steps. In your amplify/backend.ts file, use the following code to add the default event bus as a data source for your API: The selection set returned by the mutation must match the selection set of the subscription. If the selection set of the mutation is different from the selection set of the subscription, the subscription will not receive the event. In the code snippet above, the addEventBridgeDataSource method is used to add the default event bus as a data source to your API. This allows you to reference the event bus in your custom queries and mutations. The CfnRule construct is used to create an EventBridge rule that routes events to the AppSync API. The rule specifies the event pattern to match and the target to invoke when the event is received. In this example, the target is an AppSync mutation named publishOrderFromEventBridge . The appSyncParameters"}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_connect-eventbridge-datasource.txt", "text": "property specifies the mutation to invoke when the event is received. The inputTransformer property maps the event data to the mutation arguments. Now that your event bus has been added as a data source, you can reference it in custom queries and mutations using the a.handler.custom() modifier which accepts the name of the data source and an entry point for your resolver. Use the following code to add publishOrderToEventBridge and publishOrderFromEventBridge custom mutations, and an onOrderStatusChange custom subscription to your schema: In the code snippet above: The publishOrderToEventBridge custom mutation uses an EventBridge data source and so it is able to publish events to the event bus from its resolver. The publishOrderFromEventBridge custom mutation uses a None data source as a passthrough and is invoked by the EventBridge rule when an event is received that matches the rule pattern. The allow.guest rule uses IAM under the hood and allows the mutation to be invoked by the EventBridge rule. The onOrderFromEventBridge custom subscription can be triggered either by EventBridge invoking the publishOrderFromEventBridge mutation or by a client invoking the publishOrderToEventBridge mutation. Next, create the following files in your amplify/data folder and use the code examples to define custom resolvers for the custom queries and mutations added to your schema from the previous step. These are AppSync JavaScript resolvers Subscription Publish Order to EventBridge Publish Order From EventBridge The following code defines the custom business logic handler for the onOrderStatusChange subscription. Since the subscription uses a None data source the response function is empty as the subscription does not require any additional processing. In the following code, the request function constructs the event payload to be published to the event bus. To match the rule pattern configured in the previous steps, the event source is set to amplify.orders and the detail-type is set to OrderStatusChange . The mutation arguments are passed to the event detail. The following code defines the custom business logic handler for the publishOrderFromEventBridge mutation. The request function constructs the mutation arguments from the event payload received from the event bus. The response function returns the mutation arguments. From your generated Data client, you can find all your custom queries and mutations under the client.queries and client.mutations APIs respectively. The custom mutation below will publish an order status change event to the event bus: To subscribe to events from your event bus, you can use the client.subscriptions API: You can test your custom mutation and subscriptions by using the EventBridge console to send an event which will invoke the custom mutation. You can then observe the results from the subscription being triggered: Navigate to the Amazon EventBridge console and choose \"Send Events\" Fill out the form, specifying the event source to be amplify.orders and the detail-type to be OrderStatusChange . Choose \"Send\" and observe the subscription output in the AppSync Queries console. In this guide, you\u00e2\u0080\u0099ve added an Amazon EventBridge event bus as a data source to an Amplify API and defined custom queries and mutations to publish and"}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_connect-eventbridge-datasource.txt", "text": "receive events from the event bus. You\u00e2\u0080\u0099ve also configured custom business logic handler code to handle the event data and invoke the appropriate mutations. To clean up, you can delete your sandbox by accepting the prompt when terminating the sandbox process in your terminal. Alternatively, you can also use the AWS Amplify console to manage and delete sandbox environments."}, {"source": "data/raw_pages/react_ai_concepts_prompting.txt", "text": "Prompting LLM prompting refers to the process of providing a language model, such as Claude or Amazon Titan, with a specific input or \"prompt\" in order to generate a desired output. The prompt can be a sentence, a paragraph, or even a more complex sequence of instructions that guides the model to produce content that aligns with the user's intent. The way the prompt is structured and worded can significantly influence the model's response. By crafting the prompt carefully, users can leverage the LLM's extensive knowledge and language understanding capabilities to generate high-quality and relevant text, code, or other types of output. Effective prompting involves understanding the model's strengths and limitations, as well as experimenting with different prompt formats, styles, and techniques to elicit the desired responses. This can include using specific keywords, providing context, breaking down tasks into steps, and incorporating formatting elements like bullet points or code blocks. The model APIs have improved beyond providing a single string as input and getting a string as output. Newer models have a more structured API where you define a system prompt, message history, and tool configurations. The Amplify AI kit uses Bedrock's Converse API , which has a structured input and output rather than only text in and text out. system prompt: provides high-level instructions to the LLM about its role and how it should respond messages: The conversation history you want the model to respond to. The Amplify AI kit handles saving conversation history and providing it to the model. tool configuration: information about the tools the model can choose to invoke. The Amplify AI kit handles creating the tool configuration for you as well as invoking the tools and re-prompting the model with the results. All AI routes in the Amplify AI kit require a system prompt. This will be used in all requests to the LLM. Be as detailed as possible. Try to give as much background and context as you can. Giving the LLM a role and scope typically helps focus the model's responses. Say what it should and shouldn't do. Sometimes LLMs can be a bit verbose or go on tangents. Giving it specific parameters like \"Never use placeholder data\". Use multiple routes. You can define as many conversation and generation routes as you like, so you don't need to try to fit all the context and functionality you need in a single route. You don't need to put everything into the system prompt. The message history or even just a single user message can contain a lot of dynamic information. Prompting strategies differ based on the model. Always read up on the model itself and what works/doesn't work well with the particular model you are using."}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_tokens-and-credentials.txt", "text": "Tokens and credentials Amplify Auth interacts with its underlying Amazon Cognito user pool as an OpenID Connect (OIDC) provider. When users successfully authenticate you receive OIDC-compliant JSON web tokens (JWT). These tokens are used to identity your user, and access resources. Access tokens are used to verify the bearer of the token (i.e. the Cognito user) is authorized to perform an action against a resource. Below is an example payload of an access token vended by Cognito: ID tokens are intended to be used within your frontend application only. This token contains personally identifiable information (PII) and should not be used to authorize access against a resource. Below is an example of an ID token with the default Amplify Auth configuration of email and password auth. When additional user attributes are specified for Amplify Auth, their values will be found in the ID token. For example, if a nickname attribute is requested it will be available on the ID token with the nickname claim: Conversely, user pool group claims are found in both the access token and ID token on the cognito:groups claim: Visit the AWS documentation for using tokens with Cognito user pools to learn more about tokens, how they're used with Cognito, and their intended usage. Token keys are automatically rotated for you for added security but you can update how they are stored, customize the refresh rate and expiration times, and revoke tokens on sign-out. You can update the storage mechanism to choose where and how tokens are persisted in your application. The default option is localStorage . Additionally, you can import the sessionStorage , sharedInMemoryStorage or CookieStorage options as well. If you want to customize your own mechanism, you can import the KeyValueStorageInterface interface and implement it in your own class. Browser Local Storage In Amplify the localStorage is the default storage mechanism. It saves the tokens in the browser's localStorage . This local storage will persist across browser sessions and tabs. You can explicitly set to this storage by calling: Cookie Storage CookieStorage saves the tokens in the browser's Cookies . The cookies will persist across browser sessions and tabs. You can explicitly set to this storage by calling: Browser Session Storage sessionStorage saves the tokens in the browser's sessionStorage and these tokens will clear when a tab is closed. The benefit to this storage mechanism is that the session only lasts as long as the browser is open and you can sign out users when they close the tab. You can update to this storage by calling: Custom Storage You can implement your own custom storage mechanism by creating a class that implements the storage interface. Here is an example that uses memory storage: When you get the current user session, the tokens will be saved in your custom location. Token revocation is enabled automatically in Amplify Auth. To revoke tokens you can set up global sign-out with signOut({ global: true }) to globally sign out your user from all of their devices."}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching_monorepos.txt", "text": "Monorepo setup Some teams choose a monorepo approach, or single repositories that contain multiple packages or components to simplify the deployment process for shared libraries and components. Without a monorepo, you have to deploy each package individually, keep track of package versions and dependencies across packages, and ensure version compatibility. This can become exponentially more complex as the number of packages grows. With a monorepo, all packages and dependencies are contained within a single repository. Amplify Gen 2 supports monorepo workflows for fullstack builds with monorepo tools such as Nx and yarn workspaces. When building with Gen 2, we recommend creating the amplify/ folder in a shared workspace. We will use the following example for this guide: Monorepos require a slightly different setup. We are going to deploy 3 Amplify apps: my-shared-backend admin-dashboard marketing-site The first app, my-shared-backend , will be the only app that updates changes to the backend. The other apps will only run frontend builds that point to the shared backend. To get started, deploy the shared backend Amplify app. With Gen 2, you can now setup backend-only CI/CD apps. Navigate to the Amplify console and select Create new app . Once you connect your repository, select your monorepo project. Check the box that says My app is a monorepo and enter the path to your amplify backend. Your build settings should be automatically detected. Save and deploy. For the frontend apps, connect the frontend projects in the Amplify console separately, and update the build commands to include: To locate the App ID for your backend application, navigate to the Amplify console and select your backend-app . On the Overview page, the App ID is displayed under the project name. If you're using Amplify Data, we recommend adding a paths entry in your tsconfig.json file that points to the amplify/data/resource.ts file to easily access your schema type definitions from your frontend apps. You can then import the Schema type from this path in your frontend code to get code completion and strong typing for your API calls:"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-event-api.txt", "text": "Connect to AWS AppSync Events This guide walks through how you can connect to AWS AppSync Events using the Amplify library. AWS AppSync Events lets you create secure and performant serverless WebSocket APIs that can broadcast real-time event data to millions of subscribers, without you having to manage connections or resource scaling. With this feature, you can build multi-user features such as a collaborative document editors, chat apps, and live polling systems. Learn more about AWS AppSync Events by visiting the Developer Guide . Before you begin, you will need: An Event API created via the AWS Console Take note of: HTTP endpoint, region, API Key This guide walks through how you can add an Event API to an existing Amplify backend. We'll be using Cognito User Pools for authenticating with Event API from our frontend application. Any signed in user will be able to subscribe to the Event API and publish events. Before you begin, you will need: An existing Amplify backend (see Quickstart ) Latest versions of @aws-amplify/backend and @aws-amplify/backend-cli ( npm add @aws-amplify/backend@latest @aws-amplify/backend-cli@latest ) First, we'll add a new Event API to our backend definition. To test your changes, deploy your Amplify Sandbox. After the sandbox deploys, connect your frontend application to the Event API. We'll be using the Amplify Authenticator component to sign in to our Cognito User Pool. If you don't already have the Authenticator installed, you can install it by running npm add @aws-amplify/ui-react ."}, {"source": "data/raw_pages/react_build-a-backend_storage.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-ui_formbuilder_customize.txt", "text": "Customize form inputs In this guide, you will learn how to customize connected forms that are generated by running npx ampx generate forms . Before you begin you will need: Your cloud sandbox with an Amplify Data resource up and running (npx ampx sandbox) A frontend application that has generated a connected form All Amplify forms are built with the Amplify UI library . The generated form provides a mechanism to override properties for each individual input component, like TextField , TextAreaField , SelectField . You can override any props to those components with the overrides prop on the form component. For example, if you want to change the variation and label of the content field in the TodoCreateForm: import TodoCreateForm from '@/ui-components/TodoCreateForm' Copy highlighted code example Note: We do not recommend overriding properties that are already set by the generated form. This could lead to unexpected behavior during runtime. Verify the set properties by navigating to the component in the src/ui-components/[your-form-component].jsx file. You own updating the code directly for the generated form. Here's how you can customize the form. You can manually add a form input connected to a data model to the generated form. For example, let's say you add a priority field to your data model. Make the following edits to the generated form: Select Fields , Radio Group Fields , and Autocomplete Fields require a set of options for your users to choose from. For example, a \"Status\" input can only have the options \"Not started\", \"In progress\", and \"Done\". This would be identical to the above 6 steps, but in step 6 you would replace <TextField> with <SelectField> Add spacing to your form and between inputs. Spacing values can either be a CSS length value ( px , rem , em , % ) or a reference to your theme object's spacing value ( xss , medium , large ). import TodoCreateForm from '@/ui-components/TodoCreateForm' < TodoCreateForm overrides = { { Copy highlighted code example You can customize action button labels to better describe your form's use case, such as changing Submit to Create Todo . import TodoCreateForm from '@/ui-components/TodoCreateForm' < TodoCreateForm overrides = { { Copy highlighted code example You can customize the visibility of action buttons to better accommodate your form's use case. import TodoCreateForm from '@/ui-components/TodoCreateForm' < TodoCreateForm overrides = { { Copy highlighted code example If you hide all form action buttons, you can still leverage the onChange event handler to self-manage the form lifecycle. This is useful for a form that updates data in real-time without explicit user confirmation. import TodoCreateForm from '@/ui-components/TodoCreateForm' Copy highlighted code example"}, {"source": "data/raw_pages/react_deploy-and-host_sandbox-environments_setup.txt", "text": "Use cloud sandbox in dev environment You can use a personal cloud sandbox environment that provides an isolated development space to rapidly build, test, and iterate on a fullstack app. Each developer on your team can use their own disposable sandbox environment connected to cloud resources. Cloud sandbox environments are designed for development purposes and are not intended for production workloads. To accelerate deployments, Amplify utilizes CDK hot swapping where supported, enabling rapid updates to resources such as AWS Lambda functions and AWS AppSync resolver templates without requiring a full redeployment. However, certain operations cannot be hot-swapped and require resource recreation to proceed: Amazon DynamoDB GSI Operations: In sandbox environments (but not in production), modifying or deleting a Global Secondary Index (GSI) is a time-intensive process. To simplify this, Amplify drops and recreates the table instead of modifying the index. Amazon Cognito User Pool Changes: Unsupported modifications, such as deleting a required field, result in Amplify dropping and recreating the user pool to ensure a consistent and functional configuration. You can set up a new sandbox environment on your machine once you have an Amplify app set up. If you have not yet created an Amplify Gen 2 app, visit the Quickstart . First, open the terminal and run the following command: When you deploy a cloud sandbox, Amplify creates an AWS CloudFormation stack following the naming convention of amplify-<app-name>-<$(whoami)>-sandbox in your AWS account with the resources configured in your amplify/ folder. After a successful deployment, sandbox watches for file changes in your amplify/ folder and performs real-time updates to the associated CloudFormation stack. This functionality is built leveraging the hot swap capability of the AWS Cloud Development Kit (CDK) . After testing all the changes associated with the backend, you can terminate the sandbox session via Ctrl + c . To delete all the resources in the sandbox environment, run the following command: You can view and manage all the sandbox environments for your team in the new Amplify console . This is useful for a team leader to audit all of the Amplify sandbox environments deployed within an account. Choose Manage Sandboxes to get started: You can then check the number, status, and last updates for sandbox environments across your team. You can also use the console to delete sandbox environments when no longer needed. Keep the following best practices in mind when working with cloud sandbox environments: Sandboxes are identical in fidelity to your production environments. Code changes are continuously deployed to your sandbox on every save for fast iterations. Use sandboxes for experimentation and testing, not for production workloads. Deploy one sandbox per Amplify app per developer to prevent conflicts. Reset sandboxes occasionally to clear out unused resources and save costs."}, {"source": "data/raw_pages/react_how-amplify-works.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_functions_examples_add-user-to-group.txt", "text": "Add user to group You can use defineAuth and defineFunction to create a Cognito post confirmation Lambda trigger that extends the behavior to perform some action when a user is confirmed. A user is \"confirmed\" when they verify their account. Typically this happens when the user confirms their email via the verification email. The post confirmation handler will not be triggered for federated sign-ins (i.e. social sign-in). To get started, install the AWS SDK v3 package, which will be used to perform actions against your auth resource, and the @types/aws-lambda package, which is used to define the handler type: Next, create a new directory and a resource file, amplify/auth/post-confirmation/resource.ts . Then, define the Function with defineFunction : After creating the Function definition you will need to: create the EVERYONE group grant access to your auth resource to ensure it can perform the addUserToGroup action set the Function as the post confirmation trigger Then create the Function's corresponding handler file, amplify/auth/post-confirmation/handler.ts , file with the following contents: After deploying the changes, whenever a user signs up and verifies their account they are automatically added to the group named \"EVERYONE\"."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging_create-campaign.txt", "text": "Create an in-app messaging campaign on AWS Console As an alternative to writing AWS Cloud Development Kit (CDK) code, you can use the AWS console to create a campaign that sends messages through any single channel that is supported by Amazon Pinpoint: Mobile Push, In-App, Email, SMS or Custom channels. Learn how to create a campaign using Amazon Pinpoint to continue integrating in-app messages in your app with Amplify. Login to the AWS Console , and Search for Pinpoint . Click on your project from the list of available project. Your project name would be the name you provided when you created the pinpoint project using CDK. Click on Campaigns from the left navigation menu, and then click on Create a campaign Add a name to your campaign, and keep the following options as follows and then click Next: Campaign type: Standard campaign Channel: In-App messaging set prioritization: Fairly important Click on the Create a segment radio button, add a name for your segment, and then click Next . You can add as many segments as needed to the campaign. For this quickstart, you can use Include any audiences under the Segment group 1 section. You can add a criteria to your segments to ensure that audiences that satisfy that criteria can receive the in-app message. If you see an error message titled Segment might include multiple channels , click I understand to proceed. Click on the Create a new in-app message radio button. You have the ability to customize the following attributes of the in-app message: Layout : Which includes all of the different messaging layout options. Header : Title of the in-app message, including the text color/alignment. Message : The body of the Message, including the text color/alignment. Background : Control the background color of the in-app message. Image URL : Add an image to be displayed as part of the in-app message body. Primary button : Allows the addition of a button to add functionality to the in-app message. Secondary button : Allows the addition of an extra button for additional functionality. Custom Data : Allows the in-app message to pass additional data to the frontend app once it is triggered by an event. For this tutorial you can create a simple message as shown below. Customers in your application will see the same message once the event is triggered. Once you have finished customizing your in-app message, click on Next . Under Trigger events , add the name of the analytics trigger that will be sent from your frontend app. You have the ability to customize the trigger to allow only certain attributes or metrics that are passed with the analytics event to trigger the in-app message. (Optional) By default, the number of messages shown per session is 1. You can update this threshold during campaign setup. Review your campaign, and then click on Launch campaign . Your campaign is now setup, and you are ready to start integrating the In-App Messaging functionality into your app."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging_create-campaign.txt", "text": "Note: Campaign start time must be at least 15 minutes in future. In-app messages can only be synced to local device once the campaign becomes active (status should be \"In Progress\" in the campaigns screen of the Pinpoint console)."}, {"source": "data/raw_pages/react_build-a-backend_data_customize-authz_signed-in-user-data-access.txt", "text": "Signed-in user data access The authenticated authorization strategy restricts record access to only signed-in users authenticated through IAM, Cognito, or OpenID Connect, applying the authorization rule to all users. It provides a simple way to make data private to all authenticated users. You can use the authenticated authorization strategy to restrict a record's access to every signed-in user. Note: If you want to restrict a record's access to a specific user, see Per-user/per-owner data access . The authenticated authorization strategy detailed on this page applies the authorization rule for data access to every signed-in user. In the example below, anyone with a valid JWT token from the Cognito user pool is allowed access to all Todos. In your application, you can perform CRUD operations against the model using client.models.<model-name> with the userPool auth mode. import { generateClient } from 'aws-amplify/data' ; import type { Schema } from '../amplify/data/resource' ; const client = generateClient < Schema > ( ) ; const { errors , data : newTodo } = await client . models . Todo . create ( Copy highlighted code example You can also override the authorization provider. In the example below, identityPool is specified as the provider which allows you to use an \"Unauthenticated Role\" from the Cognito identity pool for public access instead of an API key. Your Auth resources defined in amplify/auth/resource.ts generates scoped down IAM policies for the \"Unauthenticated role\" in the Cognito identity pool automatically. In your application, you can perform CRUD operations against the model using client.models.<model-name> with the iam auth mode. The user must be logged in for the Amplify Library to use the authenticated role from your Cognito identity pool. import { generateClient } from 'aws-amplify/data' ; import type { Schema } from '../amplify/data/resource' ; const client = generateClient < Schema > ( ) ; const { errors , data : newTodo } = await client . models . Todo . create ( Copy highlighted code example authMode : 'identityPool' , In addition, you can also use OpenID Connect with authenticated authorization. See OpenID Connect as an authorization provider ."}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching_pr-previews.txt", "text": "Fullstack previews With fullstack previews, you can set up ephemeral fullstack environments on every pull request. This allows you to test features in isolation from production. Once fullstack previews are enabled, your typical workflow would look like the following diagram: Your main (production branch) and featureA branch are deployed on Amplify. You and your team work on featureA until it's ready. The featureA branch is updated to main HEAD and then a pull request to main is opened. The pull request preview is deployed on Amplify and available at pr-1.appid.amplifyapp.com . Once the pull request is merged into main , the request is closed and the fullstack environment is also automatically torn down. Before you get started, make sure you have the following: A fullstack Amplify app deployed Ensure that your git repository is private. For security purposes, fullstack previews are disabled for public repositories with Amplify backend templates. To enable fullstack web previews for your Amplify app, follow these steps: Login to the Amplify console and select your app. Navigate to Hosting > Previews . Select the main branch and click on Edit settings . Click on the Pull request previews toggle button and choose Confirm to enable previews. Done! You have successfully enabled previews on the production branch. Ship updates to the dev branch. Now, when you create a pull request for the main branch, Amplify will build and deploy your fullstack PR and provide you with a preview URL. For GitHub repositories only , you can access your preview URL directly on the pull request from the Amplify Hosting's bot comment: After the pull request is merged or closed, the preview URL is deleted and any ephemeral fullstack environment is also deleted. Fullstack previews allow teams a way to preview changes from pull requests before merging code to a production branch. Pull requests let you tell others about changes you\u00e2\u0080\u0099ve pushed to a branch in a repository and the changes can be reviewed by accessing the preview URL. When previews are enabled on a git branch, by default every pull request created against the git branch creates an ephemeral fullstack environment. In some instances, you may not want to deploy new resources for every preview branch. For example, you might want all your preview branches to point to the backend resources deployed by the dev branch so you can reuse seed data, users, and groups. To achieve this, you can update your app build settings to reuse backend resources across your preview branches. In the Amplify console, select your app on the All apps page. From the App overview page, select Hosting > Build settings to view your app's build specification YAML file. Update the build settings for the backend phase to run npx ampx generate outputs --branch dev app-id $AWS_APP_ID to generate the amplify_outputs.json file for all preview branches. After this update, any new deployed preview branches will not deploy backend resources as part of the build and instead will use the deployed backend resources from"}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching_pr-previews.txt", "text": "the dev branch."}, {"source": "data/raw_pages/react_build-a-backend_data_working-with-files.txt", "text": "Working with files/attachments The Storage and GraphQL API categories can be used together to associate a file, such as an image or video, with a particular record. For example, you might create a User model with a profile picture, or a Post model with an associated image. With Amplify's GraphQL API and Storage categories, you can reference the file within the model itself to create an association. Set up your project by following the instructions in the Quickstart guide . Open amplify/data/resource.ts and add the following model as shown below: Next, Let's configure Storage and allow access to all authenticated(signed in) users of your application. create a file amplify/storage/resource.ts and add the following code,This will restrict file access to only the signed-in user. Configure the storage in the amplify/backend.ts file as demonstrated below: Your application needs authorization credentials for reading and writing to both Storage and the Data, except in the case where all data and files are intended to be publicly accessible. The Storage and Data categories govern data access based on their own authorization patterns, meaning that it's necessary to configure appropriate auth roles for each individual category. Although both categories share the same access credentials set up through the Auth category, they work independently from one another. For instance, adding an allow.authenticated() to the Data does not guard against file access in the Storage category. Likewise, adding authorization rules to the Storage category does not guard against data access in the API. When you configure Storage, Amplify will configure appropriate IAM policies on the bucket using a Cognito Identity Pool role. You will then have the option of adding CRUD (Create, Update, Read and Delete) based permissions as well, so that Authenticated and Guest users will be granted limited permissions within these levels. Even after adding this configuration, all Storage access is still guest by default. To guard against accidental public access, the Storage access levels must either be configured on the Storage object globally, or set within individual function calls. This guide uses the former approach, setting all Storage access to authenticated users. The ability to independently configure authorization rules for each category allows for more granular control over data access, and adds greater flexibility. For scenarios where authorization patterns must be mixed and matched, configure the access level on individual Storage function calls. For example, you may want to use entity_id CRUD access on an individual Storage function call for files that should only be accessible by the owner (such as personal files), authenticated read access to allow all logged in users to view common files (such as images in a shared photo album), and guest read access to allow all users to view a file (such as a public profile picture). For more details on how to configure Storage authorization levels, see the Storage documentation . For more on configuring Data authorization, see the API documentation . You can create a record via the Amplify Data client, upload a file to Storage, and finally"}, {"source": "data/raw_pages/react_build-a-backend_data_working-with-files.txt", "text": "update the record to associate it with the uploaded file. Use the following example with the Amplify Data client and Amplify Storage library helpers, uploadData and getUrl , to create a record and associate it the file with the record. The API record's id is prepended to the Storage file name to ensure uniqueness. If this is excluded, multiple API records could then be associated with the same file path unintentionally. To associate a file with a record, update the record with the path returned by the Storage upload. The following example uploads the file using Storage, updates the record with the file's path, then retrieves the signed URL to download the image. If an image is already associated with the record, this will update the record with the new image. To retrieve the file associated with a record, first query the record, then use Storage to get the signed URL. The signed URL can then be used to download the file, display an image, etc: There are three common deletion workflows when working with Storage files and the GraphQL API: Remove the file association, continue to persist both file and record. Remove the record association and delete the file. Delete both file and record. The following example removes the file association from the record, but does not delete the file from S3, nor the record from the database. The following example removes the file from the record, then deletes the file from S3: You may want to add multiple files to a single record, such as a user profile with multiple images. To do this, you can add a list of file keys to the record. The following example adds a list of file keys to a record: Add the following model in `amplify/data/resource.ts\" file. CRUD operations when working with multiple files is the same as when working with a single file, with the exception that we are now working with a list of image keys, as opposed to a single image key. First create a record via the GraphQL API, then upload the files to Storage, and finally add the associations between the record and files. To associate additional files with a record, update the record with the paths returned by the Storage uploads. Updating a file for an associated record is the same as updating a file for a single file record, with the exception that you will need to update the list of file keys. To retrieve the files associated with a record, first query the record, then use Storage to retrieve all of the signed URLs. The workflow for deleting and removing files associated with API records is the same as when working with a single file, except that when performing a delete you will need to iterate over the list of file paths and call Storage.remove() for each file. Remove the file association, continue to persist both files and record Remove the record association and delete the files Delete record and all associated files"}, {"source": "data/raw_pages/react_build-a-backend_data_working-with-files.txt", "text": "The recommended access patterns in these docs attempt to remove deleted files, but favor leaving orphans over leaving records that point to non-existent files. This optimizes for read latency by ensuring clients rarely attempt to fetch a non-existent file from Storage. However, any app that deletes files can inherently cause records on-device to point to non-existent files. One example is when we create an API record, associate the Storage file with that record, and then retrieve the file's signed URL . \"Device A\" calls the GraphQL API to create API_Record_1 , and then associates that record with First_Photo . Before \"Device A\" is about to retrieve the signed URL, \"Device B\" might query API_Record_1 , delete First_Photo , and update the record accordingly. However, \"Device A\" is still using the old API_Record_1 , which is now out-of-date. Even though the shared global state is correctly in sync at every stage, the individual device (\"Device A\") has an out-of-date record that points to a non-existent file. Similar issues can conceivably occur for updates. Depending on your app, some of these mismatches can be minimized even more with real-time data / GraphQL subscriptions . It is important to understand when these mismatches can occur and to add meaningful error handling around these cases. This guide does not include exhaustive error handling, real-time subscriptions, re-querying of outdated records, or attempts to retry failed operations. However, these are all important considerations for a production-level application. Single File (TS) Multi-File (TS)"}, {"source": "data/raw_pages/react_ai_concepts_tools.txt", "text": "Tools Large language models (LLMs) are stateless text generators, they have no knowledge of the real world and can't access data on their own. For example, if you asked an LLM \"what is the weather in San Jose?\" it would not be able to tell you because it does not know what the weather is today. Tools (sometimes referred to as function calling) are functions/APIs that LLMs can choose to invoke to get information about the world. This allows the LLM to answer questions with information not included in their training data like the weather, application-specific, and even user-specific data. When an LLM is prompted with tools, it can choose to respond by saying that it wants to call a tool to get some data or take an action on the user's behalf. That data is then added to the conversation history so the LLM can see what data was returned. Here is a simplified flow of what happens: User: \"what is the weather in san jose?\" Code: Call LLM with this message: \"what is the weather in san jose?\", and let it know it has access to a tool called getWeather that takes an input like { city: string } LLM: \"I want to call the 'getWeather' tool with the input {city: 'san jose'} \" Code: Run getWeather({city: 'san jose'}) and append the results to the conversation history so far and call the LLM again LLM: \"In san jose it is 72 degrees and sunny\" Note: the LLM itself is not actually executing any function or code. It responds with a special message saying that it wants to call that tool with specific input. That tool then needs to called and the results returned to the LLM in a message history. For more information on tools, see the Bedrock docs on tool use"}, {"source": "data/raw_pages/react_build-a-backend_auth_modify-resources-with-cdk.txt", "text": "Modify Amplify-generated Cognito resources with CDK Amplify Auth provides sensible defaults for the underlying Amazon Cognito resource definitions. You can customize your authentication resource to enable it to behave exactly as needed for your use cases by modifying it directly using AWS Cloud Development Kit (CDK) You can override the password policy by using the L1 cfnUserPool construct and adding a addPropertyOverride . While Email MFA is not yet supported with defineAuth , this feature can be enabled by modifying the underlying CDK construct. Start by ensuring your defineAuth resource configuration includes a compatible account recovery option and a custom SES sender. Next, extend the underlying CDK construct by activating Amazon Cognito's Advanced Security Features (ASF) and add EMAIL_OTP to the enabled MFA options. You can modify the underlying Cognito user pool resource to enable sign in with passwordless methods. Learn more about passwordless sign-in methods ."}, {"source": "data/raw_pages/react_reference_cli-commands.txt", "text": "CLI commands This page serves as a reference for commands found in the @aws-amplify/backend-cli package. All commands can be prefixed with AWS CLI environment variables to change the AWS account behavior with Amplify Gen 2 commands. Sandbox enables you to develop your backend alongside your frontend's development server. Run npx ampx sandbox to deploy to your personal cloud sandbox, this command will automatically watch for changes in the amplify/ folder, and redeploy each time you save a file. --stream-function-logs ( boolean ) - Whether to stream function execution logs. (default: false) --logs-filter ( array ) - Regex pattern to filter logs from only matched functions. E.g. to stream logs for a function, specify it's name, and to stream logs from all functions starting with auth specify 'auth' (default: Stream all logs) --logs-out-file ( string ) - File to append the streaming logs. The file is created if it does not exist. (default: stdout) --debug ( boolean ) - Print debug logs to the console (default: false) --dir-to-watch ( string ) - Directory to watch for file changes. All subdirectories and files will be included. Defaults to the amplify directory. --exclude ( string[] ) - An array of paths or glob patterns to ignore. Paths can be relative or absolute and can either be files or directories. --identifier ( string ) - An optional name to distinguish between different sandbox environments. Default is the name of the system user executing the process --once ( boolean ) - Execute a single sandbox deployment without watching for future file changes. --outputs-out-dir ( string ) - A path to a directory where the client config file is written. If not provided, defaults to the working directory of the current process. --outputs-format ( string ) - Format in which the client config file is written (choices: json , dart ). --outputs-version ( string ) - Version of the configuration. Version 0 represents classic amplify-cli config file amplify-configuration and 1 represents newer config file amplify_outputs (choices: 0 , 1 ). --profile ( string ) - An AWS profile name. Use with an alternate profile You can use the --profile flag to run sandbox with an AWS profile other than default : Additionally, you can use AWS CLI environment variables to specify a different profile: Use with an alternate Region Use AWS environment variables to deploy to a Region other than your AWS profile's configured Region: Use with mobile applications For mobile applications, you will need to set the output directory and format of the generated configuration file, specifically amplify_outputs.json : Delete your personal cloud sandbox. This should only be used if you have an active cloud sandbox that you opted to not delete when exiting npx ampx sandbox . --name ( string ) - An optional name to distinguish between different sandbox environments. Default is the name in your package.json. --profile ( string ) - An AWS profile name. -y, --yes ( boolean ) - Do not ask for confirmation before deleting the sandbox environment. Manage"}, {"source": "data/raw_pages/react_reference_cli-commands.txt", "text": "backend secrets used with your personal cloud sandbox. --profile ( string ) - An AWS profile name. Using with an alternate AWS profile You can use the --profile flag to run sandbox with an AWS profile other than default : Additionally, you can use AWS environment variables to specify a different profile: Creating a secret Create secrets for use with your personal cloud sandbox by using sandbox secret set : This is how you configure secrets to be retrieved and used within your backend using secret() . Removing a secret If you want to remove a secret you previously set, use sandbox secret remove : Removing all secrets If you want to remove all secrets you previously set, use sandbox secret remove --all : Listing secrets List all available secrets for your personal sandbox in the default AWS profile and Region: Get a secret and view its details You can view an existing secret and its details, such as the current version and when it was last updated: Generate is not intended to be used standalone; however, it does offer a few subcommands to generate information or code that is supplemental to your frontend development. Each of the following generate subcommands require either a CloudFormation stack name or an existing Amplify App ID and corresponding git branch: Generate the backend outputs file (e.g. amplify_outputs.json ) for your frontend application to consume. This is intended to be used to manually generate a configuration file for an environment other than your personal cloud sandbox. For example, you might use it if you would like to verify something your coworker is seeing in their cloud sandbox, or to demonstrate frontend changes locally using a pre-existing \"staging\" branch. In addition to the required options noted in ampx generate : --profile ( string ) - An AWS profile name. --format ( string ) - The format into which the configuration should be exported (choices: json , dart ). --out-dir ( string ) - A path to the directory where config is written. If not provided, it defaults to the working directory of the current process. --outputs-version ( string ) - Version of the configuration. Version 0 represents classic amplify-cli config file amplify-configuration and 1 represents newer config file amplify_outputs (choices: 0 , 1 ). As mentioned above, you can specify a team member's cloud sandbox CloudFormation stack: Use with mobile applications Similar to sandbox , you can specify an alternate outputs file format by using --format : Use with pre-existing branch If you have a pre-existing branch that you want to generate outputs for, you can use the --branch with the --app-id flag: Generate GraphQL statements and types for your frontend application to consume. The available parameters for npx ampx generate graphql-client-code are: Required parameters: Stack identifier --stack ( string ) - A stack name that contains an Amplify backend. Project identifier --app-id ( string ) - The Amplify App ID of the project. --branch ( string ) - A git branch of the Amplify"}, {"source": "data/raw_pages/react_reference_cli-commands.txt", "text": "project. Optional parameters: --out ( string ) - Specifies the path to the directory where the config is written. If not provided, defaults to the current process working directory. --format ( string ) (choices: modelgen , graphql-codegen , introspection ) - Specifies the format of the GraphQL client code to be generated. --model-target ( string ) (choices: java , swift , javascript , typescript , dart ) - Specifies the modelgen export target. Only applies when the --format parameter is set to modelgen. --statement-target ( string ) (choices: javascript , graphql , flow , typescript , angular ) - Specifies the graphql-codegen statement export target. Only applies when the --format parameter is set to graphql-codegen. --statement-max-depth ( integer ) - Specifies the maximum depth of the generated GraphQL statements. Only applies when the --format parameter is set to graphql-codegen. --type-target ( string ) (choices: json , swift , typescript , flow , scala , flow-modern , angular ) - Specifies the optional graphql-codegen type export target. Only applies when the --format parameter is set to graphql-codegen. --all ( boolean )- Shows hidden options. --profile ( string ) - Specifies an AWS profile name. --debug ( boolean ) - Print debug logs to the console. --help ( boolean ) - Displays help information about the command. Generate GraphQL client code using the Amplify App ID and branch. Generate GraphQL client code for a branch that is connected to Amplify Sometimes you want to test your latest local changes with the backend of another deployed branch. If you want to generate the GraphQL client code file(s) for the latest deployment of another branch, you can run the following command: Generate codegen for CDK app using a joint \"AmplifyBackendStack\" construct Assume you have deployed your Amplify project with the CDK construct. You will need to remember your app's project name (designated as the second parameter in your CDK construct) and stack name (designated as part of your npx cdk deploy context) Deployment command for CDK project Run Amplify codegen command to generate GraphQL codegen: Generate codegen in specific language and format Supported GraphQL client code combinations: Generates typescript data schema from a SQL database. --stack ( string ) - A stack name that contains an Amplify backend. --branch ( string ) - Name of the git branch being deployed. --app-id ( string ) - The app id of the target Amplify app. --out ( string ) - A path to directory where generated schema is written [default: ./amplify/data/schema.sql.ts ]. --connection-uri-secret ( string ) - Amplify secret name for the database connection uri. --ssl-cert-secret ( string ) - Amplify secret name for the database ssl certificate. --profile ( string ) - An AWS profile name. Generate React form components derived from your backend data models for your frontend application to consume. --stack ( string ) - A stack name that contains an Amplify backend. --branch ( string ) - Name of the git branch being deployed. --app-id ( string ) - The app id"}, {"source": "data/raw_pages/react_reference_cli-commands.txt", "text": "of the target Amplify app. --out-dir ( string ) - A path to directory where generated forms are written. Defaults to the ./ui-components directory. --models ( array ) - Model name to generate. --profile ( string ) - An AWS profile name. Generates information on system, binaries, npm packages, and environment variables for troubleshooting Amplify issues. This command will print system information as follows: Deploys the Amplify project in a CI/CD pipeline for a specified Amplify app and branch. --branch ( string ) - Name of the git branch being deployed. --app-id ( string ) - The app id of the target Amplify app. --outputs-out-dir ( string ) - A path to a directory where the client config file is written. If not provided, defaults to the working directory of the current process. --outputs-version ( string ) - Version of the configuration. Version 0 represents classic amplify-cli config file amplify-configuration and 1 represents newer config file amplify_outputs (choices: 0 , 1 )."}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_sign-up.txt", "text": "Sign-up Amplify provides a client library that enables you to interact with backend resources such as Amplify Auth. The quickest way to get started with Amplify Auth in your frontend application is with the Authenticator component , which provides a customizable UI and complete authentication flows. To get started, you can use the signUp() API to create a new user in your backend: The signUp API response will include a nextStep property, which can be used to determine if further action is required. It may return the following next steps: By default, each user that signs up remains in the unconfirmed status until they verify with a confirmation code that was sent to their email or phone number. The following are the default verification methods used when either phone or email are used as loginWith options. You can confirm the sign-up after receiving a confirmation code from the user: Your application's users can also sign up using passwordless methods. To learn more, visit the concepts page for passwordless ."}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_user-groups.txt", "text": "User groups Amplify Auth provides a mechanism that allows you to group users. Assigning users to groups enable you to customize access for a collection of users, or leverage for auditing purposes. For example, only \"ADMINS\" users are permitted to delete posts from a bulletin, or only \"EDITORS\" are permitted to modify posts in a \"draft\" state. To get started with groups, configure the groups property: Amplify resources enable you to define access for groups using common language. For example, you can use allow.groups in data: Or in storage: By defining access with groups, Amplify configures authorization rules to read from the current user's groups. User pool groups are available as a claim in the user's ID token and access token as cognito:groups . Requests can be made to secure resources using the access token and validated against this claim to permit action on the resource. Each Cognito user pool group is assigned an IAM role . IAM roles can be modified to extend access to other AWS resources. Roles can be accessed from your backend on the role property of your group:"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_configure-location-search.txt", "text": "Configure location search Amplify's geo category enables you to search by places, addresses, and coordinates in your app with \"place index\" resources. The pricing plan for Search Index will be set to RequestBasedUsage . We advice you to go through the location service pricing along with the location service terms ( 82.5 section ) to learn more about the pricing plan. You can optionally configure the data provider and result storage location for your location search index. You can select a data provider as the source for geocoding, reverse geocoding and searches. Each provider gathers and curates their data using different means. They may also have varying expertise in different regions of the world. The available data providers of geospatial data are shown. To learn more about data providers, please refer this location service documentation . Here \u00e2\u0080\u0093 For additional information about HERE Technologies, see Here guide . Esri \u00e2\u0080\u0093 For additional information about Esri, see Esri guide . Note: If your application is tracking or routing assets you use in your business (such as delivery vehicles or employees), you may only use HERE as your geolocation provider. See section 82 of the AWS service terms for more details. You can specify how the results of a search operation will be stored by the caller. SingleUse - specifies that the results won't be stored. Storage - specifies that the result can be cached or stored in a database. Refer this location service doc for more information."}, {"source": "data/raw_pages/react_build-a-backend_functions.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_rest-api_set-up-http-api.txt", "text": "Set up Amplify HTTP API Using the AWS Cloud Development Kit (AWS CDK) , you can configure Amplify Functions as resolvers for routes of an HTTP API powered by Amazon API Gateway . To get started, create a new directory and a resource file, amplify/functions/api-function/resource.ts . Then, define the function with defineFunction : Then, create the corresponding handler file, amplify/functions/api-function/handler.ts , file with the following contents: Next, using the AWS CDK, create an HTTP API in your backend file: Use the package manager of your choice to install the Amplify JavaScript library. For example, with npm : To initialize the Amplify API category you need to configure Amplify with Amplify.configure() . Import and load the configuration file in your app. It's recommended you add the Amplify configuration step to your app's root entry point. For example src/main.ts : Make sure you call Amplify.configure as early as possible in your application\u00e2\u0080\u0099s life-cycle. A missing configuration or NoCredentials error is thrown if Amplify.configure has not been called before other Amplify JavaScript APIs. Review the Library Not Configured Troubleshooting guide for possible causes of this issue."}, {"source": "data/raw_pages/react_build-a-backend_functions_examples_s3-upload-confirmation.txt", "text": "S3 Upload confirmation You can use defineStorage and defineFunction to create a function trigger to confirm uploading a file. To get started, install the @types/aws-lambda package , which contains types for different kinds of Lambda handlers, events, and responses. Update your storage definition to define the onUpload trigger as below: Next, create a file named amplify/storage/on-upload-handler.ts and use the following code to log the object keys whenever an object is uploaded to the bucket. You can add your custom logic to this function as needed. Now, when you deploy your backend, this function will be invoked whenever an object is uploaded to the bucket."}, {"source": "data/raw_pages/react_build-a-backend_auth_grant-access-to-auth-resources.txt", "text": "manageUsers Grants CRUD access to users in the UserPool cognito-idp:AdminConfirmSignUp cognito-idp:AdminCreateUser cognito-idp:AdminDeleteUser cognito-idp:AdminDeleteUserAttributes cognito-idp:AdminDisableUser cognito-idp:AdminEnableUser cognito-idp:AdminGetUser cognito-idp:AdminListGroupsForUser cognito-idp:AdminRespondToAuthChallenge cognito-idp:AdminSetUserMFAPreference cognito-idp:AdminSetUserSettings cognito-idp:AdminUpdateUserAttributes cognito-idp:AdminUserGlobalSignOut manageGroupMembership Grants permission to add and remove users from groups cognito-idp:AdminAddUserToGroup cognito-idp:AdminRemoveUserFromGroup manageGroups Grants CRUD access to groups in the UserPool cognito-idp:GetGroup cognito-idp:ListGroups cognito-idp:CreateGroup cognito-idp:DeleteGroup cognito-idp:UpdateGroup manageUserDevices Manages devices registered to users cognito-idp:AdminForgetDevice cognito-idp:AdminGetDevice cognito-idp:AdminListDevices cognito-idp:AdminUpdateDeviceStatus managePasswordRecovery Grants permission to reset user passwords cognito-idp:AdminResetUserPassword cognito-idp:AdminSetUserPassword addUserToGroup Grants permission to add any user to any group. cognito-idp:AdminAddUserToGroup createUser Grants permission to create new users and send welcome messages via email or SMS. cognito-idp:AdminCreateUser deleteUser Grants permission to delete any user cognito-idp:AdminDeleteUser deleteUserAttributes Grants permission to delete attributes from any user cognito-idp:AdminDeleteUserAttributes disableUser Grants permission to deactivate any user cognito-idp:AdminDisableUser enableUser Grants permission to activate any user cognito-idp:AdminEnableUser forgetDevice Grants permission to deregister any user's devices cognito-idp:AdminForgetDevice getDevice Grants permission to get information about any user's devices cognito-idp:AdminGetDevice getUser Grants permission to look up any user by user name listUsers Grants permission to list users and their basic details in the UserPool listDevices Grants permission to list any user's remembered devices cognito-idp:AdminListDevices listGroupsForUser Grants permission to list the groups that any user belongs to cognito-idp:AdminListGroupsForUser listUsersInGroup Grants permission to list users in the specified group cognito-idp:ListUsersInGroup removeUserFromGroup Grants permission to remove any user from any group cognito-idp:AdminRemoveUserFromGroup resetUserPassword Grants permission to reset any user's password cognito-idp:AdminResetUserPassword setUserMfaPreference Grants permission to set any user's preferred MFA method cognito-idp:AdminSetUserMFAPreference setUserPassword Grants permission to set any user's password cognito-idp:AdminSetUserPassword setUserSettings Grants permission to set user settings for any user cognito-idp:AdminSetUserSettings updateDeviceStatus Grants permission to update the status of any user's remembered devices cognito-idp:AdminUpdateDeviceStatus updateUserAttributes Grants permission to updates any user's standard or custom attributes cognito-idp:AdminUpdateUserAttributes"}, {"source": "data/raw_pages/react_build-ui.txt", "text": "Build UI Amplify offers a UI Library that makes it easy to build web app user interfaces that are connected to the backend. Amplify UI offers: Connected components that are designed to work seamlessly with AWS Amplify backend services, allowing you to quickly add common UX patterns for authentication, storage etc. without having to build them from scratch. Tooling that generates React forms over data, and React components from Figma designs."}, {"source": "data/raw_pages/react_build-a-backend_troubleshooting.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_configure-geofencing.txt", "text": "Configure a geofence collection A Geofence is a virtual perimeter for a real-world geographic area. A Geofence contains points or vertices that form a closed boundary, defining an area of interest. Geofence collections store one or multiple Geofences. The pricing plan for the Geofence Collection will be set to RequestBasedUsage . We advice you to go through the location service pricing along with the location service terms ( 82.5 section ) to learn more about the pricing plan. Group access To scope access permissions based on Cognito User Groups Create a Cognito User Pool Group Add permissions to the Cognito User Pool Group role Note: If you combine Auth/Guest user access and Individual Group access , users who are members of a group will only be granted the permissions of the group, and not the authenticated user permissions. The permissions apply to ALL Geofences in a collection. For example, If you add Read permission such as ListGeofences and GetGeofence to User Cognito group, ALL users added to that group will be able to read the properties of ALL Geofences in that Geofence collection. Using the AWS SDK for Javascript Alternatively, if you want to add users to an existing Cognito user pool group programmatically, you can use the AWS SDK for Javascript. Refer to the API documentation . Note: After you have provisioned the Geofence Collection, depending on your application's use-case, you can also add Geofences to the provisioned Geofence Collection programmatically. Refer this API documentation for more information."}, {"source": "data/raw_pages/react_build-a-backend_data.txt", "text": "Create, update, and delete application data Mutate application data in an API by generating the client, adding items, updating existing items, deleting items, troubleshooting unauthorized errors, and canceling requests."}, {"source": "data/raw_pages/react_build-a-backend_functions_examples_override-token.txt", "text": "Override ID token claims You can use defineAuth and defineFunction to create an Amazon Cognito Pre token generation AWS Lambda trigger to override the token by adding a new claim or modifying the user's group membership. To get started, install the aws-lambda package, which is used to define the handler type. Create a new directory and a resource file, amplify/auth/pre-token-generation/resource.ts . Then, define the function with defineFunction : Then, create the corresponding handler file, amplify/auth/post-confirmation/pre-token-generation/handler.ts , file with the following contents: Lastly, set the newly created function resource on your auth resource: After deploying the changes, The idToken of the user will be modified as per the trigger above."}, {"source": "data/raw_pages/react_build-a-backend_functions_examples_email-domain-filtering.txt", "text": "Email domain filtering You can use defineAuth and defineFunction to create a Cognito pre sign-up Lambda trigger that performs filtering based on the user's email address. This can allow or deny user signups based on their email address. To get started, install the aws-lambda package, which is used to define the handler type. Next, create a new directory and a resource file, amplify/auth/pre-sign-up/resource.ts . Then, define the Function with defineFunction : Next, create the corresponding handler file, amplify/auth/pre-sign-up/handler.ts , file with the following contents: Lastly, set the newly created Function resource on your auth resource: After deploying the changes, whenever a user attempts to sign up without an amazon.com email address they will receive an error."}, {"source": "data/raw_pages/react_build-a-backend_functions_modify-resources-with-cdk.txt", "text": "Modify Amplify-generated Lambda resources with CDK Amplify Functions utilize the NodejsFunction construct from the AWS Cloud Development Kit (CDK) . The underlying resources can be modified, overridden, or extended using CDK after setting the resource on your backend. amplify/backend.ts Copy amplify/backend.ts code example import { defineBackend } from '@aws-amplify/backend' ; import { myFunction } from './functions/my-function' ; const backend = defineBackend ( { // CDK constructs can be accessed via backend . myFunction . resources // where the Lambda function can be found on backend . myFunction . resources . lambda The Lambda resource available is a representation of IFunction . To learn how to add IAM policies to a Function's execution role, visit the documentation for granting access to other resources ."}, {"source": "data/raw_pages/react_reference_project-structure.txt", "text": "Project structure Amplify Gen 2 backends are defined using TypeScript, and enable you to collocate resources depending on their function. For example, you can author a post confirmation trigger for Amazon Cognito that creates a UserProfile model right next to your auth's resource file. When you create your first Amplify project using npm create amplify@latest , it will automatically set up the scaffolding for Data and Authentication resources: As your project grows and you build out your backend, the structure of your project may look like the following: Backend resources are defined in resource files using the define* helpers: After the resources are defined, they are set up on the backend: You can extend backends by using the AWS Cloud Development Kit (AWS CDK) , which is installed by default as part of the create-amplify workflow. With the CDK, you can build using any AWS service, such as an Amazon S3 bucket that authenticated users have read and write access to. To get started with the CDK, add it to your backend:"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_interactions.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/contribute.txt", "text": "AWS Amplify Contributor Program Get involved with AWS Amplify by making open source contributions to the project. Making a Contribution How it works The Amplify Contributor Program is open to everyone in the Amplify community! In the following steps, we describe what to expect when contributing: The Amplify team labels issues using good first issue for contributors. You open a pull request that contains commits you made yourself. The Amplify team reviews your pull request. If everything looks good, the Amplify team approves the pull request. (At this time, only accepted pull requests count toward earning badges.) You collect an Amplify badge for your first contribution or make progress toward the Intermediate and Advanced badges! The Amplify Badge Program The Amplify Badge Program celebrates your contributions by offering you exclusive, eye-catching badges to display on your online profiles, such as LinkedIn, GitHub, or your own website. Quickstart videos Follow along with these videos to learn how to set up your local environment to get started making open source contributions to the Amplify project."}, {"source": "data/raw_pages/react_build-a-backend_data_optimistic-ui.txt", "text": "Optimistic UI Amplify Data can be used with TanStack Query to implement optimistic UI, allowing CRUD operations to be rendered immediately on the UI before the request roundtrip has completed. Using Amplify Data with TanStack additionally makes it easy to render loading and error states, and allows you to rollback changes on the UI when API calls are unsuccessful. In the following examples we'll create a list view that optimistically renders newly created items, and a detail view that optimistically renders updates and deletes. To get started, run the following command in an existing Amplify project with a React frontend: Modify your Data schema to use this \"Real Estate Property\" example: Save the file and run npx ampx sandbox to deploy the changes to your backend cloud sandbox. For the purposes of this guide, we'll build a Real Estate Property listing application. Next, at the root of your project, add the required TanStack Query imports, and create a client: TanStack Query Devtools are not required, but are a useful resource for debugging and understanding how TanStack works under the hood. By default, React Query Devtools are only included in bundles when process.env.NODE_ENV === 'development' , meaning that no additional configuration is required to exclude them from a production build. For more information on the TanStack Query Devtools, visit the TanStack Query Devtools docs For the complete working example, including required imports and React component state management, see the Complete Example below. TanStack Query manages query caching based on the query keys you specify. A query key must be an array. The array can contain a single string or multiple strings and nested objects. The query key must be serializable, and unique to the query's data. When using TanStack to render optimistic UI with Amplify Data, you must use different query keys depending on the API operation. When retrieving a list of items, a single string is used (e.g. queryKey: [\"realEstateProperties\"] ). This query key is also used to optimistically render a newly created item. When updating or deleting an item, the query key must also include the unique identifier for the record being deleted or updated (e.g. queryKey: [\"realEstateProperties\", newRealEstateProperty.id] ). For more detailed information on query keys, see the TanStack Query documentation . To optimistically render a list of items returned from the Amplify Data API, use the TanStack useQuery hook, passing in the Data API query as the queryFn parameter. The following example creates a query to retrieve all records from the API. We'll use realEstateProperties as the query key, which will be the same key we use to optimistically render a newly created item. To optimistically render a newly created record returned from the Amplify Data API, use the TanStack useMutation hook, passing in the Amplify Data API mutation as the mutationFn parameter. We'll use the same query key used by the useQuery hook ( realEstateProperties ) as the query key to optimistically render a newly created item. We'll use the onMutate function to update the cache"}, {"source": "data/raw_pages/react_build-a-backend_data_optimistic-ui.txt", "text": "directly, as well as the onError function to rollback changes when a request fails. import { generateClient } from 'aws-amplify/api' import type { Schema } from '../amplify/data/resource' Copy highlighted code example import { useQueryClient , useMutation } from '@tanstack/react-query' const client = generateClient < Schema > ( ) Copy highlighted code example const queryClient = useQueryClient ( ) ; Copy highlighted code example const createMutation = useMutation ( { mutationFn : async ( input : { name : string , address : string } ) => { const { data : newRealEstateProperty } = await client . models . RealEstateProperty . create ( input ) return newRealEstateProperty ; onMutate : async ( newRealEstateProperty ) => { await queryClient . cancelQueries ( { queryKey : [ \"realEstateProperties\" ] } ) ; const previousRealEstateProperties = queryClient . getQueryData ( [ if ( previousRealEstateProperties ) { queryClient . setQueryData ( [ \"realEstateProperties\" ] , ( old : Schema [ \"RealEstateProperty\" ] [ \"type\" ] [ ] ) => [ return { previousRealEstateProperties } ; onError : ( err , newRealEstateProperty , context ) => { console . error ( \"Error saving record:\" , err , newRealEstateProperty ) ; if ( context ?. previousRealEstateProperties ) { queryClient . setQueryData ( [ \"realEstateProperties\" ] , context . previousRealEstateProperties queryClient . invalidateQueries ( { queryKey : [ \"realEstateProperties\" ] } ) ; To optimistically render updates on a single item, we'll first retrieve the item from the API. We'll use the useQuery hook, passing in the get query as the queryFn parameter. For the query key, we'll use a combination of realEstateProperties and the record's unique identifier. import { generateClient } from 'aws-amplify/data' import type { Schema } from '../amplify/data/resource' import { useQuery } from '@tanstack/react-query' const client = generateClient < Schema > ( ) const currentRealEstatePropertyId = \"SOME_ID\" Copy highlighted code example data : realEstateProperty , queryKey : [ \"realEstateProperties\" , currentRealEstatePropertyId ] , if ( ! currentRealEstatePropertyId ) { return } const { data : property } = await client . models . RealEstateProperty . get ( { id : currentRealEstatePropertyId , To optimistically render Amplify Data updates for a single record, use the TanStack useMutation hook, passing in the update mutation as the mutationFn parameter. We'll use the same query key combination used by the single record useQuery hook ( realEstateProperties and the record's id ) as the query key to optimistically render the updates. We'll use the onMutate function to update the cache directly, as well as the onError function to rollback changes when a request fails. When directly interacting with the cache via the onMutate function, the newRealEstateProperty parameter will only include fields that are being updated. When calling setQueryData , include the previous values for all fields in addition to the newly updated fields to avoid only rendering optimistic values for updated fields on the UI. To optimistically render a deletion of a single record, use the TanStack useMutation hook, passing in the delete mutation as the mutationFn parameter. We'll"}, {"source": "data/raw_pages/react_build-a-backend_data_optimistic-ui.txt", "text": "use the same query key combination used by the single record useQuery hook ( realEstateProperties and the record's id ) as the query key to optimistically render the updates. We'll use the onMutate function to update the cache directly, as well as the onError function to rollback changes when a delete fails. Both useQuery and useMutation return isLoading and isError states that indicate the current state of the query or mutation. You can use these states to render loading and error indicators. In addition to operation-specific loading states, TanStack Query provides a useIsFetching hook . For the purposes of this demo, we show a global loading indicator in the Complete Example when any queries are fetching (including in the background) in order to help visualize what TanStack is doing in the background: For more details on advanced usage of TanStack Query hooks, see the TanStack documentation . The following example demonstrates how to use the state returned by TanStack to render a loading indicator while a mutation is in progress, and an error message if the mutation fails. For additional examples, see the Complete Example below."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_analytics_streaming-data.txt", "text": "Streaming analytics data The Amazon Kinesis analytics provider allows you to send analytics data to an Kinesis stream for real-time processing. The following is an example utilizing the AWS Cloud Development Kit (AWS CDK) to create the Analytics resource powered by Amazon Kinesis . If you did not use the CLI, ensure you have setup IAM permissions for kinesis:PutRecords . Example IAM policy for Amazon Kinesis: For more information visit the Amazon Kinesis Developer Documentation . Configure Kinesis: You can send a data to a Kinesis stream with the standard record() method: The recorded events are saved in a buffer and sent to the remote server periodically (You can tune it with the flushInterval option) . If needed, you have the option to manually clear all the events from the buffer by using the 'flushEvents' API."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging_identify-user.txt", "text": "Identify a user To fully harness the potential of In-App Messaging, you must segment and target your In-App Messaging campaigns to specific user subsets. By identifying users with additional information, including their device demographics, location and any attributes of your choosing, you will be able to display intelligent, targeted in-app messages to the right users. When using identifyUser with Amazon Pinpoint, in addition to the other user info properties you can configure the address , optOut , and userAttributes properties under options ."}, {"source": "data/raw_pages/react_build-a-backend_data_field-level-validation.txt", "text": "Field-level validation You can enable field-level validation in your model schema by chaining a validate function to the field. For string fields: Note: Our schema transformer uses the Java regex engine under the hood. Because of how TypeScript processes string literals, you must quadruple-escape special regex characters in your schema. In a TypeScript string literal, writing \\\\\\\\s produces the string \\\\s , which is the correct form for the Java regex engine. If you write \\\\s , it produces \\s , which is invalid. Therefore, for the matches validator, ensure you use quadruple-escaping. For example: a.string().validate(v => v.matches(\"^[a-zA-Z0-9\\\\\\\\s]+$\", 'Content must contain only letters, numbers, and spaces')) For integer and float fields: Note: Currently, we only support validation on non-array fields of type string , integer , and float ."}, {"source": "data/raw_pages/react_build-a-backend_storage_manage-with-amplify-console.txt", "text": "Manage files with Amplify console The File storage page in the Amplify console provides a user-friendly interface for managing your application's backend file storage. It allows for efficient testing and management of your files. If you have not yet created a storage resource, visit the Storage setup guide . After you've deployed your storage resource, you can access the manager on Amplify Console. Log in to the Amplify console and choose your app. Select the branch you would like to access. Select Storage from the left navigation bar. On the Storage page, select the Upload button Select the file you would like to upload and then select Done Alternatively, you can Drag and drop a file onto the Storage page. On the Storage page, select the file you want to delete. Select the Actions dropdown and then select Delete . On the Storage page, select the file you want to copy. Select the Actions dropdown and then select Copy to . Select or create the folder you want a copy of your file to be saved to. Select Copy to copy your file to the selected folder. On the Storage page, select the file you want to move. Select the Actions dropdown and then select Move to . Select or create the folder you want to move your file to. Select Move to move your file to the selected folder."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_rest-api_existing-resources.txt", "text": "Use existing AWS resources Existing Amazon API Gateway resources can be used with the Amplify Libraries by calling Amplify.configure() with the API Gateway API name and options. Note, you will need to parse the Amplify configuration using parseAmplifyConfig before calling Amplify.configure() . The following example shows how to configure additional API Gateway resources to an existing Amplify application: YourAPIName : Friendly name for the API endpoint : The HTTPS endpoint of the API region : AWS Region where the resources are provisioned. If not specified, the region will be inferred from the endpoint. Note that before you can add an AWS resource to your application, the application must have the Amplify libraries installed. If you need to perform this step, see Install Amplify Libraries ."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_existing-resources.txt", "text": "Use existing Amazon Location resources To use existing Amazon Location Services resources with your Amplify backend or frontend application, use the addOutput method to surface backend resource outputs to the amplify_outputs.json file: To use your existing Amazon Location Service resources (i.e. maps and place indices) with Amplify Geo, you need to ensure your role has the right authorization permissions through Cognito. Note: Here is a guide on Creating an Amazon Cognito identity pool for use with Amazon Location Service There are two roles created by Cognito: an \"authenticated role\" that grants signed-in-user-level access and an \"unauthenticated role\" that allows unauthenticated access to resources. Attach the following policies for the appropriate resources and roles (Auth and/or Unauth). Replace {region} , {account-id} , and {enter Map/PlaceIndex name} with the correct items. Note that certain actions cannot be performed with unauthenticated access. The list of actions allowed for the Unauth role is in the Granting access to Amazon Location Service guide . You can first import and parse the generated amplify_outputs.json . You can then manually configure Amplify Geo like this: Now you can proceed to displaying a map or adding location search to your app."}, {"source": "data/raw_pages/react_ai_conversation_connect-your-frontend.txt", "text": "Connect your frontend In this guide, you will learn how to create, update, and delete conversations, as well as send messages and subscribe to assistant responses. Conversations and their associated messages are persisted in Amazon DynamoDB. This means the previous messages for a conversation are automatically included in the history sent to the LLM. Access to conversations and messages are scoped to individual users through the owner based authorization strategy . There are two main types within the conversation flow, Conversation and Message . A Conversation is an instance of a chat session between an application user and an LLM. It contains data and methods for interacting with the conversation. A conversation has a one-to-many relationship with its messages. The Conversation type is accessible via Schema['myChat']['type'] type definition, where 'myChat' is the name of the conversation route in your data schema. A Message is a single chat message between an application user and an LLM. Each message has a role property that indicates whether the message is from the user or the assistant. User and assistant messages have a one-to-one relationship. Assistant messages contain an associatedUserMessageId property that points to the id of the user message that triggered the assistant response. The Message type is accessible via Schema['myChat']['messageType'] , where 'myChat' is the name of the conversation route in your data schema. Create a new conversation with .create() or get an existing one with .get() . Subscribe to assistant responses for a conversation with .onStreamEvent() . Send messages to the conversation with .sendMessage() . Create a new conversation by calling the .create() method on your conversation route. In the examples below, we're using a conversation route named chat . You can optionally attach a name and metadata to a conversation by passing them as arguments to the .create() method. There are no uniqueness constraints on conversation name or metadata values. You can get an existing conversation by calling the .get() method on your conversation route with the conversation's id . You can list all conversations for a user with the .list() method. Retrieved conversations are sorted by updatedAt in descending order. This means the most recently used conversations are returned first. Use the nextToken value to paginate through conversations and optionally specify a limit to limit the number of conversations returned. You can update a conversation's name and metadata with the .update() method. This is useful if you want to update the conversation name based on the messages sent or attach arbitrary metadata at a later time. Deleting a conversation makes it unusable in the future. However it does not delete its associated messages. Once you have a conversation instance, you can interact with it by calling methods on the instance. These methods are documented in the Conversation and Message types section. Once you have a conversation instance, you can send a message to the AI assistant by calling the .sendMessage() method. In its simplest form you just pass the message content as text. The message returned is the user"}, {"source": "data/raw_pages/react_ai_conversation_connect-your-frontend.txt", "text": "message sent. Assistant messages are streamed back to the client and can be subscribed to with the .onStreamEvent() method. See Subscribe to assistant responses for more information. There are other arguments you can pass to .sendMessage() to customize the message according to your application's needs. Customizing the message content sendMessage() accepts a object type with a content property that provides a flexible way to send different types of content to the AI assistant. Image Content Use image to send an image to the AI assistant. Supported image formats are png , gif , jpeg , and webp . Mixing text and image in a single message is supported. AI context The aiContext argument allows you to optionally attach arbitrary data to the message. This is useful for passing additional information, like user information or current state of your application, in a user message to the AI assistant. The toolConfiguration argument allows you to optionally pass a client tool configuration to the AI assistant with a user message. See the Tools concept page and Tools guide for more information on how tools works. Client tools are conceptually the same as data tools and lambda executable tools. They are API definitions provided to an LLM alongside a user message. The LLM can use the provided tool configuration to decide which tool (if any) to call in order to better respond to the user. However, there's an important distinction with client tools -- you are responsible for implementing the tool execution logic and responding to the AI assistant with the tool's response. The json property is simply a JSON Schema definition of the tool's input. The AI assistant will use this schema to provide the expected input to your tool. The response from the AI assistant will be a JSON object that matches the inputSchema definition. See Subscribe to assistant responses for more information on how to handle the response. Assistant responses are streamed back to the client as they are generated. This allows for a more natural conversation flow where the user doesn't have to wait for a complete response from the AI assistant to see progress and begin reading the response. To subscribe to assistant responses, call the .onStreamEvent() method on your conversation instance. onStreamEvent() takes two callback functions as arguments: next and error . The next callback is invoked with each assistant response. The error callback is invoked if there's an error while processing messages. The next callback is invoked with a ConversationStreamEvent object. This type is accessible via Schema['myChat']['streamEventType'] and is a union of the following types: ConversationStreamTextEvent As text is streamed back to the client, the next callback is invoked with a ConversationStreamTextEvent object. ConversationStreamDoneAtIndexEvent When the AI assistant completes a content block, the next callback is invoked with a ConversationStreamDoneAtIndexEvent object. ConversationStreamTurnDoneEvent When the AI assistant completes a turn, the next callback is invoked with a ConversationStreamTurnDoneEvent object. This event indicates that the assistant has completed a turn and is waiting for the next user message. When"}, {"source": "data/raw_pages/react_ai_conversation_connect-your-frontend.txt", "text": "the AI assistant uses a client tool, the next callback is invoked with a ConversationStreamToolUseEvent object. Tool use events are accumulated in your cloud resources and sent to the client as a single event. A note on ordering There are no guarantees that events will be received by the client in order. For example, a ConversationStreamTextEvent with contentBlockDeltaIndex of 1 may be received before the preceding text with contentBlockDeltaIndex of 0 . Assume that events may be received out of order and use the contentBlockIndex and contentBlockDeltaIndex properties to order the events as needed. Retrieve all messages for a conversation by calling the .listMessages() method on your conversation instance. Recall that messages are automatically persisted, so you can retrieve them at any time to display the conversation history. Similar to the client.conversations.chat.list() method, retrieved messages are paginated. Use the nextToken value to paginate through messages and optionally specify a limit to limit the number of messages returned."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_predictions_label-image.txt", "text": "Label objects in an image Note: Make sure to complete the getting started section first, where you will set up the IAM roles with the right policy actions Detect labels, such if an image has a desk or a chair in it Detect unsafe content in an image For both labels and unsafe content"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-from-server-runtime_nuxtjs-server-runtime.txt", "text": "Nuxt.js server runtime This guide walks through how you can connect to Amplify Data from Nuxt.js Server-side Runtime (SSR). For Nuxt.js applications, Amplify provides first-class support for Routing (Pages) , API Routes , and Middleware . Before you begin, you will need: Connecting to Amplify Data will include setting up the AmplifyAPIs Plugin with the runWithAmplifyServerContext adapter, using the useNuxtApp() composable, setting up the Amplify server context utility and then using the runAmplifyApi function to call the API in an isolated server context. Nuxt 3 offers universal rendering by default, where your data fetching logic may be executed on both the client and server sides. Amplify offers APIs that are capable of running within a server context to support use cases such as server-side rendering (SSR) and static site generation (SSG), though Amplify's client-side APIs and server-side APIs of Amplify are slightly different. You can set up an AmplifyAPIs plugin to make your data fetching logic run smoothly across the client and server. To learn more about how to use Amplify categories APIs in server side rendering, refer to this documentation . Create a plugins directory under the root of your Nuxt project. Create two files 01.amplify-apis.client.ts and 01.amplify-apis.server.ts under the plugins directory. In these files, you will register both client-specific and server-specific Amplify APIs that you will use in your Nuxt project as a plugin. You can then access these APIs via the useNuxtApp composable. Modify the 01.amplify-apis.client.ts file, with the following code: Expand to view the code implementation Make sure you call Amplify.configure as early as possible in your application\u00e2\u0080\u0099s life-cycle. A missing configuration or NoCredentials error is thrown if Amplify.configure has not been called before other Amplify JavaScript APIs. Review the Library Not Configured Troubleshooting guide for possible causes of this issue. Close accordion Next, modify the 01.amplify-apis.server.ts file, with the following code: Expand to view the code implementation Close accordion Using the GraphQL API in ~/app.vue : The app.vue file can be rendered on both the client and server sides by default. The useNuxtApp().$Amplify composable will pick up the correct implementation of 01.amplify-apis.client.ts and 01.amplify-apis.server.ts to use, depending on the runtime. Only a subset of Amplify APIs are usable on the server side, and as the libraries evolve, amplify-apis.client and amplify-apis.server may diverge further. You can guard your API calls to ensure an API is available in the context where you use it. E.g., you can use if (process.client) to ensure that a client-only API isn't executed on the server. Following the specification of Nuxt, your API route handlers will live under ~/server , which is a separate environment from other parts of your Nuxt app; hence, the plugins created in the previous step are not usable here, and extra work is required. Setup Amplify Server Context Utility Create a utils directory under the server directory of your Nuxt project. Create an amplifyUtils.ts file under the utils directory. In this file, you will create a helper function to call Amplify APIs that are capable of running"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-from-server-runtime_nuxtjs-server-runtime.txt", "text": "on the server side with context isolation. Modify the amplifyUtils.ts file, with the following code: Expand to view the code implementation Close accordion Now, you can use the runAmplifyApi function to call Amplify APIs in an isolated server context. Create a new API route /api/current-user in the server directory and modify the current-user.ts file, with the following code: You can then fetch data from this API route, for example: fetch('http://localhost:3000/api/current-user')"}, {"source": "data/raw_pages/react_build-a-backend_functions_add-lambda-layers.txt", "text": "Lambda Layers Amplify offers the ability to add layers to your functions which contain your library dependencies. Lambda layers allow you to separate your function code from its dependencies, enabling easier management of shared components across multiple functions and reducing deployment package sizes. Note: Configuring or adding layers in defineFunction is not supported for Custom Functions . To add a Lambda layer to your function, follow these steps: First, create and set up your Lambda layer in AWS. You can do this through the AWS Console or using the AWS CLI. For guidance on creating layers, refer to the AWS documentation on creating Lambda layers . Once your layer is created and available in AWS, you can reference it in your Amplify project as shown below. Specify the layers property in defineFunction , for example: The Lambda layer is represented by an object of key/value pairs where the key is the module name that is exported from your layer and the value is the ARN of the layer. The key (module name) is used to externalize the module dependency so it doesn't get bundled with your Lambda function. A maximum of 5 layers can be attached to a function, and they must be in the same region as the function. Alternatively, you can specify the layer as myLayer:1 where myLayer is the name of the layer and 1 is the version of the layer. For example: Amplify will automatically convert this to the full layer ARN format arn:aws:lambda:<region>:<account-id>:layer:myLayer:1 using your existing account ID and region. When using layers, be mindful of versioning. The ARN includes a version number (e.g., :12 in the example). Ensure you're using the appropriate version and have a strategy for updating layers when new versions are released. Then use the locally installed module in the function handler: For further information on creating and managing your layers refer to AWS documentation for Lambda layers"}, {"source": "data/raw_pages/react_ai_concepts.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_connect-amazon-translate.txt", "text": "Connect to Amazon Translate for language translation APIs Amazon Translate is a neural machine translation service provided by Amazon Web Services (AWS). It uses advanced deep learning technologies to deliver fast and high-quality language translation. With Amazon Translate, you can easily add multilingual support to your applications and services, enabling users to communicate and interact in their preferred language. Key features of Amazon Translate include: Accurate and Fluent Translations : Amazon Translate produces translations that are both accurate and natural-sounding, providing a seamless experience for users. Support for Multiple Languages : The service supports a broad range of languages, allowing you to expand your application\u00e2\u0080\u0099s reach to diverse audiences around the world. Real-Time and Batch Translation : Amazon Translate can handle real-time translation for dynamic content and batch translation for larger volumes of text, making it suitable for various use cases. Cost-Effective and Scalable : With its pay-as-you-go pricing model and automatic scaling, Amazon Translate is an economical and flexible solution for adding translation capabilities to your applications. In this section, you will learn how to integrate Amazon Translate into your application using AWS Amplify, enabling you to leverage its powerful translation capabilities effortlessly. Set up your project by following the instructions in the Quickstart guide . To install the Amazon Translate SDK, run the following command in your project's root folder: To access Amazon Translate service, you need to add Amazon Translate as an HTTP Data Source and configure the proper IAM policy for AWS Lambda to utilize the desired feature effectively. Update amplify/backend.ts file as shown below. Next, create the following translate.js file in your amplify/data folder and use the code below to define custom resolvers. After adding Amazon Translate as a data source, you can reference it in a custom query using the a.handler.custom() modifier, which takes the name of the data source and an entry point for your resolvers. In your amplify/data/resource.ts file, specify TranslateDataSource as the data source and translate.js as the entry point, as shown below. Import and load the configuration file in your app. It's recommended you add the Amplify configuration step to your app's root entry point. Sample frontend code to translate text from one language to another."}, {"source": "data/raw_pages/react_build-a-backend_functions_configure-functions.txt", "text": "Configure Functions defineFunction comes out-of-the-box with sensible but minimal defaults. The following options are provided to tweak the function configuration. Note: The following options are not supported for Custom Functions except for resourceGroupName . By default, functions are named based on the directory the defineFunction call is placed in. In the above example, defining the function in amplify/functions/my-demo-function/resource.ts will cause the function to be named my-demo-function by default. If an entry is specified, then the name defaults to the basename of the entry path. For example, an entry of ./signup-trigger-handler.ts would cause the function name to default to signup-trigger-handler . This optional property can be used to explicitly set the name of the function. By default, functions will time out after 3 seconds. This can be configured to any whole number of seconds up to 15 minutes. By default, functions have 512 MB of memory allocated to them. This can be configured from 128 MB up to 10240 MB. Note that this can increase the cost of function invocation. For more pricing information see here . By default, functions have 512MB of ephemeral storage to them. This can be configured from 512 MB upto 10240 MB. Note that this can increase the cost of function invocation. For more pricing information visit the Lambda pricing documentation . Currently, only Node runtimes are supported by defineFunction . However, you can change the Node version that is used by the function. The default is the oldest Node LTS version that is supported by AWS Lambda (currently Node 18). If you wish to use an older version of Node, keep an eye on the Lambda Node version deprecation schedule . As Lambda removes support for old Node versions, you will have to update to newer supported versions. By default, Amplify will look for your function handler in a file called handler.ts in the same directory as the file where defineFunction is called. To point to a different handler location, specify an entry value. By default, functions are grouped together in a resource group named function . You can override this to group related function with other Amplify resources like auth , data , storage , or separate them into your own custom group. This is typically useful when you have resources that depend on each other and you want to group them together."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_deletion-backup-resources.txt", "text": "Deletion protection and Backup resources Deleting a Amplify sandbox with a resource enabled with deletion protection, the deploy process will fail and the resource will need to be manually deleted on the AWS console. Using the AWS Cloud Development Kit (CDK) we can configure Amplify generated resource to enable deletion protection and backups on supported resources. For example, you can use AWS CDK to enable Point-in-time recovery for DynamoDB tables , or use AWS Backup as a advanced backup option. Using underlying CDK construct properties you can modify resource configurations. This allows you to customize backend resources beyond what is offered via the define* functions. For example, if you would like to enable deletion protection on a Cognito user pool resource created by Amplify Auth. For example, if you would like to enable Deletion protection on all DynamoDB tables created by GraphQL API. For example, enabling Point-in-time recovery for all the DynamoDB tables created by GraphQL API. By default Point-in-Time recovery retains backups for 35 days. For example, if your DynamoDB tables requires backups that extend the default 35 days point-in-time recovery, AWS Backup service can be utilized to centralize and automate backups for DynamoDB tables. The example below outlines a backup plan configured to run daily at midnight, for all DynamoDB tables. For example, if you would like to retain a resource on stack deletion, you can use the applyRemovalPolicy property on the resource to add a retention policy. ampx sandbox delete ignores any resource removal policy and always deletes all resources."}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_sign-in.txt", "text": "Sign-in Amplify provides a client library that enables you to interact with backend resources such as Amplify Auth. The quickest way to get started with Amplify Auth in your frontend application is with the Authenticator component , which provides a customizable UI and complete authentication flows. The signIn API response will include a nextStep property, which can be used to determine if further action is required. It may return the following next steps: For more information on handling the MFA steps that may be returned, see multi-factor authentication . When you have Email or SMS MFA enabled, Cognito will send messages to your users on your behalf. Email and SMS messages require that your users have email address and phone number attributes respectively. It is recommended to set these attributes as required in your user pool if you wish to use either Email MFA or SMS MFA. When these attributes are required, a user must provide these details before they can complete the sign up process. If you have set MFA to be required and you have activated more than one authentication factor, Cognito will prompt new users to select an MFA factor they want to use. Users must have a phone number to select SMS and an email address to select email MFA. If a user doesn't have the necessary attributes defined for any available message based MFA, Cognito will prompt them to set up TOTP. Visit the multi-factor authentication documentation to learn more about enabling MFA on your backend auth resource. Following sign in, you will receive a nextStep in the sign-in result of one of the following types. Collect the user response and then pass to the confirmSignIn API to complete the sign in flow. Note: The Amplify authentication flow will persist relevant session data throughout the lifespan of a page session. This enables the confirmSignIn API to be leveraged even after a full page refresh in a multi-page application, such as when redirecting from a login page to a sign in confirmation page. To sign in using an external identity provider such as Google, use the signInWithRedirect function. Note: if you do not pass an argument to signInWithRedirect it will redirect your users to the Cognito Hosted UI , which has limited support for customization. Alternatively if you have configured OIDC or SAML-based identity providers in your auth resource, you can specify a \"custom\" provider in signInWithRedirect : The autoSignIn API will automatically sign-in a user when it was previously enabled by the signUp API and after any of the following cases has completed: User confirmed their account with a verification code sent to their phone or email (default option). User confirmed their account with a verification link sent to their phone or email. In order to enable this option you need to go to the Amazon Cognito console , look for your userpool, then go to the Messaging tab and enable link mode inside the Verification message option. Finally you need to define the signUpVerificationMethod"}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_sign-in.txt", "text": "to link inside the Cognito option of your Auth config. Note : When MFA is enabled, your users may be presented with multiple consecutive steps that require them to enter an OTP to proceed with the sign up and subsequent sign in flow. This requirement is not present when using the USER_AUTH flow. Your application's users can also sign in using passwordless methods. To learn more, visit the concepts page for passwordless . Pass SMS_OTP as the preferredChallenge when calling the signIn API in order to initiate a passwordless authentication flow with SMS OTP. Pass EMAIL_OTP as the preferredChallenge when calling the signIn API in order to initiate a passwordless authentication flow using email OTP. Pass WEB_AUTHN as the preferredChallenge in order to initiate the passwordless authentication flow using a WebAuthn credential. Pass either PASSWORD or PASSWORD_SRP as the preferredChallenge in order to initiate a traditional password based authentication flow. Omit the preferredChallenge parameter to discover what first factors are available for a given user. The confirmSignIn API can then be used to select a challenge and initiate the associated authentication flow."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_rest-api_update-data.txt", "text": "Update data To create or update a item via the API endpoint:"}, {"source": "data/raw_pages/react_build-a-backend_auth_set-up-auth.txt", "text": "Set up Amplify Auth Amplify Auth is powered by Amazon Cognito . Cognito is a robust user directory service that handles user registration, authentication, account recovery, and other operations. Review the concepts to learn more . To get started with defining your authentication resource, open or create the auth resource file: By default, your auth resource is scaffolded using email as the default login mechanism. You can also configure your auth resource to allow signing in with phone numbers or an external provider such as Google, Facebook, Amazon, or Sign in with Apple. Note: At a minimum you will need to pass a loginWith value to set up how your users sign in to your app. Signing in with email and password is configured by default if you do not provide any value. After you have chosen and defined your authentication resource, run the following command to create your resource in your personal cloud sandbox. After a successful deployment, this command also generates an outputs file ( amplify_outputs.json ) to enable your frontend app to connect to your backend resources. The values you configure in your backend authentication resource are set in the generated outputs file to automatically configure the frontend Authenticator connected component . Creating and correctly implementing the sign-in flow can be challenging and time-consuming. Amplify's Authenticator UI component streamlines this by enabling you to rapidly build the entire authentication flow for your app. The component works seamlessly with configuration in amplify/auth/resource.ts to automatically connect with your backend resources. Amplify has pre-built UI components for React, Vue, Angular, React Native, Swift, Android, and Flutter. In this guide, we are focusing on those for web applications. First, install the @aws-amplify/ui-react library: Next, open pages/_app.tsx and add the Authenticator component. Once you add the Authenticator component to your app, you can test the sign-up, sign-in, and sign-out functionality. You can also customize the Authenticator connected component to adjust colors and styling as needed. Now that you have completed setting up authentication in your Amplify app with email and password, you may also want to add some additional features. We recommend you learn more about:"}, {"source": "data/raw_pages/react_start.txt", "text": "Get started AWS Amplify is a collection of cloud services and libraries for fullstack application development. Amplify provides frontend libraries, UI components, backend building, and frontend hosting for building fullstack cloud apps. This tutorial will teach you how to use Amplify's new code-first developer experience to build a fullstack application with data, authentication, and frontend hosting which are all deployed to AWS. If you're completely new to AWS Amplify, you may want to read more about how it works and the concepts behind the second generation of AWS Amplify , which this tutorial will use."}, {"source": "data/raw_pages/react_build-a-backend_storage_reference.txt", "text": "Get a temporary presigned URL to download the specified S3 object. The presigned URL expires when the associated role used to sign the request expires or the option expiresIn is reached. The expiresAt property in the output object indicates when the URL MAY expire. By default, it will not validate the object that exists in S3. If you set the options.validateObjectExistence to true, this method will verify the given object already exists in S3 before returning a presigned URL, and will throw StorageError if the object does not exist."}, {"source": "data/raw_pages/react_build-a-backend_functions_environment-variables-and-secrets.txt", "text": "Environment variables and secrets Amplify Functions support setting environment variables and secrets on the environment property of defineFunction . Note: do not store secret values in environment variables. Environment variables values are rendered in plaintext to the build artifacts located at .amplify/artifacts and may be emitted to CloudFormation stack event messages. To store secrets skip to the secrets section Note: Environment variables and secrets configuration in defineFunction is not supported for Custom Functions . Environment variables can be configured in defineFunction using the environment property. Any environment variables specified here will be available to the function at runtime. Some environment variables are constant across all branches and deployments. But many environment values differ between deployment environments. Branch-specific environment variables can be configured for Amplify hosting deployments . Suppose you created a branch-specific environment variable in hosting called \"API_ENDPOINT\" which had a different value for your \"staging\" vs \"prod\" branch. If you wanted that value to be available to your function you can pass it to the function using Within your function handler, you can access environment variables using the normal process.env global object provided by the Node runtime. However, this does not make it easy to discover what environment variables will be available at runtime. Amplify generates an env symbol that can be used in your function handler and provides typings for all variables that will be available at runtime. Copy the following code to use it. Understanding the \"env\" symbol and how to manually configure your Amplify project to use it At the end of AWS Cloud Development Kit's (AWS CDK) synthesis, Amplify gathers names of environment variables that will be available to the function at runtime and generates the file .amplify/generated/env/<function-name>.ts . If you created your project with create-amplify , then Amplify has already set up your project to use the env symbol. If you did not, you will need to manually configure your project. Within your amplify/tsconfig.json file add a paths compiler option: Close accordion When you configure your function with environment variables or secrets, Amplify's backend tooling generates a file using the function's name in .amplify/generated with references to your environment variables and secrets, as well as environment variables predefined by the Lambda runtime . This provides a type-safe experience for working with environment variables that does not require typing process.env manually. Note: generated files are created before deployments when executing ampx sandbox or ampx pipeline-deploy For example, if you have a function with the following definition: Upon starting your next deployment, Amplify will create a file at the following location: Using the TypeScript path alias, $amplify , you can import the file in your function's handler: Encountering issues with this file? Visit the troubleshooting guide for Cannot find module $amplify/env/<function-name> Sometimes it is necessary to provide a secret value to a function. For example, it may need a database password or an API key to perform some business use case. Environment variables should NOT be used for this because environment variable values are included in"}, {"source": "data/raw_pages/react_build-a-backend_functions_environment-variables-and-secrets.txt", "text": "plaintext in the function configuration. Instead, secret access can be used. Before using a secret in a function, you need to define a secret . After you have defined a secret, you can reference it in your function config. You can use this secret value at runtime in your function the same as any other environment variable. However, you will notice that the value of the environment variable is not stored as part of the function configuration. Instead, the value is fetched when your function runs and is provided in memory."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_analytics_existing-resources.txt", "text": "Use existing AWS resources To use existing Amazon Pinpoint resources with your Amplify backend or frontend application, use the addOutput method to surface backend resource outputs to the amplify_outputs.json file: Alternatively, you can configure the client library directly using Amplify.configure() . This manual setup enables you to use your existing Amazon Pinpoint resource in your app. Amazon Pinpoint requires an AWS Identity and Access Management (IAM) policy in order to use the record and identifyUser APIs:"}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_manage-user-attributes.txt", "text": "Manage user attributes User attributes such as email address, phone number help you identify individual users. Defining the user attributes you include for your user profiles makes user data easy to manage at scale. This information will help you personalize user journeys, tailor content, provide intuitive account control, and more. You can capture information upfront during sign-up or enable customers to update their profile after sign-up. In this section we take a closer look at working with user attributes, how to set them up and manage them. You can create user attributes during sign-up or when the user is authenticated. To do this as part of sign-up you can pass them in the userAttributes object of the signUp API: Custom attributes can be passed in with the userAttributes option of the signUp API: You can retrieve user attributes for your users to read in their profile using the fetchUserAttributes API. This helps you personalize their frontend experience as well as control what they will see. You can use the updateUserAttribute API to create or update existing user attributes. Note: If you change an attribute that requires confirmation (i.e. email or phone_number), the user will receive a confirmation code either to their email or cellphone. This code can be used with the confirmUserAttribute API to confirm the change. You can use the updateUserAttributes API to create or update multiple existing user attributes. Some attributes require confirmation for the attribute update to complete. If the attribute needs to be confirmed, part of the result of the updateUserAttribute or updateUserAttributes APIs will be CONFIRM_ATTRIBUTE_WITH_CODE . A confirmation code will be sent to the delivery medium mentioned in the delivery details. When the user gets the confirmation code, you can present a UI to the user to enter the code and invoke the confirmUserAttribute API with their input: If an attribute needs to be verified while the user is authenticated, invoke the sendUserAttributeVerificationCode API as shown below: The deleteUserAttributes API allows to delete one or more user attributes."}, {"source": "data/raw_pages/react_build-a-backend_data_enable-logging.txt", "text": "Enable logging You can enable logging to debug your GraphQL API using Amazon CloudWatch logs. To learn more about logging and monitoring capabilities for your GraphQL API, visit the AWS AppSync documentation for logging and monitoring . Default logging can be enabled by setting the logging property to true in the call to defineData . For example: Using logging: true applies the default configuration: You can customize individual configuration values by providing a DataLogConfig object. For example: WARNING : Setting excludeVerboseContent to false logs full queries and user parameters, which can contain sensitive data. We recommend limiting CloudWatch log access to only those roles or users (e.g., DevOps or developers) who genuinely require it, by carefully scoping your IAM policies. true : Enables default logging. DataLogConfig object: Overrides one or more default fields."}, {"source": "data/raw_pages/react_build-a-backend_data_connect-from-server-runtime.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_auth_use-existing-cognito-resources.txt", "text": "Use existing Cognito resources Amplify Auth can be configured to use an existing Amazon Cognito user pool and identity pool. If you are in a team setting or part of a company that has previously created auth resources, you can configure the client library directly , or maintain references with AWS Cloud Development Kit (AWS CDK) in your Amplify backend. Note: when using existing auth resources, it may be necessary to add additional policies or permissions for your authenticated and unauthenticated IAM roles. These changes must be performed manually. You can use existing resources without an Amplify backend by configuring the client library directly. Amplify cannot modify the configuration of your referenced resources and only captures the resource configuration at the time of reference, any subsequent changes made to the referenced resources will not be automatically reflected in your Amplify backend. If you have created Amazon Cognito resources outside of the context of your Amplify app such as creating resources through the AWS Console or consuming resources created by a separate team, you can use referenceAuth to reference the existing resources. It requires a user pool, a user pool client, identity pool, and an authenticated & unauthenticated IAM role configured on your identity pool. IAM policies specific to your Amplify application will be appended to your authenticated and unauthenticated roles, and applications using the referenced resource will be able to create users in the Cognito user pool and identities in the Cognito identity pool. You can also use the access property to grant permissions to your auth resource from other Amplify backend resources. For example, if you have a function that needs to retrieve details about a user: Additionally, you can also use the groups property to reference groups in your user pool. This is useful if you want to work with groups in your application and provide access to resources such as storage based on group membership. In a team setting you may want to reference a different set of auth resources depending on the deployment context. For instance if you have a staging branch that should reuse resources from a separate \"staging\" environment compared to a production branch that should reuse resources from the separate \"production\" environment. In this case we recommend using environment variables. Environment variables must be configured separately on your machine for sandbox deployments and Amplify console for branch deployments."}, {"source": "data/raw_pages/react_build-a-backend_data_custom-subscription.txt", "text": "Add custom real-time subscriptions Create a custom real-time subscription for any mutation to enable PubSub use cases. For every custom subscription, you need to set: the mutation(s) that should trigger a subscription event, a return type that matches the subscribed mutations' return type, authorization rules. Optionally, you can set filter arguments to customize the server-side subscription filter rules. Use a.subscription() to define your custom subscription in your amplify/data/resource.ts file: For this example, we're building a generic PubSub capability. This requires us to convert the arguments for publish into the Channel 's format. Create a new publish.js file in your amplify/data/ folder with the following contents: Next, create a new receive.js file in your amplify/data/ folder to define handlers for your subscription. In this case, it'll just be a simple passthrough. In the next section, we'll explore how to use this handler to construct more advanced subscription filters. Note: We're planning a developer experience enhancement in the near future that'll create this passthrough under the hood. From your generated Data client, you can find all your custom subscriptions under client.subscriptions . Subscribe using the .subscribe() function and then use the next function to handle incoming events. You can try publishing an event using the custom mutation to test the real-time subscription. Your subscription event should be received and logs the payload into your app's developer console. Unsubscribe your subscription to disconnect using the unsubscribe() function. You can add subscription filters by adding arguments to the custom subscriptions. If you want to customize the filters, modify the subscription handler. For this example, we'll allow a customer to pass in a namePrefix parameter that allows the end users to only receive channel events in channels that start with the namePrefix . In your handler, you can set custom subscription filters based on arguments passed into the custom subscription. For this example, create a new receive.js file alongside the amplify/data/resource.ts file:"}, {"source": "data/raw_pages/react_build-a-backend_functions_set-up-function.txt", "text": "Set up a Function Amplify Functions are powered by AWS Lambda , and allow you to perform a wide variety of customization through self-contained functions . Functions can respond to events from other resources, execute some logic in-between events like an authentication flow, or act as standalone jobs. They are used in a variety of settings and use cases: Authentication flow customizations (e.g. attribute validations, allowlisting email domains) Resolvers for GraphQL APIs Handlers for individual REST API routes, or to host an entire API Scheduled jobs To get started, create a new directory and a resource file, amplify/functions/say-hello/resource.ts . Then, define the Function with defineFunction : Next, create the corresponding handler file at amplify/functions/say-hello/handler.ts . This is where your function code will go. The handler file must export a function named \"handler\". This is the entry point to your function. For more information on writing functions, refer to the AWS documentation for Lambda function handlers using Node.js . Lastly, this function needs to be added to your backend. Now when you run npx ampx sandbox or deploy your app on Amplify, it will include your Function. To invoke your Function, we recommend adding your Function as a handler for a custom query with your Amplify Data resource . This will enable you to strongly type Function arguments and the return statement, and use this to author your Function's business logic. To get started, open your amplify/data/resource.ts file and specify a new query in your schema: Now you can use this query from the Schema export to strongly type your Function handler: Finally, use the data client to invoke your Function by calling its associated query. Now that you have completed setting up your first Function, you may also want to add some additional features or modify a few settings. We recommend you learn more about:"}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching_share-resources.txt", "text": "Share resources across branches In some instances, you may not want to deploy new resources for every branch. For example, you might want all your feature branches to point to the backend resources deployed by the dev branch so you can reuse seed data, users, and groups. You can update your app build settings to share resources across branches. From the Amplify console, go to your App overview page, select Build settings under the Hosting for viewing your app's build specification YAML file. Update the build settings for the backend phase to run npx ampx generate outputs --branch dev app-id $AWS_APP_ID to generate the amplify_outputs.json file for all branches other than main or dev . After this update, any new deployed branches will not deploy backend resources as part of the build and instead will use the deployed backend resources from the dev branch. Update the build settings for the backend phase to run npx ampx generate outputs --branch dev app-id $AWS_APP_ID to generate the amplify_outputs.json file for all branches other than main or dev . After this update, any new deployed branches will not deploy backend resources as part of the build and instead will use the deployed backend resources from the dev branch."}, {"source": "data/raw_pages/react_build-a-backend_storage_authorization.txt", "text": "Customize authorization rules Customize authorization for your storage bucket by defining access to file paths for guests, authenticated users, and user groups. Access can also be defined for functions that require access to the storage bucket. Refer to the following examples to understand how you can further customize authorization against different user types. Authentication is required to continue using Amplify Storage, please make sure you set it up if you haven't already - documentation to set up Auth . Note: Paths in access definitions cannot have a '/' at the beginning of the string. By default, all paths are denied to all types of users unless explicitly granted within defineStorage using the access callback as shown below. Guest Users Authenticated Users User Groups Owners Functions To grant all guest (i.e. not signed in) users of your application read access to files under media/ , use the following access values. Note: When a user is part of a group, they are assigned the group role, which means permissions defined for the authenticated role will not apply for this user. To grant access to users within a group, you must explicitly define access permissions for the group against the desired prefix. To grant all authenticated (i.e. signed in) users of your application read access to files under media/ , use the following access configuration. Note: When a user is part of a group, they are assigned the group role, which means permissions defined for the authenticated role will not apply for this user. To grant access to users within a group, you must explicitly define access permissions for the group against the desired prefix. If you have configured user groups when setting up auth in your defineAuth object, you can scope storage access to specific groups. In this example, assume you have a defineAuth config with admin and auditor groups. With the following access definition, you can configure permissions such that auditors have read only permissions to media/* while admin has full permissions. If multiple groups require the same set of actions, this can be combined into a single rule. In some use cases, you may want just the uploader of a file to be able to perform actions on it. For example, in a music sharing app anyone can listen to a song, but only the person who uploaded that song could delete it. In Amplify Storage, you can do this by using the entity_id to represent the user which scopes files to individual users. The entity_id is a reserved token that will be replaced with the users' identifier when the file is being uploaded. You can specify the method of identification when defining access to the path like allow.entity(<identification_method>).to([..]) . Currently, Identity Pool is the only identification method available - allow.entity('identity').to([..]) The following policy would allow authenticated users full access to media/ that matches their identity id. A user with identity id user123 would be able to perform read/write/delete operations on files within media/user123/* but would not be able to"}, {"source": "data/raw_pages/react_build-a-backend_storage_authorization.txt", "text": "perform actions on files with any other path. Likewise, a user with identity ID userABC would be able to perform read/write/delete operation on files only within media/userABC/* . In this way, each user can be granted access to a storage path that is not accessible to any other user. The following example shows how you can define access to profile pictures where anyone can view them but only the owner can modify/delete them. When a rule for guests, authenticated users, user groups, or resources is applied to a path with the {entity_id} token, the token is replaced with a wildcard ( * ). This means that the access will apply to files uploaded by any user. In the above policy, write and delete is scoped to just the owner, but read is allowed for guest and authenticated users for any file within media/profile-pictures/*/* . In addition to granting application users access to storage files, you may also want to grant a backend function access to storage files. This could be used to enable a use case like resizing images or automatically deleting old files. The following configuration is used to define function access. This would grant the function demoFunction the ability to read write and delete files within media/* . When a function is granted access to storage, it also receives an environment variable that contains the name of the Amazon S3 bucket configured by storage. This environment variable can be used in the function to make AWS SDK calls to the storage bucket. The environment variable is named <storage-name>_BUCKET_NAME . In the above example, it would be named myProjectFiles_BUCKET_NAME . Learn more about function resource access environment variables There are some rules for the types of paths that can be specified at the same time in the storage access definition. All paths must end with /* . Only one level of nesting is allowed. For example, you can define access controls on media/* and media/albums/* but not on media/albums/photos/* because there are two other definitions along the same path. Wildcards cannot conflict with the {entity_id} token. For example, you cannot have both media/* and media/{entity_id}/* defined because the wildcard in the first path conflicts with the {entity_id} token in the second path. A path cannot be a prefix of another path with an {entity_id} token. For example media/* and media/albums/{entity_id}/* is not allowed. When one path is a subpath of another, the permissions on the subpath always override the permissions from the parent path. Permissions are not \"inherited\" from a parent path. Consider the following access definition example: The access control matrix for this configuration is Authenticated users have access to read, write, and delete everything under media/* EXCEPT media/profile-pictures/* and media/albums/* . For those subpaths, the scoped down access overrides the access granted on the parent media/* When you configure access to a particular path, you can scope the access to one or more CRUDL actions. Note: read is a combination of get and list access definitions and"}, {"source": "data/raw_pages/react_build-a-backend_storage_authorization.txt", "text": "hence cannot be defined in the presence of get or list . To configure defineStorage in Amplify Gen 2 to behave the same way as the storage category in Gen 1, the following definition can be used."}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_guest-access.txt", "text": "Guest access Amplify Auth can be configured to automatically obtain guest credentials once the device is online so that you are able to use other categories \"anonymously\" without the need to sign in. You will not be able to perform user specific methods while in this state such as updating attributes, changing your password, or getting the current user. However, you can obtain the unique Identity ID which is assigned to the device through the fetchAuthSession method described here . Amplify Gen 2 enables guest access by default. To disable it, you can update the backend.ts file with the following changes:"}, {"source": "data/raw_pages/react_build-a-backend_data_set-up-data.txt", "text": "Set up Amplify Data In this guide, you will learn how to set up Amplify Data. This includes building a real-time API and database using TypeScript to define your data model, and securing your API with authorization rules. We will also explore using AWS Lambda to scale to custom use cases. Before you begin, you will need: Node.js v18.16.0 or later npm v6.14.4 or later git v2.14.1 or later With Amplify Data, you can build a secure, real-time API backed by a database in minutes. After you define your data model using TypeScript, Amplify will deploy a real-time API for you. This API is powered by AWS AppSync and connected to an Amazon DynamoDB database. You can secure your API with authorization rules and scale to custom use cases with AWS Lambda. If you've run npm create amplify@latest already, you should see an amplify/data/resource.ts file, which is the central location to configure your data backend. The most important element is the schema object, which defines your backend data models ( a.model() ) and custom queries ( a.query() ), mutations ( a.mutation() ), and subscriptions ( a.subscription() ). Every a.model() automatically creates the following resources in the cloud: a DynamoDB database table to store records query and mutation APIs to create, read (list/get), update, and delete records createdAt and updatedAt fields that help you keep track of when each record was initially created or when it was last updated real-time APIs to subscribe for create, update, and delete events of records The allow.publicApiKey() rule designates that anyone authenticated using an API key can create, read, update, and delete todos. To deploy these resources to your cloud sandbox, run the following CLI command in your terminal: Once the cloud sandbox is up and running, it will also create an amplify_outputs.json file, which includes the relevant connection information to your data backend, like your API endpoint URL and API key. To connect your frontend code to your backend, you need to: Configure the Amplify library with the Amplify client configuration file ( amplify_outputs.json ) Generate a new API client from the Amplify library Make an API request with end-to-end type-safety First, install the Amplify client library to your project: In your app's entry point, typically main.tsx for React apps created using Vite, make the following edits: Let's first add a button to create a new todo item. To make a \"create Todo\" API request, generate the data client using generateClient() in your frontend code, and then call .create() operation for the Todo model. The Data client is a fully typed client that gives you in-IDE code completion. To enable this in-IDE code completion capability, pass in the Schema type to the generateClient function. Run the application in local development mode with npm run dev and check your network tab after creating a todo. You should see a successful request to a /graphql endpoint. Try playing around with the code completion of .update(...) and .delete(...) to get a sense of other mutation operations. Next,"}, {"source": "data/raw_pages/react_build-a-backend_data_set-up-data.txt", "text": "list all your todos and then refetch the todos after a todo has been added: You can also use observeQuery to subscribe to a live feed of your backend data. Let's refactor the code to use a real-time observeQuery instead. Success! You've learned how to create your first real-time API and database with Amplify Data. There's so much more to discover with Amplify Data. Learn more about:"}, {"source": "data/raw_pages/react_build-a-backend_data_data-modeling_secondary-index.txt", "text": "Customize secondary indexes You can optimize your list queries based on \"secondary indexes\". For example, if you have a Customer model, you can query based on the customer's id identifier field by default but you can add a secondary index based on the accountRepresentativeId to get list customers for a given account representative. A secondary index consists of a \"hash key\" and, optionally, a \"sort key\". Use the \"hash key\" to perform strict equality and the \"sort key\" for greater than (gt), greater than or equal to (ge), less than (lt), less than or equal to (le), equals (eq), begins with, and between operations. The example client query below allows you to query for \"Customer\" records based on their accountRepresentativeId : Review how this works under the hood with Amazon DynamoDB Amplify uses Amazon DynamoDB tables as the default data source for a.model() . For key-value databases, it is critical to model your access patterns with \"secondary indexes\". Use the .secondaryIndexes() modifier to configure a secondary index. Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale but making it work for your access patterns requires a bit of forethought. DynamoDB query operations may use at most two attributes to efficiently query data. The first query argument passed to a query (the hash key) must use strict equality and the second attribute (the sort key) may use gt, ge, lt, le, eq, beginsWith, and between. DynamoDB can effectively implement a wide variety of access patterns that are powerful enough for the majority of applications. Close accordion You can define \"sort keys\" to add a set of flexible filters to your query, such as \"greater than\" (gt), \"greater than or equal to\" (ge), \"less than\" (lt), \"less than or equal to\" (le), \"equals\" (eq), \"begins with\" (beginsWith), and \"between\" operations. On the client side, you should find a new listBy... query that's named after hash key and sort keys. For example, in this case: listByAccountRepresentativeIdAndName . You can supply the filter as part of this new list query: You can also customize the auto-generated query name under client.models.<MODEL_NAME>.listBy... by setting the queryField() modifier. In your client app code, you'll see query updated under the Data client: To customize the underlying DynamoDB's index name, you can optionally provide the name() modifier."}, {"source": "data/raw_pages/react_build-a-backend_auth_customize-auth-lifecycle.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/angular_start_quickstart.txt", "text": "Quickstart \u00f0\u009f\u0091\u008b Welcome to AWS Amplify! In this quickstart guide, you will: Deploy an Angular app Build and connect to a database with real-time data updates Configure authentication and authorization rules We've created a starter \"To-do\" application to help get started faster. First, you will create a repository in your GitHub account using our starter Angular template. Use our starter template to create a repository in your GitHub account. This template scaffolds a starter Angular application with Amplify backend capabilities. Create repository from template Use the form in GitHub to finalize your repo's creation. Now that the repository has been created, deploy it with Amplify. Deploy to AWS Select GitHub . After you give Amplify access to your GitHub account via the popup window, pick the repository and main branch to deploy. Make no other changes and click through the flow to Save and deploy . While you are waiting for your app to deploy (~5 mins) Learn about the project structure Let's take a tour of the project structure in this starter repository by opening it on GitHub. The starter application has pre-written code for a to-do list app. It gives you a real-time database with a feed of all to-do list items and the ability to add new items. Close accordion When the build completes, visit the newly deployed branch by selecting \"Visit deployed URL\". Since the build deployed an API, database, and authentication backend, you will be able to create new to-do items. In the Amplify console, click into the deployment branch (in this case main ) > select Data in the left-hand menu > Data manager to see the data entered in your database. Let's learn how to enhance the app functionality by creating a delete flow for to-do list items. Now let's set up our local development environment to add features to the frontend. Click on your deployed branch and you will land on the Deployments page which shows you your build history and a list of deployed backend resources. At the bottom of the page you will see a tab for Deployed backend resources . Click on the tab and then click the Download amplify_outputs.json file button. Clone the repository locally. Now move the amplify_outputs.json file you downloaded above to the root of your project. The amplify_outputs.json file contains backend endpoint information, publicly-viewable API keys, authentication flow information, and more. The Amplify client library uses this outputs file to connect to your Amplify Backend. You can review how the outputs file is imported within the app.component.ts file and then passed into the Amplify.configure(...) function of the Amplify client library. Close accordion Go to the src/app/todos/todos.component.ts file and add a new deleteTodo function. Call the deleteTodo function from the UI. Try out the deletion functionality now by starting the local dev server: This should start a local dev server at http://localhost:4200 . The starter application already has a pre-configured auth backend defined in the amplify/auth/resource.ts file. We've configured it to support email and password login"}, {"source": "data/raw_pages/angular_start_quickstart.txt", "text": "but you can extend it to support a variety of login mechanisms, including Google, Amazon, Sign In With Apple, and Facebook. The fastest way to get your login experience up and running is to use our Authenticator UI component. In your src/app/app.component.ts file, import the AmplifyAuthenticatorModule . Update the application UI and include styles. The Authenticator component auto-detects your auth backend settings and renders the correct UI state based on the auth backend's authentication flow. Try out your application in your localhost environment again. You should be presented with a login experience now. To get these changes to the cloud, commit them to git and push the changes upstream. Amplify automatically deploys the latest version of your app based on your git commits. In just a few minutes, when the application rebuilds, the hosted app will be updated to support the deletion functionality. Let's update our backend to implement per-user authorization rules, allowing each user to only access their own to-dos. To make backend updates, we are going to require AWS credentials to deploy backend updates from our local machine. Skip ahead to step 8 , if you already have an AWS profile with credentials on your local machine, and your AWS profile has the AmplifyBackendDeployFullAccess permission policy. Otherwise, set up local AWS credentials that grant Amplify permissions to deploy backend updates from your local machine. To update your backend without affecting the production branch, use Amplify's cloud sandbox. This feature provides a separate backend environment for each developer on a team, ideal for local development and testing. To start your cloud sandbox, run the following command in a new Terminal window : Once the cloud sandbox has been fully deployed (~5 min), you'll see the amplify_outputs.json file updated with connection information to a new isolated authentication and data backend. The npx ampx sandbox command should run concurrently to your npm run dev . You can think of the cloud sandbox as the \"localhost-equivalent for your app backend\". The to-do items in the starter are currently shared across all users, but, in most cases, you want data to be isolated on a per-user basis. To isolate the data on a per-user basis, you can use an \"owner-based authorization rule\". Let's apply the owner-based authorization rule to your to-do items: In the application client code, let's also render the username to distinguish different users once they're logged in. Now, let's go back to your local application and test out the user isolation of the to-do items. You will need to sign up new users again because now you're working with the cloud sandbox instead of your production backend. To get these changes to the cloud, commit them to git and push the changes upstream. Once your build completes in the Amplify Console, the main backend will update to support the changes made within the cloud sandbox. The data in the cloud sandbox is fully isolated and won't pollute your production database. That's it! You have successfully built a fullstack app on"}, {"source": "data/raw_pages/angular_start_quickstart.txt", "text": "AWS Amplify. If you want to learn more about how to work with Amplify, here's the conceptual guide for how Amplify works ."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_predictions_identify-entity.txt", "text": "Identify entities from images Note: Make sure to complete the getting started section first, where you will set up the IAM roles with the right policy actions Predictions.identify({entities: {...}}) => Promise<> Detects entities from an image and potentially related information such as position, faces, and landmarks. Can also identify celebrities and entities that were previously added. This function returns a Promise that returns an object with the entities that was identified. Input can be sent directly from the browser (using File object or ArrayBuffer object) or an Amazon S3 key from project bucket. Detect entities directly from image uploaded from the browser. (File object) Detect entities directly from image binary from the browser. (ArrayBuffer object) This technique is useful when you have base64 encoded binary image data, for example, from a webcam source. From Amazon S3 key The following options are independent of which source is specified. For demonstration purposes it will be used file but it can be used S3 Key as well. Detecting bounding box of faces from an image with its landmarks (eyes, mouth, nose). Detecting celebrities on an image. It will return only celebrities the name and urls with related information. Detecting entities from previously uploaded images (e.g. Advanced Configuration for Identify Entities)"}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching_mono-and-multi-repos.txt", "text": "Separate frontend and backend teams You might have different frontend and backend teams that maintain their own repositories. With Amplify Gen 2, you can deploy repositories that have backend-only code, so frontend and backend teams can operate independently of each other. Run mkdir backend-app && cd backend-app && npm create amplify@latest to set up a backend-only Amplify project. Commit the code to a Git provider of your choice. Connect the backend-app in the new console. Navigate to the Amplify console and select Create new app . When you connect the repository, notice the only auto-detected framework is Amplify. Once you choose Save and deploy , your backend project will build. Now let's set up the frontend app and connect to the deployed backend. Install Amplify dependencies. To connect to the deployed backend, run the following command. To locate the App ID for your backend application, navigate to the Amplify console and select your backend-app . On the Overview page, the App ID is displayed under the project name. This will generate the amplify_outputs.json file that contains all the information about your backend at the root of your project. To validate that your frontend can connect to the backend, add the Authenticator login form to your app. Let's also add an amplify.yml build-spec to our repository. Now let's deploy the app. In the Amplify console, choose Create new app . Connect the repository with the default settings. You should see that the build generates the output and does not deploy a frontend. Validate that your app is working fine. The ideal scenario is that the frontend automatically retrieves the latest updates from the backend every time there is a modification made to the backend code. Use the Amplify Console to create an incoming webhook . Navigate to the multi-repo-example app, under Hosting > Build settings select Create webhook . Provide a name for the webhook and select the target branch to build on incoming webhook requests. Next, select the webhook and copy the curl command which will be used to trigger a build for the multi-repo-example app. Now update the build settings for the backend-app to include the curl command to trigger a frontend build any time there are changes to the backend. If you're using Amplify Data, we recommend adding a paths entry in the tsconfig.json of your frontend app that points to the amplify/data/resource.ts file in your backend app to easily access your schema type definitions from your frontend apps. First, cone your backend repo into the same parent directory as your frontend app, then add the following entry: You can then import the Schema type from this path in your frontend code to get code completion and strong typing for your API calls:"}, {"source": "data/raw_pages/gen2_start_quickstart.txt", "text": "Quickstart \u00f0\u009f\u0091\u008b Welcome to AWS Amplify! In this quickstart guide, you will: Deploy a React and Vite app Build and connect to a database with real-time data updates Configure authentication and authorization rules We've created a starter \"To-do\" application to help get started faster. First, you will create a repository in your GitHub account using our starter React template. Use our starter template to create a repository in your GitHub account. This template scaffolds create-vite-app with Amplify backend capabilities. Create repository from template Use the form in GitHub to finalize your repo's creation. Now that the repository has been created, deploy it with Amplify. Deploy to AWS Select GitHub . After you give Amplify access to your GitHub account via the popup window, pick the repository and main branch to deploy. Make no other changes and click through the flow to Save and deploy . While you are waiting for your app to deploy (~5 mins) Learn about the project structure Let's take a tour of the project structure in this starter repository by opening it on GitHub. The starter application has pre-written code for a to-do list app. It gives you a real-time database with a feed of all to-do list items and the ability to add new items. Close accordion When the build completes, visit the newly deployed branch by selecting \"Visit deployed URL\". Since the build deployed an API, database, and authentication backend, you will be able to create new to-do items. In the Amplify console, click into the deployment branch (in this case main ) > select Data in the left-hand menu > Data manager to see the data entered in your database. Let's learn how to enhance the app functionality by creating a delete flow for to-do list items. Now let's set up our local development environment to add features to the frontend. Click on your deployed branch and you will land on the Deployments page which shows you your build history and a list of deployed backend resources. At the bottom of the page you will see a tab for Deployed backend resources . Click on the tab and then click the Download amplify_outputs.json file button. Clone the repository locally. Now move the amplify_outputs.json file you downloaded above to the root of your project. The amplify_outputs.json file contains backend endpoint information, publicly-viewable API keys, authentication flow information, and more. The Amplify client library uses this outputs file to connect to your Amplify Backend. You can review how the outputs file is imported within the main.tsx file and then passed into the Amplify.configure(...) function of the Amplify client library. Close accordion Go to the src/App.tsx file and add in a new deleteTodo functionality and pass function into the <li> element's onClick handler. Try out the deletion functionality now by starting the local dev server: This should start a local dev server at http://localhost:5173 . The starter application already has a pre-configured auth backend defined in the amplify/auth/resource.ts file. We've configured it to support email and"}, {"source": "data/raw_pages/gen2_start_quickstart.txt", "text": "password login but you can extend it to support a variety of login mechanisms, including Google, Amazon, Sign In With Apple, and Facebook. The fastest way to get your login experience up and running is to use our Authenticator UI component. In your src/main.tsx file, import the Authenticator UI component and wrap your <App> component. The Authenticator component auto-detects your auth backend settings and renders the correct UI state based on the auth backend's authentication flow. In your src/App.tsx file, add a button to enable users to sign out of the application. Import the useAuthenticator hook from the Amplify UI library to hook into the state of the Authenticator. Try out your application in your localhost environment again. You should be presented with a login experience now. To get these changes to the cloud, commit them to git and push the changes upstream. Amplify automatically deploys the latest version of your app based on your git commits. In just a few minutes, when the application rebuilds, the hosted app will be updated to support the deletion functionality. Let's update our backend to implement per-user authorization rules, allowing each user to only access their own to-dos. To make backend updates, we are going to require AWS credentials to deploy backend updates from our local machine. Skip ahead to step 8 , if you already have an AWS profile with credentials on your local machine, and your AWS profile has the AmplifyBackendDeployFullAccess permission policy. Otherwise, set up local AWS credentials that grant Amplify permissions to deploy backend updates from your local machine. To update your backend without affecting the production branch, use Amplify's cloud sandbox. This feature provides a separate backend environment for each developer on a team, ideal for local development and testing. To start your cloud sandbox, run the following command in a new Terminal window : Once the cloud sandbox has been fully deployed (~5 min), you'll see the amplify_outputs.json file updated with connection information to a new isolated authentication and data backend. The npx ampx sandbox command should run concurrently to your npm run dev . You can think of the cloud sandbox as the \"localhost-equivalent for your app backend\". The to-do items in the starter are currently shared across all users, but, in most cases, you want data to be isolated on a per-user basis. To isolate the data on a per-user basis, you can use an \"owner-based authorization rule\". Let's apply the owner-based authorization rule to your to-do items: In the application client code, let's also render the username to distinguish different users once they're logged in. Go to your src/App.tsx file and render the user property from the useAuthenticator hook. Now, let's go back to your local application and test out the user isolation of the to-do items. You will need to sign up new users again because now you're working with the cloud sandbox instead of your production backend. To get these changes to the cloud, commit them to git and push the changes"}, {"source": "data/raw_pages/gen2_start_quickstart.txt", "text": "upstream. Once your build completes in the Amplify Console, the main backend will update to support the changes made within the cloud sandbox. The data in the cloud sandbox is fully isolated and won't pollute your production database. That's it! You have successfully built a fullstack app on AWS Amplify. If you want to learn more about how to work with Amplify, here's the conceptual guide for how Amplify works ."}, {"source": "data/raw_pages/react_how-amplify-works_concepts.txt", "text": "Concepts AWS Amplify Gen 2 uses a TypeScript-based, code-first developer experience (DX) for defining backends. The Gen 2 DX offers a unified Amplify developer experience with hosting, backend, and UI-building capabilities and a code-first approach. Amplify empowers frontend developers to deploy cloud infrastructure by simply expressing their app\u00e2\u0080\u0099s data model, business logic, authentication, and authorization rules completely in TypeScript. Amplify automatically configures the correct cloud resources and removes the requirement to stitch together underlying AWS services. You can use Amplify for end-to-end fullstack development. With the Gen 2 DX, you can provision backend infrastructure by authoring TypeScript. In the following diagram, the box at the bottom (outlined in pink), highlights the main difference in how you provision infrastructure compared to Gen 1. In Gen 1, you would use Studio's console or the CLI to provision infrastructure; in Gen 2, you author TypeScript code in files following a file-based convention (such as amplify/auth/resource.ts or amplify/auth/data.ts ). With TypeScript types and classes for resources, you gain strict typing and IntelliSense in Visual Studio Code to prevent errors. A breaking change in the backend code immediately reflects as a type error in the co-located frontend code. The file-based convention follows the \"convention over configuration\" paradigm\u00e2\u0080\u0094you know exactly where to look for resource definitions when you group them by type in separate files. Per-developer cloud sandbox environments are optimized for faster iterations. Each developer on a team gets an isolated cloud development environment against which they can test their changes. These cloud sandbox environments are meant for local development only, but they deploy high-fidelity AWS backends while you build. Depending on the workflow, iterative updates are now deployed up to 8X faster than Gen 1 deployments. In the diagram below, four developers are able to work on fullstack features independently without disrupting each other's environments. All shared environments (such as production , staging , gamma ) map 1:1 to Git branches in your repository. New features can be tested in ephemeral environments with pull request previews (or feature branches) before they are merged into production. Unlike the Gen 1 experience, which requires users to configure a number of steps in the CLI or Console to set up a fullstack environment, the Gen 2 experience is zero-config. Because of our code-first approach, the Git repository is always the source of truth for the state of the fullstack app\u00e2\u0080\u0094all backend resources are defined as code for reproducibility and portability across branches. This, along with central management of environment variables and secrets, simplifies the promotion workflow from lower to upper environments. All branches can be managed in the new Amplify console. The Amplify Gen 2 console provides a single place for you to manage your builds, hosting settings (such as custom domains), deployed resources (such as data browser or user management), and environment variables and secrets. Even though you can access deployed resources directly in other AWS service consoles, the Amplify console will offer a first-party experience for the categories almost every app needs\u00e2\u0080\u0094data, auth, storage, and"}, {"source": "data/raw_pages/react_how-amplify-works_concepts.txt", "text": "functions. For example, with Data, Amplify offers an API playground and a data manager (coming soon) with relationship building, seed data generation, and file upload capabilities. The @aws-amplify/backend library offers a TypeScript-first Data library for setting up fully typed real-time APIs (powered by AWS AppSync GraphQL APIs) and NoSQL databases (powered by Amazon DynamoDB tables). After you generate an Amplify backend, you will have an amplify/data/resource.ts file, which will contain your app's data schema. The defineData function turns the schema into a fully functioning data backend with all the boilerplate handled automatically. The schema-based approach is an evolution of the Amplify GraphQL API in Gen 1. It offers several benefits, including dot completion, IntelliSense, and type validation. A data model for a chat app may look something like this, for example: On your app's frontend, you can use the generateClient function, which provides a typed client instance, making it easy to integrate CRUD (create, read, update, delete) operations for your models in your application code. Gen 2 automatically generates your types without the explicit codegen step that was part of Gen 1. Auth works similarly to data. You can configure the authentication settings you want for your app in amplify/auth/resource.ts . If you want to change the verification email's subject line, you can change out the default generated code with the following: You can customize your authentication flow with customized sign-in and registration flows, multi-factor authentication (MFA), and third-party social providers. Amplify deploys an Amazon Cognito instance in your AWS account when you add auth to your app. Then, you could use the Amplify Authenticator component or the client libraries to add user flows. Amplify makes it easy to quickly build web app user interfaces using the UI component library, Figma-to-code generation, and CRUD form-generation capabilities. Learn more. Gen 2 is layered on top of AWS Cloud Development Kit (CDK) \u00e2\u0080\u0094the Data and Auth capabilities in @aws-amplify/backend wrap L3 AWS CDK constructs. As a result, extending the resources generated by Amplify does not require any special configuration. The following example adds Amazon Location Services by adding a file: amplify/custom/maps/resource.ts . This is then included in the amplify/backend.ts file so it gets deployed as part of your Amplify app. Amplify is designed to work with your existing AWS resources and configurations. For example, you can use Amplify's pre-built authentication UI components with an existing Amazon Cognito user pool you created and configured separately. Or you can display images and files from an existing Amazon S3 bucket in your app's user interface by integrating with Amplify Storage. Amplify's libraries provide an interface to leverage your existing AWS services so that you can adopt Amplify's capabilities incrementally into your current workflows, without disrupting your existing backend infrastructure. Now that you have a conceptual understanding of AWS Amplify's capabilities, complete the quickstart tutorial to put it into action in an app."}, {"source": "data/raw_pages/react_build-a-backend_data_override-resources.txt", "text": "Modify Amplify-generated AWS resources Amplify GraphQL API uses a variety of auto-generated, underlying AWS services and resources. You can customize these underlying resources to optimize the deployed stack for your specific use case. In your Amplify app, you can access every underlying resource using CDK \"L2\" or \"L1\" constructs. Access the generated resources as L2 constructs via the .resources property on the returned stack or access the generated resources as L1 constructs using the .resources.cfnResources property. Apply all the customizations on backend.data.resources.graphqlApi or backend.data.resources.cfnResources.cfnGraphqlApi . For example, to enable X-Ray tracing for the AppSync GraphQL API: Pass in the model type name into backend.data.resources.amplifyDynamoDbTables[\"MODEL_NAME\"] to modify the resources generated for that particular model type. For example, to enable time-to-live on the Todo @model type's DynamoDB table: Set the DynamoDB billing mode for the DynamoDB table as either \"PROVISIONED\" or \"PAY_PER_REQUEST\". Override the default ProvisionedThroughput provisioned for each model table and its Global Secondary Indexes (GSI). This override is only valid if the \"DynamoDBBillingMode\" is set to \"PROVISIONED\". Enable/disable DynamoDB point-in-time recovery for each model table."}, {"source": "data/raw_pages/react_build-a-backend_auth_advanced-workflows.txt", "text": "Advanced workflows You can take specific actions when users sign-in or sign-out by subscribing to authentication events in your app. Please see our Hub Module Developer Guide for more information. You can alternatively create your own custom credentials provider to get AWS credentials directly from Cognito Federated Identities and not use User Pool federation. You must supply the custom credentials provider to Amplify via the Amplify.configure method call. Below, you can see sample code of how such a custom provider can be built to achieve the use case. Now that the custom credentials provider is built and supplied to Amplify.configure , let's look at how you can use the custom credentials provider to finish federation into Cognito identity pool. You can use Auth0 as one of the providers of your Cognito Identity Pool. This will allow users authenticated via Auth0 have access to your AWS resources. Step 1. Follow Auth0 integration instructions for Cognito Federated Identity Pools Step 2. Login with Auth0 , then use the id token returned to get AWS credentials from Cognito Federated Identity Pools using custom credentials provider you created at the start: With the triggers property of defineAuth and defineFunction from the new Functions implementation, you can define Lambda Triggers for your Cognito User Pool. These enable you to add custom functionality to your registration and authentication flows. Check out a preSignUp hook example here. If you have a Pre Authentication Lambda trigger enabled, you can pass clientMetadata as an option for signIn . This metadata can be used to implement additional validations around authentication. Many Cognito Lambda Triggers also accept unsanitized key/value pairs in the form of a clientMetadata attribute. This attribute can be specified for various Auth APIs which result in Cognito Lambda Trigger execution. These APIs include: signIn signUp confirmSignIn confirmSignUp resetPassword confirmResetPassword resendSignUpCode updateUserAttributes Please note that some of triggers which accept a validationData attribute will use clientMetadata as the value for validationData . Exercise caution with using clientMetadata when you are relying on validationData . You can use AWS Service Interface Objects to work with AWS Services in authenticated State. You can call methods on any AWS Service interface object by passing your credentials from Amplify fetchAuthSession to the service call constructor: Note: To work with Service Interface Objects, your Amazon Cognito users' IAM role must have the appropriate permissions to call the requested services. Create a custom Auth token provider for situations where you would like provide your own tokens for a service. For example, using OIDC Auth with AppSync. You must supply the token provider to Amplify via the Amplify.configure method call. Below, you can see sample code of how such a custom provider can be built to achieve the use case. For the complete API documentation for Authentication module, visit our API Reference"}, {"source": "data/raw_pages/react_build-a-backend_auth_manage-users_manage-passwords.txt", "text": "Manage passwords Amplify Auth provides a secure way for your users to change their password or recover a forgotten password. By default, your users can retrieve access to their accounts if they forgot their password by using either their phone or email. The following are the default account recovery methods used when either phone or email are used as login options. To reset a user's password, use the resetPassword API which will send a reset code to the destination (e.g. email or SMS) based on the user's settings. To complete the password reset process, invoke the confirmResetPassword API with the code your user received and the new password they want to set. You can update a signed in user's password using the updatePassword API. You can always change the channel used by your authentication resources by overriding the following setting. By default your password policy is set to the following: MinLength : 8 characters requireLowercase : true requireUppercase : true requireNumbers : true requireSymbols : true tempPasswordValidity : 3 days You can customize the password format acceptable by your auth resource by modifying the underlying cfnUserPool resource:"}, {"source": "data/raw_pages/react_build-a-backend_data_customize-authz_using-oidc-authorization-provider.txt", "text": "Use OpenID Connect as an authorization provider Private, owner, and group authorization can be configured with an OpenID Connect (OIDC) authorization mode. Add \"oidc\" to the authorization rule as the provider. Use the oidcAuthorizationMode property to configure the OpenID Connect provider name , OpenID Connect provider domain , Client ID , Issued at TTL , and Auth Time TTL . The example below highlights the supported authorization strategies with a oidc authorization provider. For owner and group-based authorization, you also will need to specify a custom identity and group claim . In your application, you can perform CRUD operations against the model using client.models.<model-name> by specifying the oidc auth mode. import { generateClient } from 'aws-amplify/data' ; import type { Schema } from '../amplify/data/resource' ; const client = generateClient < Schema > ( ) ; const { errors , data : todos } = await client . models . Todo . list ( { Copy highlighted code example"}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_multi-factor-authentication.txt", "text": "Multi-factor authentication Amplify Auth supports multi-factor authentication (MFA) for user sign-in flows. MFA is an extra layer of security used to make sure that users trying to gain access to an account are who they say they are. It requires users to provide additional information to verify their identity. Amplify Auth supports MFA with time-based one-time passwords (TOTP), text messages (SMS), and email. In this guide we will review how you can set up MFA with each of these methods and the discuss tradeoffs between them to help you choose the right setup for your application. We will also review how to set up MFA to remember a device and reduce sign-in friction for your users. Use defineAuth to enable MFA for your app. The example below is setting up MFA with TOTP but not SMS as you can see that the phone number is not a required attribute. If you plan to use SMS for MFA, then the phoneNumber attribute must be marked as required in your userAttributes . Note that if you have loginWith.phone as true this attribute will automatically be marked as required. If you plan to use email for MFA, then the email attribute must also be true must be marked as required in your userAttributes . Note that if you have loginWith.email as true this attribute will automatically be marked as required. When MFA is REQUIRED with SMS in your backend auth resource, you will need to pass the phone number during sign-up API call. If you are using the email or username as the primary sign-in mechanism, you will need to pass the phone_number attribute as a user attribute. Similarly, when MFA is REQUIRED with email as your delivery mechanism, you will need to pass an email address during the sign-up API call. If you are using phoneNumber or username as the primary sign-in mechanism, you will need to pass the email attribute as a user attribute. This configuration may change depending on the combination of MFA methods enabled in your user pool. When enabling MFA you will have two key decisions to make: MFA enforcement: As part of this setup you will determine how MFA is enforced. If you require MFA by setting MFA mode to REQUIRED , all your users will need to complete MFA to sign in. If you keep it OPTIONAL , your users will have the choice whether to enable MFA or not for their account. MFA methods: You will also specify which MFA method you are using: TOTP (Time-based One-time Password), SMS (text message), email, or any combination thereof. We recommend that you use TOTP-based MFA as it is more secure and you can reserve SMS or email for account recovery. Compare TOTP, SMS, and EMAIL MFA methods Close accordion If multiple MFA methods are enabled for the user, and none are set as preferred, the signIn API will return CONTINUE_SIGN_IN_WITH_MFA_SELECTION as the next step in the auth flow. During this scenario, the user should be prompted to"}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_multi-factor-authentication.txt", "text": "select the MFA method they want to use to sign in and their preference should be passed to confirmSignIn . If you are using the Authenticator component with Amplify, this feature works without any additional code. The guide below is for writing your own implementation. Once you have setup SMS as your second layer of authentication with MFA as shown above, your users will get an authentication code via a text message to complete sign-in after they sign in with their username and password. You will need to pass phone_number as a user attribute to enable SMS MFA for your users during sign-up. However, if the primary sign-in mechanism for your Cognito resource is phone_number (without enabling username ), then you do not need to pass it as an attribute. By default, you have to verify a user account after they sign up using the confirmSignUp API, which will send a one-time password to the user's phone number or email, depending on your Amazon Cognito configuration. After a user signs in, if they have MFA enabled for their account, a challenge will be returned that you would need to call the confirmSignIn API where the user provides their confirmation code sent to their phone number. If MFA is ON or enabled for the user, you must call confirmSignIn with the OTP sent to their phone. After a user has been signed in, call updateMFAPreference to record the MFA type as enabled for the user and optionally set it as preferred so that subsequent logins default to using this MFA type. If you are using the Authenticator component with Amplify, this feature works without any additional code. The guide below is for writing your own implementation. You can use Time-based One-Time Password (TOTP) for multi-factor authentication (MFA) in your web or mobile applications. The Amplify Auth category includes support for TOTP setup and verification using authenticator apps, offering an integrated solution and enhanced security for your users. These apps, such as Google Authenticator, Microsoft Authenticator, have the TOTP algorithm built-in and work by using a shared secret key and the current time to generate short-lived, six digit passwords. After you initiate a user sign in with the signIn API where a user is required to set up TOTP as an MFA method, the API call will return CONTINUE_SIGN_IN_WITH_TOTP_SETUP as a challenge and next step to handle in your app. You will get that challenge if the following conditions are met: MFA is marked as Required in your user pool. TOTP is enabled in your user pool. User does not have TOTP MFA set up already. The CONTINUE_SIGN_IN_WITH_TOTP_SETUP step signifies that the user must set up TOTP before they can sign in. The step returns an associated value of type TOTPSetupDetails which must be used to configure an authenticator app like Microsoft Authenticator or Google Authenticator. TOTPSetupDetails provides a helper method called getSetupURI which generates a URI that can be used, for example, in a button to open the user's installed authenticator"}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_multi-factor-authentication.txt", "text": "app. For more advanced use cases, TOTPSetupDetails also contains a sharedSecret which can be used to either generate a QR code or be manually entered into an authenticator app. Once the authenticator app is set up, the user can generate a TOTP code and provide it to the library to complete the sign in process. The TOTP code can be obtained from the user via a text field or any other means. Once the user provides the TOTP code, call confirmSignIn with the TOTP code as the challengeResponse parameter. After a user has been signed in, call updateMFAPreference to record the MFA type as enabled for the user and optionally set it as preferred so that subsequent logins default to using this MFA type. TOTP MFA can be set up after a user has signed in. This can be done when the following conditions are met: MFA is marked as Optional or Required in your user pool. TOTP is marked as an enabled MFA method in your user pool. TOTP can be set up by calling the setUpTOTP and verifyTOTPSetup APIs in the Auth category. Invoke the setUpTOTP API to generate a TOTPSetupDetails object which should be used to configure an Authenticator app like Microsoft Authenticator or Google Authenticator. TOTPSetupDetails provides a helper method called getSetupURI which generates a URI that can be used, for example, in a button to open the user's installed Authenticator app. For more advanced use cases, TOTPSetupDetails also contains a sharedSecret which can be used to either generate a QR code or be manually entered into an Authenticator app. that contains the sharedSecret which will be used to either to generate a QR code or can be manually entered into an Authenticator app. Once the Authenticator app is set up, the user must generate a TOTP code and provide it to the library. Pass the code to verifyTOTPSetup to complete the TOTP setup process. After TOTP setup is complete, call updateMFAPreference to record the MFA type as enabled for the user and optionally set it as preferred so that subsequent logins default to using this MFA type. If a user loses access to their TOTP device, they will need to contact an administrator to get help accessing their account. Based on the Cognito user pool configuration, the administrator can use the AdminSetUserMFAPreference to either change the MFA preference to a different MFA method or to disable MFA for the user. In a scenario where MFA is marked as \"Required\" in the Cognito User Pool and another MFA method is not set up, the administrator would need to first initiate an AdminUpdateUserAttributes call and update the user's phone number attribute. Once this is complete, the administrator can continue changing the MFA preference to SMS as suggested above. Once you have setup email as your second layer of authentication with MFA as shown above, your users will get an authentication code via email to complete sign-in after they sign in with their username and password. In order"}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_multi-factor-authentication.txt", "text": "to send email authentication codes, the following prerequisites must be met: You will need to pass email as a user attribute to enable email MFA for your users during sign-up. However, if the primary sign-in mechanism for your Cognito resource is already email (without enabling username ), then you do not need to pass it as an attribute. By default, you have to verify a user account after they sign up using the confirmSignUp API. Following the initial signUp request, a one-time passcode will be sent to the user's phone number or email, depending on your Amazon Cognito configuration. After a user signs in, if they have MFA enabled for their account, a challenge will be issued that requires calling the confirmSignIn API with the user provided confirmation code sent to their email address. If MFA is ON or enabled for the user, you must call confirmSignIn with the OTP sent to their email address. After a user has been signed in, call updateMFAPreference to record the MFA type as enabled for the user and optionally set it as preferred so that subsequent logins default to using this MFA type. Depending on your user pool configuration, it's possible that multiple MFA options may be available to a given user. In order to avoid requiring your users to select an MFA method each time they sign-in to your application, Amplify provides two utility APIs to manage an individual user's MFA preferences. Invoke the following API to get the current MFA preference and enabled MFA types, if any, for the current user. Invoke the following API to update the MFA preference for the current user. Only one MFA method can be marked as preferred at a time. If the user has multiple MFA methods enabled and tries to mark more than one MFA method as preferred, the API will throw an error. Remembering a device is useful in conjunction with MFA because it allows the second factor requirement to be automatically met when your user signs in on that device and reduces friction in their sign-in experience. By default, this feature is turned off. Note: The device tracking and remembering features are not available if any of the following conditions are met: the federated OAuth flow with Cognito User Pools or Hosted UI is used, or when the signIn API uses the USER_PASSWORD_AUTH as the authFlowType . You can configure device tracking with deviceTracking construct. Understand key terms used for tracking devices There are differences to keep in mind when working with remembered, forgotten, and tracked devices. Tracked: Every time the user signs in with a new device, the client is given the device key at the end of a successful authentication event. We use this device key to generate a salt and password verifier which is used to call the ConfirmDevice API. At this point, the device is considered to be \"tracked\". Once the device is in a tracked state, you can use the Amazon Cognito console to see the time"}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_multi-factor-authentication.txt", "text": "it started to be tracked, last authentication time, and other information about that device. Remembered: Remembered devices are also tracked. During user authentication, the device key and secret pair assigned to a remembered device is used to authenticate the device to verify that it is the same device that the user previously used to sign in. Not Remembered: A not-remembered device is a tracked device where Cognito has been configured to require users to \"Opt-in\" to remember a device but the user has chosen not to remember the device. This use case is for users signing into their application from a device that they don't own. Forgotten: In the event that you no longer want to remember or track devices, you can use the forgetDevice() API to remove devices from being both remembered and tracked. Close accordion"}, {"source": "data/raw_pages/javascript_start_quickstart.txt", "text": "Quickstart \u00f0\u009f\u0091\u008b Welcome to AWS Amplify! In this quickstart guide, you will: Deploy a Vanilla JavaScript app with Vite Build and connect to a database with real-time data updates Configure authentication and authorization rules Create a new Vanilla JavaScript app with vite using the following commands, create the directory ( amplify-js-app ) and files for the app. Initialize npm and install dependencies and dev dependencies. This runs a development server and allows you to see the output generated by the build. You can see the running app by navigating to http://localhost:5173 . Add the following to the index.html file: Add the following to style.css file: In main.js remove the boilerplate code and leave it empty. Then refresh the browser to see the changes. The easiest way to get started with AWS Amplify is through npm with create-amplify command. You can run it from your base project directory. Running this command will scaffold Amplify backend files in your current project with the following files added: To make backend updates, we are going to require AWS credentials to deploy backend updates from our local machine. Skip ahead to step 8 , if you already have an AWS profile with credentials on your local machine, and your AWS profile has the AmplifyBackendDeployFullAccess permission policy. Otherwise, set up local AWS credentials that grant Amplify permissions to deploy backend updates from your local machine. To deploy your backend use Amplify's per-developer cloud sandbox. This feature provides a separate backend environment for every developer on a team, ideal for local development and testing. To run your application with a sandbox environment, you can run the following command: Once the sandbox environment is deployed, it will create a GraphQL API, database, and auth service. All the deployed resources will be available in the amplify_outputs.json . The initial scaffolding already has a pre-configured data backend defined in the amplify/data/resource.ts file. The default example will create a Todo model with content field. Update your main.js file to create new to-do items. That's it! You have successfully built a fullstack app on AWS Amplify. If you want to learn more about how to work with Amplify, here's the conceptual guide for how Amplify works ."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_analytics.txt", "text": "Set up Amplify Analytics The Analytics category enables you to collect analytics data for your app. The Analytics category comes with built-in support for Amazon Pinpoint and Amazon Kinesis (Kinesis support is currently only available in the Amplify JavaScript library). The Analytics category uses Amazon Cognito Identity pools to identify users in your App. Cognito allows you to receive data from authenticated, and unauthenticated users in your App."}, {"source": "data/raw_pages/react_build-a-backend_data_customize-authz_grant-lambda-function-access-to-api.txt", "text": "Grant Lambda function access to API and Data Function access to defineData can be configured using an authorization rule on the schema object. Create a new directory and a resource file, amplify/functions/data-access/resource.ts . Then, define the Function with defineFunction : The object returned from defineFunction can be passed directly to allow.resource() in the schema authorization rules. This will grant the function the ability to execute Query, Mutation, and Subscription operations against the GraphQL API. Use the .to() method to narrow down access to one or more operations. Function access can only be configured on the schema object. It cannot be configured on individual models or fields. In the handler file for your function, configure the Amplify data client When configuring Amplify with getAmplifyDataClientConfig , your function consumes schema information from an S3 bucket created during backend deployment with grants for the access your function need to use it. Any changes to this bucket outside of backend deployment may break your function. Once you have generated the client code, update the function to access the data. The following code creates a todo and then lists all todos."}, {"source": "data/raw_pages/react_build-a-backend_data_customize-authz_per-user-per-owner-data-access.txt", "text": "Per-user/per-owner data access The owner authorization strategy restricts operations on a record to only the record's owner. When configured, the owner field will automatically be added and populated with the identity of the created user. The API will authorize against the owner field to allow or deny operations. You can use the owner authorization strategy to restrict a record's access to a specific user. When owner authorization is configured, only the record's owner is allowed the specified operations. In your application, you can perform CRUD operations against the model using client.models.<model-name> with the userPool auth mode. import { generateClient } from 'aws-amplify/data' ; import type { Schema } from '../amplify/data/resource' ; const client = generateClient < Schema > ( ) ; const { errors , data : newTodo } = await client . models . Todo . create ( Copy highlighted code example Behind the scenes, Amplify will automatically add a owner: a.string() field to each record which contains the record owner's identity information upon record creation. By default, the Cognito user pool's user information is populated into the owner field. The value saved includes sub and username in the format <sub>::<username> . The API will authorize against the full value of <sub>::<username> or sub / username separately and return username . You can alternatively configure OpenID Connect as an authorization provider . By default, owners can reassign the owner of their existing record to another user. To prevent an owner from reassigning their record to another user, protect the owner field (by default owner: String ) with a field-level authorization rule . For example, in a social media app, you would want to prevent Alice from being able to reassign Alice's Post to Bob. You can override the owner field to your own preferred field, by specifying a custom ownerField in the authorization rule."}, {"source": "data/raw_pages/swift_start_quickstart.txt", "text": "Quickstart Before you get started, make sure you have the following installed: Open Xcode and select Create New Project... In the next step select the App template under iOS . Click on next. Next steps are: Adding a Product Name (e.g. MyAmplifyApp) Select a Team (e.g. None) Select a Organization Identifier (e.g. com.example) Select SwiftUI an Interface . Press Next Now you should have your project created. Close accordion The easiest way to get started with AWS Amplify is through npm with create-amplify command. You can run it from your base project directory. Running this command will scaffold Amplify backend files in your current project with the following files added: To deploy your backend use Amplify's per-developer cloud sandbox. This feature provides a separate backend environment for every developer on a team, ideal for local development and testing. To run your application with a sandbox environment, you can run the following command: Once the sandbox environment is deployed, it will create an amplify_outputs.json . However, Xcode won't be able to recognize them. For recognizing the files, you need to drag and drop the generated files to your project. The initial scaffolding already has a pre-configured auth backend defined in the amplify/auth/resource .ts file. We've configured it to support email and password login but you can extend it to support a variety of login mechanisms, including Google, Amazon, Sign In With Apple, and Facebook. The fastest way to get your login experience up and running is to use our Authenticator UI component available in the Amplify UI library. To use the Authenticator, open your project in Xcode and select File > Add Packages... and add the following dependencies: Now update the MyAmplifyAppApp class with the following code: Update ContentView with the following code: The Authenticator component auto-detects your auth backend settings and renders the correct UI state based on the auth backend's authentication flow. Run your application in your local environment again. You should be presented with a login experience now. The initial scaffolding already has a pre-configured data backend defined in the amplify/data/resource.ts file. The default example will create a Todo model with content field. Let's modify this to add the following: A boolean isDone field. An authorization rules specifying owners, authenticated via your Auth resource can \"create\", \"read\", \"update\", and \"delete\" their own records. Update the defaultAuthorizationMode to sign API requests with the user authentication token. Next, let's implement UI to create, list, and delete the to-do items. Amplify can automatically generate code for interacting with the backend API. The command below generates model classes from the Data schema: Move the generated files to your project. You can do this by dragging and dropping the files to your project. Once you are done, add the API dependencies to your project. Select File > Add Package Dependencies... and add the AWSAPIPlugin . After adding the dependencies, update the import part of your MyAmplifyAppApp.swift file with the following code: Then, update the init() part of your MyAmplifyAppApp.swift file with the following"}, {"source": "data/raw_pages/swift_start_quickstart.txt", "text": "code: Create a new file called TodoViewModel.swift and the createTodo function with the following code: The code above will create a random todo with the current time. Next, update the listTodos function in the TodoViewModel.swift for listing to-do items: This will assign the value of the fetched todos into a Published object. Now let's update the UI code to observe the todos. Update the VStack in the ContentView.swift file with the following code: Throughout the Swift implementation, the async/await pattern has been used and for using it easily, we take advantage of the Task structure. For more information about the Task structure, you can check the documentation . The code will create a todo and update the todo list each time a todo is created. Next step is to update and delete the todos. For that, create updateTodo and deleteTodo functions in the TodoViewModel.swift file with the following code: Update the List in the ContentView.swift file with code to fetch the todos when the View is displayed and to call deleteTodos(indexSet:) when the user left-swipe a todo. Lastly, create a new file called TodoRow.swift with the following code: This will update the UI to show a toggle to update the todo isDone and a swipe to delete the todo. Now if you run the application you should see the following flow. You can terminate the sandbox environment now to clean up the project. Publishing changes to the cloud requires a remote git repository. Amplify offers fullstack branch deployments that allow you to automatically deploy infrastructure and application code changes from feature branches. To learn more, visit the fullstack branch deployments guide . That's it! You have successfully built a fullstack app on AWS Amplify. If you want to learn more about how to work with Amplify, here's the conceptual guide for how Amplify works ."}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_user-attributes.txt", "text": "User attributes Amplify Auth stores user profile information in user attributes. When the default method for user sign-in, Amplify Auth will automatically configure an email or phoneNumber attribute that is required for sign-in. To extend a user profile beyond the default email or phoneNumber attribute that is automatically configured when specified in your auth resource's loginWith property, you can configure attributes with the userAttributes property: Warning : After you create your auth resource, you cannot switch an attribute between required and not required. User attributes are defined as Cognito Standard Attributes . Attributes can be configured to be required for user sign-up in addition to whether the values are mutable . When configuring your resource to allow your users to login with email , an email must be specified for user sign-up and cannot be changed later. However additional attributes can be configured to be optional, and mutable after sign-up. In addition to the provided standard attributes, you can configure Custom Attributes . These are attributes that are typically unique to your use case, such as a tenant ID or a user's display name. Custom attributes are identified by the custom: prefix: Unlike standard attributes, custom attributes cannot natively be required for sign-up, however can be codified to require some value by validating user attributes upon sign-up with a pre sign-up trigger . Custom attributes can also be configured with specific data types. The following data types are supported: String Number Boolean DateTime Shown in the snippet above, String and Number can be assigned minimum and maximum constraints. This is useful to defer simple validations to the underlying service, although does not extend to complex validations such as matching against a regular expression."}, {"source": "data/raw_pages/react_build-a-backend.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_functions_examples.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_predictions_set-up-predictions.txt", "text": "Set up Predictions To enable Predictions we need to set up the appropriate IAM policy for Roles in your Cognito Identity Pool in order to use an appropriate feature. Additionally, we need to use the addOutput method to patch the custom Predictions resource to the expected output configuration. To install the Amplify library to use predictions features, run the following commands in your project's root folder: Import and load the configuration file in your app. It is recommended you add the Amplify configuration step to your app's root entry point. For example main.ts in React and Angular."}, {"source": "data/raw_pages/react_build-a-backend_data_manage-with-amplify-console.txt", "text": "Manage Data with Amplify console The Data manager page in the Amplify Console offers a user-friendly interface for managing the backend GraphQL API data of an application. It enables real-time creation and updates of application data, eliminating the need to build separate admin views. If you have not yet created a data resource, visit the Data setup guide . After you've deployed your data resource, you can access the manager on Amplify console. Log in to the Amplify console and choose your app. Select the branch you would like to access. Select Data from the left navigation bar. Then, select Data manager . On the Data manager page, select a table from the Select table dropdown. For this example, we are using a Todo table. Select Create Todo . In the Add Todo pane, specify your custom values for the fields in the table. For example, enter my first todo for the Content field and toggle the Is done field. Select Submit . On the Data manager page, select a table from the Select table dropdown. From the list of records, select a record you want to update. In the Edit Todo pane, make any changes to your record, and then select Submit . On the Data manager page, select a table from the Select table dropdown. From the list of records, select the checkbox to the left of the record(s) you want to delete. Select the Actions dropdown, and then select delete item(s) . On the Data manager page, select a table from the Select table dropdown. Select the Actions dropdown and then select Auto-generate data . In the Auto-generate data pane, specify how many rows of data you want to generate and constraints for the generated data. Then select Generate data You can generate up to 100 records at a time. Seed data cannot be generated for tables that have the following field types: AWSPhone, Enum, Custom Type, or Relationship On the Data manager page, select a table from the Select table dropdown. Select the Actions dropdown. Here you have two options for downloading data. Choose Download selected items (.csv) to download only the selected rows of data. Choose Download all items (.csv) to download all rows of records on the currently selected table. Once you have selected a download option, your data should immediately start downloading as a CSV file."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_set-up-geo.txt", "text": "Set up Amplify Geo Amplify provides APIs and map UI components for maps and location search for your web apps.You can add maps and location search functionality to your app in just a few lines of code. The following is an example utilizing the AWS Cloud Development Kit (AWS CDK) to create a Geo resource powered by Amazon Location Services . But do note there are no official hand-written (L2) constructs for this service yet. To display a map in your application, you can use the Amplify React MapView component or the MapLibre GL with maplibre-gl-js-amplify libraries are required. Install the necessary dependencies by running the following command: Note: Make sure that version 6.0.0 or above is installed. Import and load the configuration file in your app. It's recommended you add the Amplify configuration step to your app's root entry point. Make sure you call Amplify.configure as early as possible in your application\u00e2\u0080\u0099s life-cycle. A missing configuration or NoCredentials error is thrown if Amplify.configure has not been called before other Amplify JavaScript APIs. Notes: If you want to use existing Amazon Location Service resources follow this guide instead. If you want to use Amazon Location Service APIs not directly supported by Geo, use the escape hatch to access the Amazon Location Service SDK. Location Construct Library The pricing plan for the Map example is set to RequestBasedUsage . We advice you to go through the location service pricing along with the location service terms ( 82.5 section ) to learn more about the pricing plan."}, {"source": "data/raw_pages/react_build-a-backend_functions_examples_kinesis-stream.txt", "text": "Amazon Kinesis Data Streams With AWS Lambda, you can seamlessly integrate various event sources, such as Amazon Kinesis, Amazon SQS, and others, to trigger Lambda functions in response to real-time events. This feature enables you to build responsive, event-driven applications that react to changes in data or system state without the need for polling services. In this guide, let us configure a Lambda function with a Kinesis data stream as an event source. The Lambda function is automatically triggered whenever new data is published to the stream - whether you're processing streaming data, reacting to application events, or automating workflows. To get started, install the AWS Lambda Powertools Logger, which provides structured logging capabilities for your Lambda function, and the aws-lambda package, which is used to define the handler type. Second, create a new directory and a resource file, amplify/functions/kinesis-function/resource.ts . Then, define the function with defineFunction : Third, create the corresponding handler file, amplify/functions/kinesis-function/handler.ts , file with the following contents: Lastly, create the Kinesis stream and add it as a event source in the amplify/backend.ts file: For examples on streaming analytics data to the Kinesis stream from your frontend, see the Streaming analytics data documentation."}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching_custom-pipelines.txt", "text": "Custom pipelines While building with Amplify CI/CD offers benefits such as zero-config setup, fullstack previews, centralized secrets management, Amplify Gen 2 makes it easy to integrate fullstack CI/CD into your custom pipelines (for example, AWS CodePipeline, Amazon CodeCatalyst, GitHub Actions, and more). You can set up your backend deployments using the following steps: Create an Amplify app by connecting a fullstack Gen 2 branch from your Git repository. This is a one time setup as for subsequent deployments, we will be using a custom pipeline. Disable Auto-build for your branch. This will ensure code commits to your branch will not trigger a build. Update the Amplify build specification file to add npx ampx generate outputs --branch $AWS_BRANCH --app-id $AWS_APP_ID and comment out the pipeline-deploy script. ampx pipeline-deploy runs a script to deploy backend updates, while ampx generate outputs fetches the latest amplify_outputs.json for the specified environment. Now go to your pipeline provider and update the build settings to include the following: Run npm ci . Run export CI=1 to tell the deployment script that is a CI environment. Run npx ampx pipeline-deploy --branch BRANCH_NAME --app-id AMPLIFY_APP_ID . BRANCH_NAME refers to the branch you're deploying. AMPLIFY_APP_ID is the Amplify App ID. To locate the App ID for your backend application, navigate to the Amplify console and select your backend-app . On the Overview page, the App ID is displayed under the project name. The example below demonstrates how you would set up the build-spec when using Amazon CodeCatalyst. Trigger a git push to your branch. Your build logs should show that there is an AWS CloudFormation deployment underway. If you want to complete the fullstack CI/CD setup, we have to build, deploy, and host the frontend in addition to the backend. Use the Amplify Console to create an incoming webhook . Navigate to the frontend app, under Hosting > Build settings select Create webhook . Provide a name for the webhook and select the target branch to build on incoming webhook requests. Next, select the webhook and copy the curl command which will be used to trigger a build for the frontend app. Now update your custom-pipeline build settings to include the curl command to trigger a frontend build after the pipeline-deploy succeeds. Using the same Amazon CodeCatalyst example above, this step includes: This should trigger a build in your Amplify app. Amplify CI will build and first generate the amplify_outputs.json for the branch and then build, deploy, and host the frontend."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_rest-api.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_auth_reference.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_ai_conversation_ai-conversation.txt", "text": "<AIConversation> Note: the example is a mocked component and not hooked up to a live backend The <AIConversation> component is highly customizable to fit into any application. The component is built so that it works with the useAIConversation hook. The hook manages the state and lifecycle of the component. The component by itself is just a renderer for the conversation state, which the hook provides. The <AIConversation> component requires some props: messages an array of the messages in the conversation handleSendMessage a handler that is called when a user message is sent. The useAIConversation hook provides these values and manages the messages state as user messages are sent and assistant responses are streamed back. The code above won't really do much, but if you wanted to play around with the component or visually test how it will look, you can do that passing in your own set of messages. Make sure to first follow our getting started guide for the Amplify AI kit to set up your Amplify AI backend. Conversations required a logged in user, so we recommend using the <Authenticator> component to easily add authentication flows to your app. LLMs can respond with markdown. The <AIConversation> component does not have built-in markdown rendering, but does allow for you to pass in your own markdown renderer. The messageRenderer property lets you customize how markdown is rendered within the chat according to your application's needs. The example below demonstrates how to add code syntax highlighting by using ReactMarkdown with rehypeHighlight . The <AIConversation> component renders images in the conversation history by default. You can also customize how images are rendered with messageRenderer , similar to the text example above. You can have the <AIConversation> component display a welcome message when a user starts a new conversation. The welcome message will disappear once a message has been sent. All messages have a timestamp associated with them that are displayed next to the username. To customize how the timestamp displays you can pass a custom text formatter function called getMessageTimestampText into the displayText property on the <AIConversation> component. This function will receive a Date object as its argument and should return a string. Browsers have a really nice built-in date/time formatter you can use called Intl.DateTimeFormat . You could also return an empty string if you wanted to hide the timestamps altogether. Some of the newer LLMs like the Claude 3 family of models from Anthropic support multi-modal input, so you can send images in your message to the model and it can respond based on the messages. To enable this functionality in the component, there is an allowAttachments prop you can enable. There are some limitations on the filetype and size of the images attached. The file size for each file should be below 400kb when base64 encoded. Also the currently supported file types are: png, jpg, gif, and webp. You can customize the usernames and avatars used in the AIConversation component by using the avatars prop. This lets you control"}, {"source": "data/raw_pages/react_ai_conversation_ai-conversation.txt", "text": "what your AI assistant looks like in the chat and what your user's username and avatar are. There are 2 avatars, user and ai , and each have a username and avatar attribute. The avatar is a React Node and the username is a string. Response components are a way to define custom UI components that the LLM can respond with in the conversation. This creates a richer experience than just text responses so the conversation can be more interactive and engaging. To define a response component you need any React component and give it a name, description, and define the props the LLM should know. Response components are just plain React components; they can have their own interactive state, fetch data, update shared state, or really anything you can think of. You can pair response components with data tools , so the LLM can query for some data and then use a component to display that data. Or your response component could fetch data itself. Because response components are defined at runtime and conversation histories are stored in a database, there can be times when there is a response component in the message history that the current application does not have. Response components are saved in the message history as a \"toolUse\" block, similar to how an LLM would respond when it wants to call a tool. The toolUse block contains the name of the component, and the props the LLM wanted to pass to the component. The LLM is never directly sending UI code, but rather an abstract representation of what it wants to render. If the AIConversation component receives a response component message for a response component that was not given to it, by default it will just not render anything. However if you want to add a fallback component if no component is found based on the name, you can use the FallbackResponseComponent prop. You can think of this like a 404 page for response components."}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching_secrets-and-vars.txt", "text": "Secrets and environment vars Amplify Gen 2 offers centralized management of secrets and environment variables for all fullstack branches. Secrets allow you to securely configure environment-specific values like social sign-in keys, function environment variables, function secrets, and other sensitive data needed by your application across environments. How is this different from Amplify Gen 1? In Amplify Gen 1, you need to define environment variables and secrets using the CLI and store keys in both AWS Parameter Store and a local team-provider.json file. We have streamlined this workflow in Amplify Gen 2 by centralizing the management of secrets and environment variables in the Amplify console. Close accordion You can set secrets for your fullstack branch deployments or your local dev server. You can add secrets for branch deployments in the Amplify console. From the App home page, navigate to Hosting > Secrets , and then choose the Manage secrets button. You can add a secret key or value that applies to all deployed branches or just specific branches. Secrets are stored in AWS Systems Manager Parameter Store under the following naming conventions: Secrets that apply to all branches: /amplify/shared/<app-id>/<secret-key> Secrets that apply to a specific branch: /amplify/<app-id>/<branchname>-branch-<unique-hash>/<secret-key> Secrets set in a sandbox do not show up in the Amplify console. You can view them in the AWS Parameter Store console. When testing features locally, you might want to test with real secrets. You can add secrets while running the cloud sandbox with the following command: Once you have set a secret, you can access the values in code by calling the secret() function. The following example shows how to set up social sign-in with authentication in your app. Depending on your environment, Amplify will automatically load the correct secret value with no extra configuration. When deleting branch environments or sandbox environments, you need to manually delete the secrets as well. Secrets that are used in branch deployments can be managed directly in the Amplify console. You can remove them under Secret management by choosing Remove . To remove a secret in your local environment, run the following command in your terminal: Note: do not store secret values in environment variables. Environment variables values are rendered in plaintext to the build artifacts and can be accessed by anyone with access to the build artifacts or get-app command. Environment variables work like key-value pairs to help manage configurable settings across different deployment environments, including development, staging, and production. Unlike secrets, which store sensitive data, environment variables are typically nonconfidential and are used for controlling application behavior in different environments. Another key difference is that environment variables are stored and managed by the Amplify managed service. You can set environment variables in the Amplify console (view the AWS Amplify Hosting User Guide for detailed instructions). You can enable access to environment variables for your fullstack branch deployments or your local dev server. You can manage your branch environment access through the Amplify console. First, create an environment variable in the Amplify console (in this"}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching_secrets-and-vars.txt", "text": "example, you will name it REACT_APP_TEST_VARIABLE ) Next, navigate to the Build Settings in console (or to the amplify.yml file) and update the build settings to pipe the environment variable into a file. Here is an example of writing it into an .env file: With the implementation above, the environment variable is written in a .env file. However, you can write it to any file depending on your platform. For Flutter, you can still use .env with an external package or generate your configuration file in Dart or JSON format. For Android, you can use Build Configurations or Gradle variables. For iOS, you can update your plist file with the necessary code or create a configuration file in JSON format. Now the .env can access the environment variable through process.env in your client code: When working on your local machine, you must manually load the sandbox's environment variables. First, add the environment variable in your .env.local file. Then, a library such as @dotenvx/dotenvx can load the environment variables, which you can then reference with process.env ."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_ai_set-up-ai.txt", "text": "Set up AI In this guide, you will learn how to get stared with the Amplify AI kit. This includes defining your AI backend with Conversation and Generation routes, and securely connecting to them from your frontend application. Before you begin, you will need: Node.js v18.16.0 or later npm v6.14.4 or later git v2.14.1 or later You will also need an AWS account that is setup for local development and has access to the Bedrock Foundation Model(s) you want to use. You can request access to Bedrock models by going in to the Bedrock console and requesting access . Running inference on large language models (LLMs) can be costly. Amazon Bedrock is a serverless service so you only pay for what you use, but be mindful of the costs associated with building generative AI applications. See Bedrock pricing for more information . Run the create amplify script in your project directory: Then run the Amplify sandbox to start your local cloud sandbox: This will provision the cloud resources you define in your amplify folder and watch for updates and redeploy them. To build an AI backend, you define AI 'routes' in your Amplify Data schema. An AI route is like an API endpoint for interacting with backend AI functionality. There are currently 2 types of routes: Conversation: A conversation route is a streaming, multi-turn API. Conversations and messages are automatically stored in DynamoDB so users can resume conversations. Examples of this are any chat-based AI experience or conversational UI. Generation: A single synchronous request-response API. A generation route is just an AppSync Query. Examples of this are: generating alt text for an image, generating structured data from unstructured input, summarization, etc. To define AI routes, open your amplify/data/resource.ts file and use a.generation() and a.conversation() in your schema. Conversation routes currently ONLY support owner-based authorization and generation routes ONLY support non-owner-based authorization ( authenticated , guest , group , publicApiKey ). If you have the Amplify sandbox running, when you save this file it will pick up the changes and redeploy the necessary resources for you. Once the cloud sandbox is up and running, it will also create an amplify_outputs.json file, which includes relevant connection information to your AI routes and other Amplify configuration. To connect your frontend code to your backend, you need to: Configure the Amplify library with the Amplify client configuration file ( amplify_outputs.json ). Generate a new API client from the Amplify library. Make an API request with end-to-end type-safety. Install the Amplify client library to your project: Call Amplify.configure() with the amplify_outputs.json file where the React application is mounted. Next, generate a type-safe frontend client to talk to our backend using our backend data schema and the generateClient() function provided by the Amplify libraries. It can be helpful to create a client.ts/js file that exports the generated Amplify data client as well as the generated React hooks. AI conversations are scoped to a user, so your users will need to be logged in with Amplify auth."}, {"source": "data/raw_pages/react_ai_set-up-ai.txt", "text": "The easiest way to do this is with the Authenticator component."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_reference_amplify_outputs.txt", "text": "About amplify_outputs.json In Amplify Gen 2, the CLI will generate an amplify_outputs.json file with your backend's outputs such as your Data endpoint and Auth metadata. This file -- also known as the \"client configuration file\" -- is used to configure the client libraries in order to interact with your backend resources. Locally, this file is created while using ampx sandbox . In Amplify's CI/CD, this is created automatically for you based on the current Amplify app ID and git branch. You can also manually create this file for a specified Amplify app ID and branch, or an AWS CloudFormation stack name with ampx generate outputs . The amplify_outputs.json file is not just a static artifact; it is designed to be extendable to suit the evolving needs of your application. By leveraging the addOutput method from your backend , you can programmatically add configurations. This is particularly useful for customizing outputs that are not directly exposed through the Amplify constructs or for dynamically adjusting your app's configuration in response to changes in your backend strategy. Overriding Amplify-managed configurations on amplify_outputs.json is not supported. One common scenario where extending the configuration becomes handy is when you need to add custom outputs or extend existing configurations without manual file edits. Consider a scenario where you want to add output parameters in your amplify_outputs.json that specify an S3 bucket and its region that your application will use for storing files. In your frontend end application, you can configure Amplify as follows: In addition to extending existing configurations, you can also add custom output parameters to your amplify_outputs.json . This is useful for surfacing arbitrary outputs, values from custom CDK resources, or any other information that might be necessary for your application's logic or configuration. In your frontend application, you can access these custom configurations as follows: The Amplify outputs file is defined using a JSON schema. You can find this schema in the aws-amplify/amplify-backend repository . { \"$schema\": \"https://json-schema.org/draft/2020-12/schema\", \"$id\": \"https://amplify.aws/2024-02/outputs-schema.json\", \"title\": \"AWS Amplify Backend Outputs\", \"description\": \"Config format for Amplify Gen 2 client libraries to communicate with backend services.\", \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"$schema\": { \"description\": \"JSON schema\", \"type\": \"string\" }, \"version\": { \"description\": \"Version of this schema\", \"const\": \"1\" }, \"analytics\": { \"description\": \"Outputs manually specified by developers for use with frontend library\", \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"amazon_pinpoint\": { \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"aws_region\": { \"description\": \"AWS Region of Amazon Pinpoint resources\", \"$ref\": \"#/$defs/aws_region\" }, \"app_id\": { \"type\": \"string\" } }, \"required\": [ \"aws_region\", \"app_id\" ] } } }, \"auth\": { \"description\": \"Outputs generated from defineAuth\", \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"aws_region\": { \"description\": \"AWS Region of Amazon Cognito resources\", \"$ref\": \"#/$defs/aws_region\" }, \"user_pool_id\": { \"description\": \"Cognito User Pool ID\", \"type\": \"string\" }, \"user_pool_client_id\": { \"description\": \"Cognito User Pool Client ID\", \"type\": \"string\" }, \"identity_pool_id\": { \"description\": \"Cognito Identity Pool ID\", \"type\": \"string\" }, \"password_policy\": { \"description\": \"Cognito User Pool password policy\", \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"min_length\": { \"type\": \"integer\", \"minimum\": 6, \"maximum\":"}, {"source": "data/raw_pages/react_reference_amplify_outputs.txt", "text": "99 }, \"require_numbers\": { \"type\": \"boolean\" }, \"require_lowercase\": { \"type\": \"boolean\" }, \"require_uppercase\": { \"type\": \"boolean\" }, \"require_symbols\": { \"type\": \"boolean\" } } }, \"oauth\": { \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"identity_providers\": { \"description\": \"Identity providers set on Cognito User Pool\", \"type\": \"array\", \"items\": { \"type\": \"string\", \"enum\": [ \"GOOGLE\", \"FACEBOOK\", \"LOGIN_WITH_AMAZON\", \"SIGN_IN_WITH_APPLE\" ] }, \"minItems\": 0, \"uniqueItems\": true }, \"domain\": { \"description\": \"Domain used for identity providers\", \"type\": \"string\" }, \"scopes\": { \"type\": \"array\", \"items\": { \"type\": \"string\" }, \"minItems\": 0, \"uniqueItems\": true }, \"redirect_sign_in_uri\": { \"description\": \"URIs used to redirect after signing in using an identity provider\", \"type\": \"array\", \"items\": { \"type\": \"string\" }, \"minItems\": 1, \"uniqueItems\": true }, \"redirect_sign_out_uri\": { \"description\": \"URIs used to redirect after signing out\", \"type\": \"array\", \"items\": { \"type\": \"string\" }, \"minItems\": 1, \"uniqueItems\": true }, \"response_type\": { \"type\": \"string\", \"enum\": [ \"code\", \"token\" ] } }, \"required\": [ \"identity_providers\", \"domain\", \"scopes\", \"redirect_sign_in_uri\", \"redirect_sign_out_uri\", \"response_type\" ] }, \"standard_required_attributes\": { \"description\": \"Cognito User Pool standard attributes required for signup\", \"type\": \"array\", \"items\": { \"$ref\": \"#/$defs/amazon_cognito_standard_attributes\" }, \"minItems\": 0, \"uniqueItems\": true }, \"username_attributes\": { \"description\": \"Cognito User Pool username attributes\", \"type\": \"array\", \"items\": { \"type\": \"string\", \"enum\": [ \"email\", \"phone_number\", \"username\" ] }, \"minItems\": 1, \"uniqueItems\": true }, \"user_verification_types\": { \"type\": \"array\", \"items\": { \"type\": \"string\", \"enum\": [ \"email\", \"phone_number\" ] } }, \"unauthenticated_identities_enabled\": { \"type\": \"boolean\", \"default\": true }, \"mfa_configuration\": { \"type\": \"string\", \"enum\": [ \"NONE\", \"OPTIONAL\", \"REQUIRED\" ] }, \"mfa_methods\": { \"type\": \"array\", \"items\": { \"enum\": [ \"SMS\", \"TOTP\" ] } } }, \"required\": [ \"aws_region\", \"user_pool_id\", \"user_pool_client_id\" ] }, \"data\": { \"description\": \"Outputs generated from defineData\", \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"aws_region\": { \"$ref\": \"#/$defs/aws_region\" }, \"url\": { \"description\": \"AppSync endpoint URL\", \"type\": \"string\" }, \"model_introspection\": { \"description\": \"generated model introspection schema for use with generateClient\", \"type\": \"object\" }, \"api_key\": { \"type\": \"string\" }, \"default_authorization_type\": { \"$ref\": \"#/$defs/aws_appsync_authorization_type\" }, \"authorization_types\": { \"type\": \"array\", \"items\": { \"$ref\": \"#/$defs/aws_appsync_authorization_type\" } } }, \"required\": [ \"aws_region\", \"url\", \"default_authorization_type\", \"authorization_types\" ] }, \"geo\": { \"description\": \"Outputs manually specified by developers for use with frontend library\", \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"aws_region\": { \"description\": \"AWS Region of Amazon Location Service resources\", \"$ref\": \"#/$defs/aws_region\" }, \"maps\": { \"description\": \"Maps from Amazon Location Service\", \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"items\": { \"type\": \"object\", \"additionalProperties\": false, \"propertyNames\": { \"description\": \"Amazon Location Service Map name\", \"type\": \"string\" }, \"patternProperties\": { \".*\": { \"$ref\": \"#/$defs/amazon_location_service_config\" } } }, \"default\": { \"type\": \"string\" } }, \"required\": [ \"items\", \"default\" ] }, \"search_indices\": { \"description\": \"Location search (search by places, addresses, coordinates)\", \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"items\": { \"type\": \"array\", \"uniqueItems\": true, \"minItems\": 1, \"items\": { \"description\": \"Actual search name\", \"type\": \"string\" } }, \"default\": { \"type\": \"string\" } }, \"required\": [ \"items\", \"default\" ] }, \"geofence_collections\": { \"description\": \"Geofencing (visualize virtual perimeters)\", \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"items\": { \"type\": \"array\", \"uniqueItems\": true, \"minItems\": 1, \"items\": { \"description\": \"Geofence name\", \"type\": \"string\" } }, \"default\": { \"type\": \"string\" } }, \"required\": [ \"items\", \"default\" ] }"}, {"source": "data/raw_pages/react_reference_amplify_outputs.txt", "text": "}, \"required\": [ \"aws_region\" ] }, \"notifications\": { \"type\": \"object\", \"description\": \"Outputs manually specified by developers for use with frontend library\", \"additionalProperties\": false, \"properties\": { \"aws_region\": { \"$ref\": \"#/$defs/aws_region\" }, \"amazon_pinpoint_app_id\": { \"type\": \"string\" }, \"channels\": { \"type\": \"array\", \"items\": { \"$ref\": \"#/$defs/amazon_pinpoint_channels\" }, \"minItems\": 1, \"uniqueItems\": true } }, \"required\": [ \"aws_region\", \"amazon_pinpoint_app_id\", \"channels\" ] }, \"storage\": { \"type\": \"object\", \"description\": \"Outputs generated from defineStorage\", \"additionalProperties\": false, \"properties\": { \"aws_region\": { \"$ref\": \"#/$defs/aws_region\" }, \"bucket_name\": { \"type\": \"string\" } }, \"required\": [ \"aws_region\", \"bucket_name\" ] }, \"custom\": { \"description\": \"Outputs generated from backend.addOutput({ custom: <config> })\", \"type\": \"object\" } }, \"required\": [ \"version\" ], \"$defs\": { \"aws_region\": { \"type\": \"string\" }, \"amazon_cognito_standard_attributes\": { \"description\": \"Amazon Cognito standard attributes for users -- https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-attributes.html\", \"type\": \"string\", \"enum\": [ \"address\", \"birthdate\", \"email\", \"family_name\", \"gender\", \"given_name\", \"locale\", \"middle_name\", \"name\", \"nickname\", \"phone_number\", \"picture\", \"preferred_username\", \"profile\", \"sub\", \"updated_at\", \"website\", \"zoneinfo\" ] }, \"aws_appsync_authorization_type\": { \"description\": \"List of supported auth types for AWS AppSync\", \"type\": \"string\", \"enum\": [ \"AMAZON_COGNITO_USER_POOLS\", \"API_KEY\", \"AWS_IAM\", \"AWS_LAMBDA\", \"OPENID_CONNECT\" ] }, \"amazon_location_service_config\": { \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"style\": { \"description\": \"Map style\", \"type\": \"string\" } } }, \"amazon_pinpoint_channels\": { \"description\": \"supported channels for Amazon Pinpoint\", \"type\": \"string\", \"enum\": [ \"IN_APP_MESSAGING\", \"FCM\", \"APNS\", \"EMAIL\", \"SMS\" ] } } }"}, {"source": "data/raw_pages/react_build-a-backend_functions_examples_custom-auth-flows.txt", "text": "Custom Auth Challenge Secure Remote Password (SRP) is a cryptographic protocol enabling password-based authentication without transmitting the password over the network. In Amazon Cognito custom authentication flows, CUSTOM_WITH_SRP incorporates SRP steps for enhanced security, while CUSTOM_WITHOUT_SRP bypasses these for a simpler process. The choice between them depends on your application's security needs and performance requirements. This guide demonstrates how to implement both types of custom authentication flows using AWS Amplify with Lambda triggers. You can use defineAuth and defineFunction to create an auth experience that uses CUSTOM_WITH_SRP and CUSTOM_WITHOUT_SRP . This can be accomplished by leveraging Amazon Cognito's feature to define a custom auth challenge and 3 triggers: Create auth challenge Define auth challenge Verify auth challenge response To get started, install the aws-lambda package, which is used to define the handler type. To get started, create the first of the three triggers, create-auth-challenge . This is the trigger responsible for creating the reCAPTCHA challenge after a password is verified. After creating the resource file, create the handler with the following contents: Next, you will want to create the trigger responsible for defining the auth challenge flow, define-auth-challenge . After creating the resource file, create the handler with the following contents if you are using CUSTOM_WITHOUT_SRP : Or if you are using CUSTOM_WITH_SRP : Lastly, create the trigger responsible for verifying the challenge response. For the purpose of this example, the verification check will always return true. After creating the resource file, create the handler with the following contents: Finally, import and set the three triggers on your auth resource: After deploying the changes, whenever a user attempts to sign in with CUSTOM_WITH_SRP or CUSTOM_WITHOUT_SRP , the Lambda challenges will be triggered."}, {"source": "data/raw_pages/react_deploy-and-host_hosting.txt", "text": "Frontend hosting AWS Amplify Hosting is a fully managed CI/CD and hosting service for fast, secure, and reliable static and server-side rendered apps that scale with your business. This service supports modern web frameworks such as React, Angular, Vue, Next.js, Nuxt.js, Gatsby, and more. Because AWS Amplify Hosting is a fully managed service, its documentation lives on the AWS Documentation site. To learn about hosting features such as custom domains, redirects, and more, please visit the Hosting documentation. View Hosting Docs"}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching_cross-account-deployments.txt", "text": "Cross-account deployments This guide walks you through how to create a trunk-based, multi-region deployment pipeline for applications built using AWS Amplify Gen 2. We will be using Amazon CodeCatalyst and AWS Amplify Hosting in this guide, but you can choose any CI/CD provider. Note : You can deploy this custom pipeline either in the us-west-2 or eu-west-1 Regions, as Amazon CodeCatalyst is currently only available in those two AWS Regions . Please refer to this Amazon CodeCatalyst guide for a detailed step-by-step walkthrough to set up your space . Sign in to the AWS Management Console . Navigate to the Amplify console and select Create new app . Select the next-pages-template repository, then select Next . Review the details on the Create Git Repository page, then select Save and deploy . Done! You have successfully deployed a fullstack Gen 2 app. You can review the status of the app deployment in the Amplify console. Add the npx ampx generate outputs --branch $AWS_BRANCH --app-id $AWS_APP_ID command to the build spec and comment out the npx ampx pipeline-deploy --branch $AWS_BRANCH --app-id $AWS_APP_ID command. ampx pipeline-deploy runs a script to deploy backend updates, while ampx generate outputs fetches the latest amplify_outputs.json for the specified environment. You can configure Amplify to disable automatic builds on every code commit. Navigate to the app in the Amplify console. Under App settings , select Branch settings . From the Branches section, select the branch and then choose Disable auto build from the Actions dropdown menu. You can set up an incoming webhook to trigger a build without committing code to your Git repository. Use the Amplify Console to create an incoming webhook . Navigate to the app, under Hosting > Build settings select Create webhook . Provide a name for the webhook and select the target branch to build on incoming webhook requests. Next, select the webhook and copy the curl command which will be used to trigger a build for the app. Please refer to this Amazon CodeCatalyst guide for a detailed step-by-step walkthrough to create a new project . Note : When creating your project, select the next-pages-template GitHub repository, which we used to deploy the app in Step 2. To achieve a cross-account deployment, you will need to implement Steps 1 through 6 outlined previously in this guide in a different AWS account (for example, production account). Navigate to the CodeCatalyst space created as part of Step 1, select Settings , and then select AWS accounts . Add the target AWS account ID (Step 7) to it and select Associate AWS account . You will also need to create an IAM role in the target AWS account which will be assumed by the staging environment to perform actions and deploy resources in the production environment. As a best practice, we recommend attaching the AmplifyBackendDeployFullAccess AWS managed policy to the IAM role as it contains all the required permissions to deploy Gen 2 resources in your account. You can learn more about adding IAM roles"}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching_cross-account-deployments.txt", "text": "to account connections in the CodeCatalyst documentation . A workflow is an automated procedure that describes how to build, test, and deploy your code as part of a continuous integration and continuous delivery (CI/CD) system. You can learn more about workflows in the Amazon CodeCatalyst User Guide . Within the CodeCatalyst project, navigate to the CI/CD feature and select Workflows . Select Create workflow . Choose the next-pages-template GitHub repository and the branch main from the dropdown menu. Next, select Create . Once you create the workflow, you should see a yaml editor in the CodeCatalyst console. Switch the experience in the console to the Visual editor. Select the Actions button to see a list of workflow actions that you can add to your workflow. Add the Build action to the workflow and select the Add variable button in the Inputs section. Add the following environment variables to it: AWS_APP_ID_STAGING: amplify app id for staging app AWS_APP_ID_PRODUCTION: amplify app id for production app AWS_BRANCH: git branch name Add another Build action to the workflow and select the Depends on button in the Inputs section. From the dropdown menu, select the name of the previous build action to set up the pipeline. Next, select the Configuration section and add the following information to each of the build actions: Environment information (optional): staging, production, etc. AWS account connection: your account connection Role: role setup with your account connection You will then need to add the following shell commands to each of the build actions: You can now run Validate to ensure your workflow definition yaml file is valid. Lastly, select Commit to save your changes. Note : Since workflows are saved as commits, and this workflow has a code push trigger enabled, committing the workflow will automatically start a new workflow run. Next, you can review the result of the workflow run from the Runs tab: Done! You have successfully set up a custom cross-account pipeline to deploy your frontend and backend for apps built using Amplify Gen 2. To summarize, this custom pipeline will enable you to deploy your backend initially with your staging environment using ampx pipeline-deploy in the CodeCatalyst workflow and ampx generate outputs will generate the amplify_outputs.json file for the main branch. Amplify Hosting will not deploy backend resources as part of the build and instead will use the deployed backend resources from the main branch. Once the staging environment deploys successfully, a similar process will be followed to deploy your production environment in a different AWS account."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_overriding-resources.txt", "text": "Overriding resources By using overrides, you may create a backend that the Amplify libraries or client config is unable to interpret properly. Always test changes in a staging environment. When defining resources, you can access some underlying AWS Cloud Development Kit (CDK) construct properties to modify resource configurations. This allows you to customize backend resources beyond what is offered through the define* functions. Overrides are defined in the amplify/backend.ts file after the defineBackend call has been made. The backend object exposes a resources property with objects for each of the components passed into the defineBackend function. Each of these resource objects exposes underlying L1 and L2 AWS CDK constructs that you can modify. For example, here is how you can access the Cognito user pool that is created by defineAuth and set a custom removal policy on the resource. Most L1 and L2 AWS CDK constructs that are used by the define* functions are accessible in this way. Consider the case that we want to grant a function created by defineFunction access to call the Cognito user pool created by defineAuth . For most cases it is recommended to use the access property on defineAuth , however for permissions not exposed by this property, access can be accomplished with the following overrides. It's possible to reach all the way down to the raw CloudFormation to mutate properties using addPropertyOverride on an AWS CDK construct. To edit the password policies of the Cognito user pool in defineAuth , you can use the following code. Note the usage of auth.resources.cfnResources . This property exposes L1 CDK constructs that map one-to-one with the underlying CloudFormation properties. The auth.resources.cfnResources.cfnUserPool property in the above example directly maps to the AWS::Cognito::UserPool CloudFormation resource . This is different from auth.resources.userPool in the first example, which is an L2 CDK construct . These are constructs that provide a convenient interface around several related L1 constructs. For situations where you need even more customization of your app backend, see the documentation on custom resources ."}, {"source": "data/raw_pages/react_build-a-backend_functions_scheduling-functions.txt", "text": "Scheduling Functions Amplify offers the ability to schedule Functions to run on specific intervals using natural language or cron expressions . To get started, specify the schedule property in defineFunction : Note: Configuring the schedule in defineFunction is not supported for Custom Functions . Function schedules are powered by Amazon EventBridge rules , and can be leveraged to address use cases such as: generating a \"front page\" of top-performing posts generating a weekly digest of top-performing posts generating a monthly report of warehouse inventory Their handlers can be typed using the EventBridgeHandler type: Note : AWS Lambda types can be installed with Schedules can either be a single interval, or multiple intervals: Schedules can also be defined to execute using minutes or hours with a shorthand syntax: Or combined to create complex schedules: Schedules can be written using natural language, using terms you use every day. Amplify supports the following time periods: day will always start at midnight week will always start on Sunday at midnight month will always start on the first of the month at midnight year will always start on the first of the year at midnight m for minutes h for hours Natural language expressions are prefixed with \"every\": Schedules can be written using cron expressions."}, {"source": "data/raw_pages/react_build-a-backend_auth_manage-users_manage-devices.txt", "text": "Manage devices Amplify Auth enables you to track devices your users use for auditing, MFA, and more. Before you begin it is important to understand the terminology for device statuses: Tracked: Every time the user signs in with a new device, the client is given the device key at the end of a successful authentication event. We use this device key to generate a salt and password verifier which is used to call the ConfirmDevice API. At this point, the device is considered to be tracked . Once the device is in a tracked state, you can use the Amazon Cognito console to see the time it started to be tracked, last authentication time, and other information about that device. Remembered: Remembered devices are also tracked. During user authentication, the device key and secret pair assigned to a remembered device is used to authenticate the device to verify that it is the same device that the user previously used to sign in. Not Remembered: A not-remembered device is a tracked device where Cognito has been configured to require users to \"Opt-in\" to remember a device, but the user has not opt-ed in to having the device remembered. This use case is used for users signing into their application from a device that they don't own. Forgotten: a forgotten device is one removed from being remembered Note: device tracking and remembering features are not available when using federating sign-in with external providers as devices are tracked on the upstream identity provider. These features are also not available when using Cognito's Hosted UI. You can remember devices using the following: You can also forget devices but note that forgotten devices are neither remembered nor tracked. You can fetch a list of remembered devices by using the following: You can now set up devices to be remembered, forgotten, and fetched."}, {"source": "data/raw_pages/react_ai_conversation_response-components.txt", "text": "Response components Response components are custom UI components you can define that the AI assistant can respond with. Response components allow you to build conversational interfaces that are more than just text in and text out. The AIConversation component takes the response components and turns them into tool configurations to send to the LLM. The tool configurations get sent when a user message is sent to the backend, then the backend Lambda merges the tools coming from the client and any schema tools . The LLM sees that it can invoke UI component \"tool\", with certain input/props. If the LLM chooses to use a response component tool, a message gets sent to the client with the response component name and props. The AIConversation component will then try to render the React component provided with the props the LLM sends. It is important to know that the LLM is NOT writing raw code that sent to the browser and evaluated. The responseComponents prop on the AIConversation component takes a named object where the keys are the component names and the value is the component definition. A response component has: description : A description of the component. The more descriptive, the easier it is component : The React component to render. The props of the component should match the props props : The props for the React component in JSONSchema format . Copy highlighted code example description : \"Used to display the weather to the user\" , component : ( { city } ) => { description : \"The name of the city to display the weather for\" , When a user sends a message to the AI assistant from the client, you can optionally send aiContext with the message. aiContext is any information about the current state of the client application that might be useful for the AI assistant to know to help it respond better. aiContext could be things like the user's name, or the current state of the application like what page they are currently on. AI context is a plain object that will get stringified and sent to the AI assistant with the next user message. You can use the aiContext to let the AI assistant know what was rendered in the response component so it can have more context to respond with. It can be helpful to continue the conversation to add some context to the next message in the conversation to let the AI know what was displayed to the user. Because a UI component can have state and also data not included in a prop, Copy highlighted code example const DataContext = React . createContext < { setData : ( value : React . SetStateAction < any > ) => void ; } > ( { data : { } , setData : ( ) => { } } ) ; function WeatherCard ( { city } : { city : string } ) { const { setData } = React . useContext ( DataContext"}, {"source": "data/raw_pages/react_ai_conversation_response-components.txt", "text": ") ; Copy highlighted code example const { data } = React . useContext ( DataContext ) ; ] = useAIConversation ( 'chat' ) ; handleSendMessage = { sendMessage } description : \"Used to display the weather to the user\" , description : \"The name of the city to display the weather for\" , Copy highlighted code example export default function Example ( ) { const [ data , setData ] = React . useState ( { } ) ; < DataContext.Provider value = { { data , setData } } > Because response components are defined at runtime in your React code, but conversation history is stored in a database, there can be times when there is a message in the conversation history that has a response component you no longer have. To handle these situations there is a FallbackResponseComponent prop you can use. Copy highlighted code example FallBackResponseComponent = { ( props ) => { return < > { JSON . stringify ( props ) } </ >"}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_multi-step-sign-in.txt", "text": "Multi-step sign-in After a user has finished signup, they can proceed to sign in. Amplify Auth signin flows can be multi-step processes. The required steps are determined by the configuration provided when you define your auth resources. See the multi-factor authentication page for more information. Depending on the configuration, you may need to call various APIs to finish authenticating a user's signin attempt. To identify the next step in a signin flow, inspect the nextStep parameter of the signin result. If the next step is CONFIRM_SIGN_IN_WITH_SMS_CODE , Amplify Auth has sent the user a random code over SMS and is waiting for the user to verify that code. To handle this step, your app's UI must prompt the user to enter the code. After the user enters the code, pass the value to the confirmSignIn API. The result includes an AuthCodeDeliveryDetails member. It includes additional information about the code delivery, such as the partial phone number of the SMS recipient, which can be used to prompt the user on where to look for the code. If the next step is CONFIRM_SIGN_IN_WITH_TOTP_CODE , you should prompt the user to enter the TOTP code from their associated authenticator app during set up. The code is a six-digit number that changes every 30 seconds. The user must enter the code before the 30-second window expires. After the user enters the code, your implementation must pass the value to Amplify Auth confirmSignIn API. If the next step is CONFIRM_SIGN_IN_WITH_EMAIL_CODE , Amplify Auth has sent the user a random code to their email address and is waiting for the user to verify that code. To handle this step, your app's UI must prompt the user to enter the code. After the user enters the code, pass the value to the confirmSignIn API. The result includes an AuthCodeDeliveryDetails member. It includes additional information about the code delivery, such as the partial email address of the recipient, which can be used to prompt the user on where to look for the code. If the next step is CONTINUE_SIGN_IN_WITH_MFA_SELECTION , the user must select the MFA method to use. Amplify Auth currently supports SMS, TOTP, and EMAIL as MFA methods. After the user selects an MFA method, your implementation must pass the selected MFA method to Amplify Auth using confirmSignIn API. The MFA types which are currently supported by Amplify Auth are: Once Amplify receives the users selection, you can expect to handle a follow up nextStep corresponding with the selected MFA type for setup: If SMS is selected, CONFIRM_SIGN_IN_WITH_SMS_CODE will be the next step. If TOTP is selected, CONFIRM_SIGN_IN_WITH_TOTP_CODE will be the next step. If EMAIL is selected, CONFIRM_SIGN_IN_WITH_EMAIL_CODE will be the next step. If the next step is CONTINUE_SIGN_IN_WITH_EMAIL_SETUP , then the user must provide an email address to complete the sign in process. Once this value has been collected from the user, call the confirmSignIn API to continue. The CONTINUE_SIGN_IN_WITH_TOTP_SETUP step signifies that the user must set up TOTP before they can sign in. The"}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_multi-step-sign-in.txt", "text": "step returns an associated value of type TOTPSetupDetails which must be used to configure an authenticator app like Microsoft Authenticator or Google Authenticator. TOTPSetupDetails provides a helper method called getSetupURI which generates a URI that can be used, for example, in a button to open the user's installed authenticator app. For more advanced use cases, TOTPSetupDetails also contains a sharedSecret which can be used to either generate a QR code or be manually entered into an authenticator app. Once the authenticator app is set up, the user can generate a TOTP code and provide it to the library to complete the sign in process. If the next step is CONTINUE_SIGN_IN_WITH_MFA_SETUP_SELECTION , then the user must indicate which of the available MFA methods they would like to setup. After the user selects an MFA method to setup, your implementation must pass the selected MFA method to the confirmSignIn API. The MFA types which are currently supported by Amplify Auth for setup are: Once Amplify receives the users selection, you can expect to handle a follow up nextStep corresponding with the selected MFA type for setup: If EMAIL is selected, CONTINUE_SIGN_IN_WITH_EMAIL_SETUP will be the next step. If TOTP is selected, CONTINUE_SIGN_IN_WITH_TOTP_SETUP will be the next step. If the next step is CONFIRM_SIGN_IN_WITH_PASSWORD , the user must provide their password as the first factor authentication method. To handle this step, your implementation should prompt the user to enter their password. After the user enters the password, pass the value to the confirmSignIn API. If the next step is CONTINUE_SIGN_IN_WITH_FIRST_FACTOR_SELECTION , the user must select a first factor method for authentication. After the user selects an option, your implementation should pass the selected method to the confirmSignIn API. The first factor types which are currently supported by Amplify Auth are: SMS_OTP EMAIL_OTP WEB_AUTHN PASSWORD PASSWORD_SRP Depending on your configuration and what factors the user has previously setup, not all options may be available. Only the available options will be presented in availableChallenges for selection. Once Amplify receives the user's selection via the confirmSignIn API, you can expect to handle a follow up nextStep corresponding with the first factor type selected: If SMS_OTP is selected, CONFIRM_SIGN_IN_WITH_SMS_CODE will be the next step. If EMAIL_OTP is selected, CONFIRM_SIGN_IN_WITH_EMAIL_CODE will be the next step. If PASSWORD or PASSWORD_SRP is selected, CONFIRM_SIGN_IN_WITH_PASSWORD will be the next step. If WEB_AUTHN is selected, Amplify Auth will initiate the authentication ceremony on the user's device. If successful, the next step will be DONE . If the next step is CONFIRM_SIGN_IN_WITH_CUSTOM_CHALLENGE , Amplify Auth is awaiting completion of a custom authentication challenge. The challenge is based on the AWS Lambda trigger you configured as part of a custom sign in flow. For example, your custom challenge Lambda may pass a prompt to the frontend which requires the user to enter a secret code. To complete this step, you should prompt the user for the custom challenge answer, and pass the answer to the confirmSignIn API. Special Handling on confirmSignIn If failAuthentication=true is returned"}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_multi-step-sign-in.txt", "text": "by the Lambda, Cognito will invalidate the session of the request. This is represented by a NotAuthorizedException and requires restarting the sign-in flow by calling signIn again. If the next step is CONFIRM_SIGN_IN_WITH_NEW_PASSWORD_REQUIRED , Amplify Auth requires the user choose a new password they proceeding with the sign in. Prompt the user for a new password and pass it to the confirmSignIn API. See the sign-in and manage-password docs for more information. If the next step is RESET_PASSWORD , Amplify Auth requires that the user reset their password before proceeding. Use the resetPassword API to guide the user through resetting their password, then call signIn to restart the sign-in flow. See the reset password docs for more information. If the next step is CONFIRM_SIGN_UP , Amplify Auth requires that the user confirm their email or phone number before proceeding. Use the resendSignUpCode API to send a new sign up code to the registered email or phone number, followed by confirmSignUp to complete the sign up. See the sign up docs for more information. The result includes an AuthCodeDeliveryDetails member. It includes additional information about the code delivery, such as the partial phone number of the SMS recipient, which can be used to prompt the user on where to look for the code. Once the sign up is confirmed, call signIn again to restart the sign-in flow. The sign-in flow is complete when the next step is DONE , which means the user is successfully authenticated. As a convenience, the SignInResult also provides the isSignedIn property, which will be true if the next step is DONE ."}, {"source": "data/raw_pages/react_ai_concepts_streaming.txt", "text": "Streaming When an LLM generates a large amount of text, like over 100 words, it can take a while for the entire response to be generated. Rather than waiting for the entire response to come back, we can send back text as it is generated. Foundation model providers like Amazon Bedrock will typically have an HTTP streaming API which can send back the response in chunks. The Amplify AI kit does not use HTTP streaming from the backend to the frontend like other AI frameworks do. Instead, streaming updates are sent to the browser via a websocket connection to AWS AppSync. The Lambda that the Amplify AI kit provisions will call Bedrock with a streaming API request. The Lambda will receive the chunks from the HTTP streaming response and send updates to AppSync, which the client then subscribes to. If you are using the provided React hook, useAIConversation you don't really need to worry about this because it takes care of all of that for you and provides you with conversation messages as React state that is updated as chunks are received."}, {"source": "data/raw_pages/react_build-ui_formbuilder_special-inputs.txt", "text": "Configure special inputs File Uploader fields allow your forms to accept file uploads, which are stored in an Amazon S3 bucket connected to your Amplify app. After uploading, that file's S3 key is stored in your data model, allowing for systematic retrieval using the Amplify JS library . In order to use the File Uploader field, your Amplify app must have an Amplify app with Authentication and Storage enabled. The File Uploader input will allow users to select from files on their local device and upload them to an S3 bucket. File Uploader automatically connects to your S3 bucket added as part of Amplify Storage. Files are uploaded immediately upon selection, and an S3 key is generated. By default, File Uploader will generate a unique S3 key based on the file uploaded. On form submission, File Uploader will return the S3 key of the uploaded file as a String . To use the FileUploader component with an autogenerated form you will first need a data model that has an attribute that is either a string or an array of strings ( a.string().array() in amplify/data/resource.ts ). Then make sure to run npx ampx generate forms after you update your data model. Then go into the generated form JSX file you want to use the FileUploader, for example: ui-components/TodoCreateForm.jsx . If your attribute is an array of strings, look for an <ArrayField> with items={images} (if your attribute name is \"images\"). Remove that entire component and replace it with the FileUploader component like this: If you want your data model to have only one image instead of an array of images, look for the <TextField> component with value={image} and replace it with the FileUploader component like this: See the documentation for the FileUploader for all configuration options. If files with identical S3 keys are uploaded to the same path, S3 will overwrite those files. To prevent accidental overwriting of files, File Uploader generates a unique S3 key by hashing the file contents . Uploading different files with the same name will not overwrite the original file. However, if a form submitter uploads two identical files to the same path - even with different file names - File Uploader will prevent file duplication in your S3 bucket. File overwriting only occurs for identical S3 keys in the same path . If the File level access for your File Uploader is set to private or protected , identical files uploaded by separate users will be saved separately. If your File level access is set to public , identical files will overwrite each other."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_analytics_auto-track-sessions.txt", "text": "Automatically track sessions Analytics auto tracking helps you to automatically track user behaviors like sessions' start/stop, page view change and web events like clicking or mouseover. You can track the session both in a web app or a React Native app by using Analytics. A web session can be defined in different ways. To keep it simple, we define a web session as being active when the page is not hidden and inactive when the page is hidden. A session in a React Native app is active when the app is in the foreground and inactive when the app is in the background. For example: By default, when the page/app transitions to the foreground, the Analytics module will send an event to the Amazon Pinpoint Service. This behavior can be disabled by calling configureAutoTrack : Use this feature to track the most frequently viewed page/url in your webapp. It automatically sends events containing url information when a page is visited. This behavior can be enabled by calling configureAutoTrack : This behavior can be disabled by calling configureAutoTrack : Use page event tracking to track user interactions with specific elements on a page. Attach the specified selectors to your DOM element and turn on the auto tracking. This behavior can be enabled by calling configureAutoTrack : For example: When the button above is clicked, an event will be sent automatically. This is equivalent to doing: This behavior can be disabled by calling configureAutoTrack :"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging_resolve-conflicts.txt", "text": "Resolve conflicts In the rare case where an event is sent and meets the criteria set forth by multiple in-app messages, the library needs to decide which message to return. If such a conflict should arise, In-App Messaging will choose a message by: Sorting the messages in order of campaign expiration Returning the top message sorted (the closest message to expiry) However, this may not be how you wish to resolve such conflicts so you may want to set your own conflict handler."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging_set-up-in-app-messaging.txt", "text": "Set up in-app messaging Amplify allows interacting with In-App Messaging APIs, enabling you to send messages to your app users. In-App Messaging is a powerful tool to engage with your users and provide them with relevant information. A campaign is a messaging initiative that engages a specific audience segment. A campaign sends tailored messages according to a schedule that you define. You can use the AWS Cloud Development Kit (AWS CDK) to create a campaign that sends messages through any single channel that is supported by Amazon Pinpoint: Mobile Push, In-App, Email, SMS or Custom channels. The following is an example utilizing the AWS CDK to create the In-App Messaging resource powered by Amazon Pinpoint . Note: there are no official hand-written (L2) constructs for this service yet. When implementing in-app messaging, please be aware of two important security considerations. First, the endpointID generated by Amazon Pinpoint should be treated as confidential information. There is no built-in authorization mechanism based on endpointID, which means if an endpointID is compromised, other users could potentially access messages intended for different users. We recommend implementing appropriate security measures in your application to protect endpointID access. Second, messages received from Amazon Pinpoint campaigns are delivered without any content sanitization. AWS Amplify acts as a pass-through service and does not perform any content validation or sanitization on these messages. To ensure application security, you should always sanitize message content before rendering it in your application to prevent potential security vulnerabilities such as cross-site scripting (XSS) attacks. Note: Campaign start time must be at least 15 minutes in future. In-app messages can only be synced to local device once the campaign becomes active (Status should be \"In Progress\" in the campaigns screen of the Pinpoint console). First, install the aws-amplify library: To finish setting up your application with Amplify, you need to configure it using the configure API. Next, to interact with In-App Messaging APIs, you need to first initialize In-App Messaging by calling the initializeInAppMessaging API directly imported from the in-app-messaging sub-path. This is required to be called as early as possible in the app lifecycle. Make sure you call Amplify.configure as early as possible in your application\u00e2\u0080\u0099s life-cycle. A missing configuration or NoCredentials error is thrown if Amplify.configure has not been called before other Amplify JavaScript APIs. Amazon Pinpoint Construct Library"}, {"source": "data/raw_pages/react_ai_concepts_models.txt", "text": "Models A foundation model is a large, general-purpose machine learning model that has been pre-trained on a vast amount of data. These models are trained in an unsupervised or self-supervised manner, meaning they learn patterns and representations from the unlabeled training data without being given specific instructions or labels. Foundation models are useful because they are general-purpose and you don't need to train the models yourself, but are powerful enough to take on a range of applications. Foundation Models, which Large Language Models are a part of, are inherently stateless. They take input in the form of text or images and generate text or images. They are also inherently non-deterministic. Providing the same input can generate different output. Before you can invoke a foundation model on Bedrock you will need to request access to the models in the AWS console . Be sure to check the region you are building your Amplify app in! Each foundation model in Amazon Bedrock has its own pricing and throughput limits for on-demand use. On-demand use is serverless, you don't need to provision any AWS resources to use and you only pay for what you use. The Amplify AI kit uses on-demand use for Bedrock. The cost for using foundation models is calculated by token usage. A token in generative AI refers to chunks of data that were sent as input and how much data was generated. A token is roughly equal to a word, but depends on the model being used. Each foundation model in Bedrock has its own pricing based on input and output tokens used. When you use the Amplify AI Kit, inference requests are charged to your AWS account based on Bedrock pricing. There is no Amplify markup, you are just using AWS resources in your own account. Always refer to Bedrock pricing for the most up-to-date information on running generative AI with Amplify AI Kit. Your Amplify project must be deployed to a region where the foundation model you specify is available. See Bedrock model support for the supported regions per model. The Amplify AI Kit uses Bedrock's Converse API to leverage a unified API across models. Amplify AI Kit makes use of \"tools\" for both generation and conversation routes. The models used must support tool use in the Converse API . Most models have different structures to how they best work with input and how they format their output. Using the Converse API makes it easy to swap different models without having to drastically change how you interact with them. Each model and model provider has their own strengths and weaknesses. We encourage you to try different models for different use-cases to find the right fit. Things to consider when choosing a model: Each model has its own context window size. The context window is how much information you can send to the model. FMs are stateless, but conversation routes manage message history, so the context window can continue to grow as you \"chat\" with a model. The"}, {"source": "data/raw_pages/react_ai_concepts_models.txt", "text": "context window for models is defined by the number of tokens it can receive. Smaller models tend to have a lower latency than larger models, but can also sometimes be less powerful. Each model has its own price and throughput. Some models are trained to be better at certain tasks or with certain languages. Choosing the right model for your use case is balancing latency, cost, and performance. Using the Amplify AI Kit you can easily use different models for different functionality in your application. Each AI route definition will have an aiModel attribute you define in your schema. To use different foundation models in your Amplify AI backend, update the aiModel using a.ai.model() : The a.ai.model() function gives you access to friendly names for the Bedrock models. We will keep this function up-to-date as new models are added to Bedrock. In case there is a new model that has not yet been added, you can always use the model ID which can be found in the Bedrock console or documentation:"}, {"source": "data/raw_pages/react_build-a-backend_storage_download-files.txt", "text": "Download files You can easily display images in your app by using the cloud-connected Storage Image React UI component. This component fetches images securely from your storage resource and displays it on the web page. Learn more about how you can further customize the UI component by referring to the Storage Image documentation . To further customize your in-app experience, you can use the getUrl or downloadData API from the Amplify Library for Storage. With the getUrl API, you can get a presigned URL which is valid for 900 seconds or 15 minutes by default. You can use this URL to create a download link for users to click on. The expiresAt property is a Date object that represents the time at which the URL will expire. Inside your template or JSX code, you can use the url property to create a link to the file: This function does not check if the file exists by default. As result, the signed URL may fail if the file to be downloaded does not exist. The behavior of the getUrl API can be customized by passing in options. Use the downloadData API to download the file locally. You can get the value of file in any of the three formats: blob , json , or text . You can call the respective method on the body property to consume the set data in the respective format. You can also perform an upload operation to a specific bucket by providing the bucket option. You can pass in a string representing the target bucket's assigned name in Amplify Backend. import { downloadData } from 'aws-amplify/storage' ; const result = await downloadData ( { path : 'album/2024/1.jpg' , Copy highlighted code example bucket : 'assignedNameInAmplifyBackend' Alternatively, you can also pass in an object by specifying the bucket name and region from the console. import { downloadData } from 'aws-amplify/storage' ; const result = await downloadData ( { path : 'album/2024/1.jpg' , Copy highlighted code example bucketName : 'bucket-name-from-console' , The behavior of the downloadData API can be customized by passing in options. Image compression or CloudFront CDN caching for your S3 buckets is not yet possible. downloadData does not provide a cache control option and it replies on runtime HTTP caching behavior. If you need to bypass the cache, you can use the getUrl API to create a presigned URL for downloading the file. downloadData does not support S3 object versioning, it always downloads the latest version."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_predictions_translate.txt", "text": "Translate language Note: Make sure to complete the getting started section first, where you will set up the IAM roles with the right policy actions Translate text from one source language to a destination language. To view the complete list of supported languages refer to Supported languages and language codes ."}, {"source": "data/raw_pages/react_build-a-backend_data_mutate-data.txt", "text": "Create, update, and delete application data In this guide, you will learn how to create, update, and delete your data using Amplify Libraries' Data client. Before you begin, you will need: You can create an item by first generating the Data client with your backend Data schema. Then you can add an item: Note: You do not need to specify createdAt or updatedAt fields because Amplify automatically populates these fields for you. To update the item, use the update function: Notes: You do not need to specify the updatedAt field. Amplify will automatically populate this field for you. If you specify extra input fields not expected by the API, this query will fail. You can see this in the errors field returned by the query. With Amplify Data, errors are not thrown like exceptions. Instead, any errors are captured and returned as part of the query result in the errors field. You can then delete the Todo by using the delete mutation. To specify which item to delete, you only need to provide the id of that item: Note: When deleting items in many-to-many relationships, the join table records must be deleted before deleting the associated records. For example, for a many-to-many relationship between Posts and Tags, delete the PostTags join record before deleting a Post or Tag. Review Many-to-many relationships for more details. Troubleshoot unauthorized errors Each API request uses an authorization mode. If you get unauthorized errors, you may need to update your authorization mode. To override the default authorization mode defined in your amplify/data/resource.ts file, pass an authMode property to the request or the client. The following examples show how you can mutate data with a custom authorization mode: Close accordion You can cancel any mutation API request by calling .cancel on the mutation request promise that's returned by .create(...) , .update(...) , or .delete(...) . You need to ensure that the promise returned from .create() , .update() , and .delete() has not been modified. Typically, async functions wrap the promise being returned into another promise. For example, the following will not work: Congratulations! You have finished the Create, update, and delete application data guide. In this guide, you created, updated, and deleted your app data. Our recommended next steps include using the API to query data and subscribe to real-time events to look for mutations in your data. Some resources that will help with this work include:"}, {"source": "data/raw_pages/react_build-a-backend_auth_customize-auth-lifecycle_triggers.txt", "text": "Triggers Amplify Auth's behavior can be customized through the use of triggers. A trigger is defined as a Function, and is a mechanism to slot some logic to execute during the authentication flow. For example, you can use triggers to validate whether emails include an allowlisted domain , add a user to a group upon confirmation , or create a \"UserProfile\" model upon account confirmation . Triggers translate to Cognito user pool Lambda triggers . When you have a Lambda trigger assigned to your user pool, Amazon Cognito interrupts its default flow to request information from your function. Amazon Cognito generates a JSON event and passes it to your function. The event contains information about your user's request to create a user account, sign in, reset a password, or update an attribute. Your function then has an opportunity to take action, or to send the event back unmodified. To get started, define a function and specify the triggers property on your auth resource: To learn more about use cases for triggers, visit the Functions examples ."}, {"source": "data/raw_pages/react_build-a-backend_storage_copy-files.txt", "text": "Copy files Note: You can only copy files up to 5GB in a single operation You can copy an existing file to a different path within the storage bucket using the copy API. The copy method duplicates an existing file to a designated path and returns an object {path: 'destPath'} upon successful completion. The operation can fail if there's a special character in the source path. You should URI encode the source path with special character. You don't need to encode the destination path. Cross identity ID copying is only allowed if the destination path has the the right access rules to allow other authenticated users writing to it. You can also perform an copy operation to a specific bucket by providing the bucket option. This option can either be a string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console. In order to copy to or from a bucket other than your default, both source and destination must have bucket explicitly defined."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_rest-api_post-data.txt", "text": "Post data Send a POST request with a JSON body. import { post } from 'aws-amplify/api' ; async function postItem ( ) { const restOperation = post ( { const { body } = await restOperation . response ; const response = await body . json ( ) ; console . log ( 'POST call succeeded' ) ; console . log ( 'POST call failed: ' , JSON . parse ( error . response . body ) ) ;"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_analytics_record-events.txt", "text": "Record events To record custom events call the record API: Analytics events are buffered in memory and periodically sent to the service and not saved locally between application sessions. If the session is ended before a buffered event is sent, it will be lost. Use the flushEvents API to manually send buffered events to the service. The record API lets you add additional attributes to an event. For example, to record artist information with an albumVisit event: Recorded events will be buffered and periodically sent to Amazon Pinpoint. Metrics can also be added to an event: Metric values must be a Number type such as a float or integer. The Amazon Pinpoint event count updates in minutes after recording your event. However, it can take upwards of 30 minutes for the event to display in the Filter section, and for its custom attributes to appear in Amazon Pinpoint. The recorded events are saved in a buffer and sent to the remote server periodically. If needed, you have the option to manually clear all the events from the buffer by using the 'flushEvents' API."}, {"source": "data/raw_pages/gen1_react.txt", "text": "Amplify Documentation for React AWS Amplify streamlines full-stack app development. With its libraries, CLI, and services, you can easily connect your frontend to the cloud for authentication, storage, APIs, and more. Build fullstack apps with your framework of choice AWS Amplify provides libraries for popular web and mobile frameworks, like JavaScript, Flutter, Swift, and React. Our guides, APIs, and other resources will help you build, connect, and host fullstack apps on AWS. Get started by selecting your preferred framework. Features for React Auth Simple configuration Easily configure auth for your app with Amplify CLI and Studio\u00e2\u0080\u0094supports login, MFA, social providers, and more. Then integrate auth with intuitive client library APIs. Pre-built UI components Quickly add polished auth and account UIs with <Authenticator> and <AccountSettings> UI components. Integrate seamlessly with minimal code. User management Manage and maintain full control of your user base in Amplify Studio, without writing code. GraphQL API Powerful data modeling Model relationships between types, customize fields, and configure validation rules using the CLI or Amplify Studio's visual editor. Amplify turns your schema into fully implemented backend and frontend code. Seamless real-time data access Fetch and mutate data through generated GraphQL queries and mutations from your frontend. Sync data in real time and integrate backend data sources easily. Granular authorization Implement fine-grained authorization to securely access data, while controlling auth at the API level or on individual fields. Manage access with AWS IAM policies or your own auth provider. Functions Flexible runtime support Write backend code in your preferred language or framework. Amplify handles deployment on AWS Lambda for serverless execution. Local testing Mock functions locally to build and test without deploying to the cloud. Debug and iterate rapidly by emulating function behavior on your local machine. Event-driven workflows Connect functions to data and auth events to trigger server-side workflows. Manage secrets, keys, and access controls. Storage Upload and Download files Upload and download files to and from cloud storage with advanced controls like pausing and resuming upload operations. Advanced file operations and access control Manage content through APIs for listing, accessing, and manipulating files. Set file permission levels, configure automatic events and triggers, and more. Cloud-connected UI components Integrate pre-built UI components to upload, display, and manage cloud-stored content with minimal coding. Focus on building your app instead of writing boilerplate UI code"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_analytics_storing-data.txt", "text": "Storing analytics data The Amazon Data Firehose analytics provider allows you to send analytics data to an Amazon Data Firehose stream for reliably storing data. The following is an example utilizing the AWS Cloud Development Kit (AWS CDK) to create the Analytics resource powered by Amazon Data Firehose . Let's create a storage bucket to store the data from the Firehose stream. next, let's create the Firehose resource. Ensure you have setup IAM permissions for firehose:PutRecordBatch . Example IAM policy for Amazon Data Firehose: Configure Firehose: You can send a data to a Firehose stream with the standard record method. Any data is acceptable and streamName is required: The recorded events are saved in a buffer and sent to the remote server periodically (You can tune it with the flushInterval option) . If needed, you have the option to manually clear all the events from the buffer by using the 'flushEvents' API."}, {"source": "data/raw_pages/react_reference_telemetry.txt", "text": "Telemetry Amplify Gen 2 collects anonymous telemetry data about general usage of the CLI. Participation is optional, and you may opt out by using ampx configure telemetry disable . Your decision to opt out is stored for your user, meaning all Amplify apps you work with on that computer will not send telemetry data. You may opt out by using the configure telemetry disable command from the root of your Amplify app: You can opt back in to the program by running the following from the root of your Amplify app: In the event you would like to disable telemetry on a one-time basis, you can opt out by defining an environment variable:"}, {"source": "data/raw_pages/react_build-a-backend_data_data-modeling.txt", "text": "Customize your data model Every data model is defined as part of a data schema ( a.schema() ). You can enhance your data model with various fields, customize their identifiers, apply authorization rules, or model relationships. Every data model ( a.model() ) automatically provides create, read, update, and delete API operations as well as real-time subscription events. Below is a quick tour of the many functionalities you can add to your data model: If you are coming from Gen 1, you can continue to use the GraphQL Schema Definition Language (SDL) for defining your schema. However, we strongly recommend you use the TypeScript-first schema builder experience in your project as it provides type safety and is the recommended way of working with Amplify going forward. Note: Some features available in Gen 1 GraphQL SDL are not available in Gen 2. See the feature matrix for features supported in Gen 2."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging_respond-interaction-events.txt", "text": "Respond to interaction events Your code can respond with additional behavior to your users interacting with in-app messages by adding interaction event listeners. Add onMessageReceived listeners to respond to an in-app message being received from the library as the result of an event matching the criteria of a synced in-app message. This is required if you are implementing a custom UI so that your UI can respond to event-triggered campaign messages but you may also find it helpful to listen for these messages for any other reason your application requires. Add onMessageDisplayed listeners to respond to an in-app message being displayed to your user. Add onMessageDismissed listeners to respond to an in-app message being dismissed by your user. Add onMessageActionTaken listeners to respond to an action being taken on an in-app message. Typically, this means that the user has tapped or clicked a button on an in-app message. If you are using the Amplify In-App Messaging UI, interaction events notifications are already wired up for you. However, if you are implementing your own UI, it is highly recommended to notify listeners of interaction events through your UI code so that the library can take further actions prescribed by the installed provider (for example, automatically recording corresponding Analytics events)."}, {"source": "data/raw_pages/react_build-a-backend_data_subscribe-data.txt", "text": "Subscribe to real-time events In this guide, we will outline the benefits of enabling real-time data integrations and how to set up and filter these subscriptions. We will also cover how to unsubscribe from subscriptions. Before you begin, you will need: With Amplify Data Construct @aws-amplify/data-construct@1.8.4 , an improvement was made to how relational field data is handled in subscriptions when different authorization rules apply to related models in a schema. The improvement redacts the values for the relational fields, displaying them as null or empty, to prevent unauthorized access to relational data. This redaction occurs whenever it cannot be determined that the child model will be protected by the same permissions as the parent model. Because subscriptions are tied to mutations and the selection set provided in the result of a mutation is then passed through to the subscription, relational fields in the result of mutations must be redacted. If an authorized end-user needs access to the redacted relational fields, they should perform a query to read the relational data. Additionally, subscriptions will inherit related authorization when relational fields are set as required. To better protect relational data, consider modifying the schema to use optional relational fields. The recommended way to fetch a list of data is to use observeQuery to get a real-time list of your app data at all times. You can integrate observeQuery with React's useState and useEffect hooks in the following way: observeQuery fetches and paginates through all of your available data in the cloud. While data is syncing from the cloud, snapshots will contain all of the items synced so far and an isSynced status of false . When the sync process is complete, a snapshot will be emitted with all the records in the local store and an isSynced status of true . Missing real-time events and model fields If you don't see all of the real-time events and model fields you expect to see, here are a few things to look for. Authorization The model's authorization rules must grant the appropriate rights to the user. If the authorization rules are correct, also ensure the session is authenticated as expected. Selection Set Parity All of the fields you expect to see in a real-time update must be present in the selection set of the mutation that triggers it. A mutation essentially \"provides\" the fields via its selection set that the corresponding subscription can then select from. One way to address this is to use a common selection set variable for both operations. For example: This works well if all subscriptions to Blog require the same subset of fields. If multiple subscriptions are involved with various selection sets, you must ensure that all Blog mutations contain the superset of fields from all subscriptions. Alternatively, you can skip the custom selection sets entirely. The internally generated selection set for any given model is identical across operations by default. The trade-off is that the default selection sets exclude related models. So, when related models are required,"}, {"source": "data/raw_pages/react_build-a-backend_data_subscribe-data.txt", "text": "you would need to either lazy load them or construct a query to fetch them separately. Mutations do not trigger real-time updates for related models. This is true even when the subscription includes a related model in the selection set. For example, if we're subscribed to a particular Blog and wish to see updates when a Post is added or changed, it's tempting to create a subscribe on Blog and assume it \"just works\": But, mutations on Post records won't trigger an real-time event for the related Blog . If you need Blog updates when a Post is added, you must manually \"touch\" the relevant Blog record. Close accordion Subscriptions is a feature that allows the server to send data to its clients when a specific event happens. For example, you can subscribe to an event when a new record is created, updated, or deleted through the API. Subscriptions are automatically available for any a.model() in your Amplify Data schema. Subscriptions take an optional filter argument to define service-side subscription filters: If you want to get all subscription events, don't specify any filter parameters. Limitations: Specifying an empty object {} as a filter is not recommended. Using {} as a filter might cause inconsistent behavior based on your data model's authorization rules. If you're using dynamic group authorization and you authorize based on a single group per record, subscriptions are only supported if the user is part of five or fewer user groups. Additionally, if you authorize by using an array of groups ( groups: [String] ), subscriptions are only supported if the user is part of 20 or fewer groups you can only authorize 20 or fewer user groups per record Now that your application is set up and using subscriptions, you may want to know when the subscription is finally established, or reflect to your users when the subscription isn't healthy. You can monitor the connection state for changes through the Hub local eventing system. Subscription connection states Connected - Connected and working with no issues. ConnectedPendingDisconnect - The connection has no active subscriptions and is disconnecting. ConnectedPendingKeepAlive - The connection is open, but has missed expected keep-alive messages. ConnectedPendingNetwork - The connection is open, but the network connection has been disrupted. When the network recovers, the connection will continue serving traffic. Connecting - Attempting to connect. ConnectionDisrupted - The connection is disrupted and the network is available. ConnectionDisruptedPendingNetwork - The connection is disrupted and the network connection is unavailable. Disconnected - Connection has no active subscriptions and is disconnecting. Troubleshoot connection issues and automated reconnection Connections between your application and backend subscriptions can be interrupted for various reasons, including network outages or the device entering sleep mode. Your subscriptions will automatically reconnect when it becomes possible to do so. While offline, your application will miss messages and will not automatically catch up when reconnected. Depending on your use case, you may want to take action for your app to catch up when it comes back online. Close"}, {"source": "data/raw_pages/react_build-a-backend_data_subscribe-data.txt", "text": "accordion You can also unsubscribe from events by using subscriptions by implementing the following: Congratulations! You have finished the Subscribe to real-time events guide. In this guide, you set up subscriptions for real-time events and learned how to filter and cancel these subscriptions when needed. Our recommended next steps include continuing to build out and customize your information architecture for your data. Some resources that will help with this work include:"}, {"source": "data/raw_pages/react_reference.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts.txt", "text": "Concepts Amplify helps you secure your application while providing an easy sign-in experience for your users. This experience is influenced by your security strategy. This security strategy includes the authentication method, security credentials, and enabling additional verification when needed. Authentication is a process to validate who you are (abbreviated as AuthN ). The system that does this validation is referred to as an Identity Provider or IdP. This can be your own self-hosted IdP or a cloud service. Oftentimes, this IdP is an external provider such as Apple, Facebook, Google, or Amazon. Authorization is the process of validating what you can access (abbreviated as AuthZ ). This is sometimes done by looking at tokens with custom logic, predefined rules, or signed requests with policies. Common authentication methods and associated risks include: External provider federation which enables easier access for your users but shares data with third parties. You can improve security credentials and verification for these authentication methods by: Modifying the default password policy to ensure your users create stronger passwords. Requiring additional contact information from users before they can reset passwords. Enabling multi-factor authentication (MFA) which adds a layer of security at sign-in but may also add friction for your users. Amplify Auth is powered by Amazon Cognito . Amazon Cognito is an identity and access management service, enabling you to secure your web or mobile applications, and is comprised of two services: Amazon Cognito User Pools is a full-featured user directory service to handle user registration, authentication, and account recovery Amazon Cognito Federated Identities or Identity Pools is a service used to authorize your users to interact with other AWS services Amplify interfaces with User Pools to store your user information, including federation with other OpenID providers like Apple, Facebook, Google, or Amazon, and leverages federated identities to manage user access to AWS resources. Authorization is often done in one of two ways: Clients pass the tokens to the backend that perform custom logic to allow or deny actions Clients sign the requests and the backend validates the signature, allowing or denying actions depending on predefined policy. The predefined rules, known as IAM access policies , are automatically configured by Amplify. The first is a common authorization method for HTTP or GraphQL APIs, while the second is necessary for interfacing with AWS services such as Amazon S3, Amazon Pinpoint, and others. Amazon Cognito can be customized based on your security strategy for authentication. However, some initial configuration options cannot be changed after the backend resources are configured: User attributes that are used to identify your individual users (such as email and phone) cannot be renamed or deleted. Sign-in methods (including username, email, and phone) cannot be added or changed after the initial configuration. This includes both defining which attributes are used to sign in and which attributes are required. Required attributes must have a value for all users once set. Verification methods (including username and email) are the same as required attributes and cannot be removed once configured."}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts.txt", "text": "The sub attribute is a unique identifier within each user pool that cannot be modified and can be used to index and search users. If MFA is set to required with phone number for all users, you will need to include MFA setup (i.e. mandating phone number) when users sign up. Visit the Amazon Cognito documentation for more details on these settings, including User pool attributes and Adding MFA to a user pool ."}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_search-and-aggregate-queries.txt", "text": "Connect to Amazon OpenSearch for search and aggregate queries Amazon OpenSearch Service provides a managed platform for deploying search and analytics solutions with OpenSearch or Elasticsearch. The zero-ETL integration between Amazon DynamoDB and OpenSearch Service allows seamless search on DynamoDB data by automatically replicating and transforming it without requiring custom code or infrastructure. This integration simplifies processes and reduces the operational workload of managing data pipelines. DynamoDB users gain access to advanced OpenSearch features like full-text search, fuzzy search, auto-complete, and vector search for machine learning capabilities. Amazon OpenSearch Ingestion synchronizes data between DynamoDB and OpenSearch Service, enabling near-instant updates and comprehensive insights across multiple DynamoDB tables. Developers can adjust index mapping templates to match Amazon DynamoDB fields with OpenSearch Service indexes. Amazon OpenSearch Ingestion, combined with S3 exports and DynamoDB streams, facilitates seamless data input from DynamoDB tables and automatic ingestion into OpenSearch. Additionally, the pipeline can back up data to S3 for potential future re-ingestion as needed. Begin by setting up your project by following the instructions in the Quickstart guide . For the purpose of this guide, we'll sync a Todo table from DynamoDB to OpenSearch. Firstly, add the Todo model to your schema: Important considerations: Ensure Point in Time Recovery (PITR) is enabled, which is crucial for the pipeline integration. Enable DynamoDB streams to capture item changes that will be ingested into OpenSearch. Create an OpenSearch instance with encryption. Important considerations: We recommend configuring the removalPolicy to destroy resources for sandbox environments. By default, OpenSearch instances are not deleted when you run npx ampx sandbox delete , as the default removal policy for stateful resources is set to retain the resource. Establish Storage to back up raw events consumed by the OpenSearch pipeline. Generate a file named amplify/storage/resource.ts and insert the provided content to set up a storage resource. Tailor your storage configurations to regulate access to different paths within your storage bucket. Get the s3BucketArn and s3BucketName values from storage resource as shown below. Additionally, configure an IAM role for the pipeline and assign the roles as indicated below. For further information on the required IAM roles, please refer to the Setting up roles and users documentation. For the S3 bucket, follow standard security practices: block public access, encrypt data at rest, and enable versioning. The IAM role should allow the OpenSearch Ingestion Service (OSIS) pipelines to assume it. Grant specific OpenSearch Service permissions and also provide DynamoDB and S3 access. You may customize permissions to follow the principle of least privilege. Define the pipeline construct and its configuration. When using OpenSearch, you can define the index template or mapping in advance based on your data structure, which allows you to set data types for each field in the document. This approach can be incredibly powerful for precise data ingestion and search. For more information on index mapping/templates, please refer to OpenSearch documentation . Customize the template_content JSON-representation to define the data structure for the ingestion pipeline. The configuration is a data-prepper feature of OpenSearch."}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_search-and-aggregate-queries.txt", "text": "For specific documentation on DynamoDB configuration, refer to OpenSearch data-prepper documentation . This configuration defines the desired behavior of the pipeline for a single model. In the source configuration, DynamoDB is specified as the data source, along with the target table for ingestion and the starting point of the stream. Additionally, besides ingesting the stream into OpenSearch, a target S3 bucket is defined for backup purposes. Furthermore, an IAM role is set for the ingestion pipeline, ensuring it possesses the necessary permissions and policies as detailed in the documentation. Regarding the sink configuration, the OpenSearch domain cluster is specified by setting the host, index name, type, and template content (index mapping) for data formatting. Document-related metadata is configured along with the maximum bulk size for requests to OpenSearch in MB. Once again, an IAM role is specified for the sink portion of the pipeline. For further details on Sink configuration, please refer to the OpenSearch documentation . The sink configuration is an array. To create a different index on the same table, you can achieve this by adding a second OpenSearch configuration to the sink array. To index multiple tables, you'll need to configure multiple pipelines in the configuration. For further guidance, please consult the pipeline section of the OpenSearch documentation. Note : An OpenSearch Ingestion pipeline supports only one DynamoDB table as its source. For more details on current limitations, Please refer to Amazon OpenSearch Limitation section. Now, create the OSIS pipeline resource: After deploying the resources, you can test the data ingestion process by adding an item to the Todo table. However, before doing that, let's verify that the pipeline has been set up correctly. In the AWS console, navigate to OpenSearch and then to the pipelines section. You should find your configured pipeline and review its settings to ensure they match your expectations: You can also check this in the DynamoDB console by going to the Integrations section of the tables. First, Add the OpenSearch data source to the data backend. Add the following code to the end of the amplify/backend.ts file. Let's create the search resolver. Create a new file named amplify/data/searchTodoResolver.js and paste the following code. For additional details please refer to Amazon OpenSearch Service Resolvers Update the schema and add a searchTodo query. Once you've deployed the resources, you can verify the changes by checking the AppSync console. Run the 'searchTodo' query and review the results to confirm their accuracy."}, {"source": "data/raw_pages/react_build-a-backend_storage_use-with-custom-s3.txt", "text": "Use Amplify Storage with any S3 bucket With Amplify Storage APIs, you can use your own S3 buckets instead of the Amplify-created ones. Important: To utilize the storage APIs with an S3 bucket outside of Amplify, you must have Amplify Auth configured in your project. For the specific Amazon S3 bucket that you want to use with these APIs, you need to make sure that the associated IAM role has the necessary permissions to read and write data to that bucket. To do this, go to Amazon S3 console > Select the S3 bucket > Permissions > Edit Bucket Policy. The policy will look something like this: Replace <AWS-account-ID> with your AWS account ID and <role-name> with the IAM role associated with your Amplify Auth setup. Replace <bucket-name> with the S3 bucket name. You can refer to Amazon S3's Policies and Permissions documentation for more ways to customize access to the bucket. In order to make calls to your manually configured S3 bucket from your application, you must also set up a CORS Policy for the bucket. Next, use the addOutput method from the backend definition object to define a custom S3 bucket by specifying the name and region of the bucket in your amplify/backend.ts file. You must also set up the appropriate resources and IAM policies to be attached to the backend. Important: You can use a storage backend configured through Amplify and a custom S3 bucket at the same time using this method. However, the Amplify-configured storage will be used as the default bucket and the custom S3 bucket will only be used as an additional bucket. Configure the S3 bucket Below are several examples of configuring the backend to define a custom S3 bucket: Guest Users Authenticated Users User Groups Owners Below is an example of expanding the original backend object to grant all guest (i.e. not signed in) users read access to files under public/ : Below is an example of expanding the original backend object to grant all authenticated (i.e. signed in) users with full access to files under public/ : Below is an example of expanding the original backend object with user group permissions. Here, any authenticated users can read from admin/ and public/ and authenticated users belonging to the \"admin\" user group can only manage admin/ : Amplify allows scoping file access to individual users via the user's identity ID. To specify the user's identity ID, you can use the token ${cognito-identity.amazonaws.com:sub} . Below is an example of expanding the original backend object to define read access for guests to the public/ folder, as well as defining a protected/ folder where anyone can view uploaded files, but only the file owner can modify/delete them: The custom authorization rules defined in the examples can be combined, and follow the same rules as Amplify-defined storage. Please refer to our documentation on customizing authorization rules for more information. To ensure the local amplify_outputs.json file is up-to-date, you can run the npx ampx generate outputs command or"}, {"source": "data/raw_pages/react_build-a-backend_storage_use-with-custom-s3.txt", "text": "download the latest amplify_outputs.json from the Amplify console as shown below. Now that you've configured the necessary permissions, you can start using the storage APIs with your chosen S3 bucket. While using the Amplify backend is the easiest way to get started, existing storage resources can also be integrated with Amplify Storage. In addition to manually configuring your storage options, you will also need to ensure Amplify Auth is properly configured in your project and associated IAM roles have the necessary permissions to interact with your existing bucket. Read more about using existing auth resources without an Amplify backend . Existing storage resource setup can be accomplished by passing the resource metadata to Amplify.configure . This will configure the Amplify Storage client library to interact with the additional resources. It's recommended to add the Amplify configuration step as early as possible in the application lifecycle, ideally at the root entry point. Alternatively, existing storage resources can be used by creating or modifying the amplify_outputs.json file directly."}, {"source": "data/raw_pages/react_build-a-backend_troubleshooting_circular-dependency.txt", "text": "Troubleshoot circular dependency issues When deploying a Amplify Gen 2 app, you may encounter the error message The CloudFormation deployment failed due to circular dependency in your backend build on Amplify Console or while running a sandbox. This error can occur due to circular dependencies between CloudFormation nested stacks or between resources in a single CloudFormation stack. If you see this error \"The CloudFormation deployment failed due to circular dependency found between nested stacks [data1234ABCD, function6789XYZ]\", it means that the nested stack for data and the nested stack for function have circular dependencies. E.g. if you are using the function as a query handler, but the function also needs access to the data (or AppSync ) API, you might run into this issue. To resolve, group this function with other resources in the data stack Similarly, if you are using your function as an auth trigger, you can group your function with other resources in the auth stack to break the circular dependency. If you are unable to resolve this error using function's resourceGroupName property, please create an issue on the GitHub repository for Amplify backend If you are creating resources using the AWS Cloud Development Kit (AWS CDK) and assigning them to a custom stack, you might also run into this issue. Your error message would look like \"The CloudFormation deployment failed due to circular dependency found between nested stacks [storage1234ABCD, auth5678XYZ, MYCustomStack0123AB ]\" To resolve this, try creating your resources in the same stack as the resources you are trying to interact with. For example, if a custom resource such as sqs needs to interact with the underlying Amazon S3 resource created by defineStorage , you can create that sqs resource in the stack created by Amplify. You can reference the existing Amplify created stack like If you see this error \"The CloudFormation deployment failed due to circular dependency found between resources [resource1, resource2] in a single stack\", that means the resources themselves have a circular dependency in the same stack. For handling such errors, review the AWS Blog post for handling circular dependency errors ."}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching_branch-deployments.txt", "text": "Fullstack branch deployments Amplify code-first DX (Gen 2) offers fullstack branch deployments that allow you to automatically deploy infrastructure and application code changes from feature branches. This enables testing changes in an isolated environment before merging to the main branch. After you've deployed your first branch , you can manually connect more, but the recommended workflow is to use the branch auto-detection feature. Log in to the Amplify console and choose your app. Navigate to App settings > Branch settings , select Edit and enable Branch auto-detection and Branch auto-disconnection . The following video uses the default settings, which will connect any branch in your repo automatically. Branch auto-disconnection will ensure that if you delete a branch from your repository, the branch will also be deleted. You can also define a pattern to connect only certain branches. For example, setting dev , staging , and feature/* will automatically connect all three branch types. Your dev and staging branches, as well as any branch that begins with feature/ , will be connected. Push a commit to your feature/A and staging branches that match the pattern. You should start seeing deployments on the console page. You will now have three fullstack branches deployed. In Gen 2, promoting changes to production follows the normal Git-based workflow. Make a change in your feature/A branch. Submit a pull request to your main branch. Once your team has validated the changes, merge the pull request to main . This will initiate a build on your main branch and update any frontend or backend resources that you changed. You can generate the config for a branch environment by running: For Web and React Native, generating the config with the default format and output directory."}, {"source": "data/raw_pages/react_reference_iam-policy.txt", "text": "IAM policy Branch deployments require the AmplifyBackendDeployFullAccess managed policy to be able to deploy backend resources during a fullstack deployment. When connecting your project through the console, a role with this policy attached will be automatically created for you. Sandbox deployments, by design, use local credentials to deploy resources. You need to ensure that the local profile has the AmplifyBackendDeployFullAccess policy attached to it."}, {"source": "data/raw_pages/react_start_migrate-to-gen2.txt", "text": "Gen 2 for Gen 1 customers We are actively developing migration tooling to aid in transitioning your project from Gen 1 to Gen 2. Until then, we recommend you continue working with your Gen 1 Amplify project. We remain committed to supporting both Gen 1 and Gen 2 for the foreseeable future. For new projects, we recommend adopting Gen 2 to take advantage of its enhanced capabilities . Meanwhile, customers on Gen 1 will continue to receive support for high-priority bugs and essential security updates. The tables below present a feature matrix for Gen 1 customers who are considering Gen 2 for their apps. This will help determine the support availability for various features."}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_connect-amazon-polly.txt", "text": "Connect to Amazon Polly for Text-To-Speech APIs Amazon Polly is a text-to-speech (TTS) service offered by Amazon Web Services (AWS). It uses advanced deep learning technologies to convert written text into lifelike speech, enabling you to create applications with speech capabilities in various languages and voices. With Amazon Polly, you can easily add voice interactions and accessibility features to your applications. The service supports a wide range of use cases, such as providing audio content for the visually impaired, enhancing e-learning experiences, creating interactive voice response (IVR) systems, and more. Key features of Amazon Polly include: Multiple Voices and Languages : Amazon Polly supports dozens of voices across various languages and dialects, giving you the flexibility to choose the most appropriate voice for your use case. High-Quality Speech : Amazon Polly's neural and standard voices offer natural and realistic speech quality. Speech Marks and Speech Synthesis Markup Language : Customize your speech output with Speech Synthesis Markup Language tags and obtain speech timing information with speech marks. Scalable and Cost-Effective : Amazon Polly's pay-as-you-go pricing model makes it a cost-effective solution for adding speech capabilities to your applications. In this section, you'll learn how to integrate Amazon Polly into your application using AWS Amplify, enabling you to leverage its powerful text-to-speech capabilities seamlessly. Set up your project by following the instructions in the Quickstart guide . We'll create a new API endpoint that'll use the the AWS SDK to call the Amazon Polly service. To install the Amazon Polly SDK, run the following command in your project's root folder: Create a file named amplify/storage/resource.ts and add the following content to configure a storage resource: To access Amazon Polly service, you need to configure the proper IAM policy for Lambda to utilize the desired feature effectively. Update the amplify/backend.ts file with the following code to add the necessary permissions to a lambda's Role policy. Define the function handler by creating a new file, amplify/data/convertTextToSpeech.ts . This function converts text into speech using Amazon Polly and stores the synthesized speech as an MP3 file in an S3 bucket. In your amplify/data/resource.ts file, define the function using defineFunction and then reference the function with your mutation using a.handler.function() as a handler. NOTE: At least one query is required for a schema to be valid. Otherwise, deployments will fail a schema error. The Amplify Data schema is auto-generated with a Todo model and corresponding queries under the hood. You can leave the Todo model in the schema until you add the first custom query to the schema in the next steps. Customize your storage settings to manage access to various paths within your storage bucket. It's necessary to update the Storage resource to provide access to the convertTextToSpeech resource. Modify the file amplify/storage/resource.ts as shown below. Import and load the configuration file in your app. It's recommended you add the Amplify configuration step to your app's root entry point. Example frontend code to create an audio buffer for playback using a text input."}, {"source": "data/raw_pages/react_build-ui_figma-to-code.txt", "text": "Figma-to-React You can generate React code using the Amplify UI Figma file and the Amplify UI Builder plugin . This file contains the following pages: README : The README page explains how to use the Figma file to create new components, theme primitives, and customize layout and styling. Theme : The theme page displays the theme values and design tokens Amplify UI uses to style the primitives. If you want to theme the primitives, use the AWS Amplify UI Builder Figma plugin to make changes to the theme. Any changes you make on the theme page itself will not be generated in code. Primitives : Primitives are building-block components such as alerts, buttons, and badges. These primitives correspond to the Amplify UI primitives and get exported to code with all the primitive properties. This page is read-only. Changes to the primitives on this page will not be reflected in code that is generated. My components : This page contains all of the custom components built using the primitives. Amplify provides dozens of components such as news feed, social media, and marketing hero components to get you started. Customize these to match your needs or build your own components. Examples : This is for demonstration purposes only, to show designers how to use our components to build entire pages. Please follow the README in our Figma file to learn how to create your components to optimize for code quality. After you duplicate the Figma file, you run the Amplify UI Builder figma plugin in dev mode or non-dev mode to generate Amplify UI React code. Turn on Figma dev mode in your Figma file. Click on the Plugins tab. Select the AWS Amplify UI Builder plugin. Choose any layer in your file to get React code and a live preview of the generated code. Click on the Plugins tab. Select the AWS Amplify UI Builder plugin. Choose Download component code to download the React code for your components."}, {"source": "data/raw_pages/react_build-a-backend_auth_moving-to-production.txt", "text": "Moving to production Amplify Auth provisions Amazon Cognito resources that are provisioned with limited capabilities for sending email and SMS messages. In its default state, it is not intended to handle production workloads, but is sufficient for developing your application and associated business logic. Cognito provides a default email functionality that limits how many emails can be sent in one day. When considering production workloads, Cognito can be configured to send emails using Amazon Simple Email Service (Amazon SES) . All new AWS accounts default to a \"sandbox\" status with Amazon SES. This comes with the primary caveat that you can only send mail to verified email addresses and domains To get started with Amazon SES in production, you must first request production access . Once you submit your request the submission cannot be modified, however you will receive a response from AWS within 24 hours. After you have configured your account for production access and have verified your sender email, you can configure your Cognito user pool to send emails using the verified sender: Now when emails are sent on new user sign-ups, password resets, etc., the sending account will be your verified email! To customize further, you can change the display name of the sender, or optionally apply a custom address for your users to reply. In order to send SMS authentication codes, you must request an origination number . Authentication codes will be sent from the origination number. If your AWS account is in the SMS sandbox, you must also add a destination phone number, which can be done by going to the Amazon Pinpoint Console , selecting SMS and voice in the navigation pane, and selecting Add phone number in the Destination phone numbers tab. To check if your AWS account is in the SMS sandbox, go to the SNS console , select the Text messaging (SMS) tab from the navigation pane, and check the status under the Account information section."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_rest-api_set-up-rest-api.txt", "text": "Set up Amplify REST API Using the AWS Cloud Development Kit (AWS CDK) , you can configure Amplify Functions as resolvers for routes of a REST API powered by Amazon API Gateway . Create a new directory and a resource file, amplify/functions/api-function/resource.ts . Then, define the function with defineFunction : Create the corresponding handler file, amplify/functions/api-function/handler.ts , file with the following contents: Use the AWS CDK to create an REST API resource powered by Amazon API Gateway . Use the package manager of your choice to install the Amplify JavaScript library. For example, with npm : To initialize the Amplify API category you need to configure Amplify with Amplify.configure() . Import and load the configuration file in your app. It's recommended you add the Amplify configuration step to your app's root entry point. For example index.js in React or main.ts in Angular. Make sure you call Amplify.configure as early as possible in your application\u00e2\u0080\u0099s life-cycle. A missing configuration or NoCredentials error is thrown if Amplify.configure has not been called before other Amplify JavaScript APIs. Review the Library Not Configured Troubleshooting guide for possible causes of this issue."}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_batch-ddb-operations.txt", "text": "Batch DynamoDB Operations Batch DynamoDB operations allow you to add multiple items in single mutation. After your query or mutation is defined, you need to author your custom business logic using a custom resolver powered by AppSync JavaScript resolver . Custom resolvers work on a \"request/response\" basis. You choose a data source, map your request to the data source's input parameters, and then map the data source's response back to the query/mutation's return type. Custom resolvers provide the benefit of no cold starts, less infrastructure to manage, and no additional charge for Lambda function invocations. Review Choosing between custom resolver and function . In your amplify/data/resource.ts file, define a custom handler using a.handler.custom . Amplify will store some values in the resolver context stash that can be accessed in the custom resolver. The Amplify generated DynamoDB table names can be constructed from the variables in the context stash. The table name is in the format <model-name>-<aws-appsync-api-id>-<amplify-api-environment-name> . For example, the table name for the Post model would be Post-123456-dev where 123456 is the AppSync API ID and dev is the Amplify API environment name. From your generated Data client, you can find all your custom queries and mutations under the client.queries. and client.mutations. APIs respectively."}, {"source": "data/raw_pages/react_build-a-backend_data_customize-authz_multi-user-data-access.txt", "text": "Multi-user data access The ownersDefinedIn rule grants a set of users access to a record by automatically creating an owners field to store the allowed record owners. You can override the default owners field name by specifying inField with the desired field name to store the owner information. You can dynamically manage which users can access a record by updating the owner field. If you want to grant a set of users access to a record, you use the ownersDefinedIn rule. This automatically creates a owners: a.string().array() field to store the allowed owners. In your application, you can perform CRUD operations against the model using client.models.<model-name> with the userPool auth mode. import { generateClient } from 'aws-amplify/data' ; import type { Schema } from '../amplify/data/resource' ; const client = generateClient < Schema > ( ) ; const { errors , data : newTodo } = await client . models . Todo . create ( Copy highlighted code example Add another user as an owner await client . models . Todo . update ( owners : [ ... ( newTodo . owners as string [ ] ) , otherUserId ] , Copy highlighted code example You can override the inField to a list of owners. Use this if you want a dynamic set of users to have access to a record. In the example below, the authors list is populated with the creator of the record upon record creation. The creator can then update the authors field with additional users. Any user listed in the authors field can access the record."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_rest-api_reference.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_ai_conversation_context.txt", "text": "Context For LLMs to provide high-quality answers to users' questions, they need to have the right information. Sometimes this information is contextual, based on the user or the state of the application. To allow for this, you can send aiContext with any user message to the LLM, which can be any unstructured or structured data that might be useful. The function passed to the aiContext prop will be run immediately before the request is sent in order to get the most up to date information. You can use React context or other state management systems to update the data passed to aiContext . Using React context we can provide more information about the current state of the application:"}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_usernames.txt", "text": "Usernames Amplify Auth does not support signing in with only username and password, however can be configured to enable usernames for display purposes. Amazon Cognito offers two ways of provisioning login mechanisms: Username attributes Alias attributes Each are described in more detail on the AWS documentation for Cognito user pool settings , however at a high-level can be described as follows: Username attributes allow you to customize which attribute can be used as the \"username\", or allowing users to sign in with an email or phone in place of a username Alias attributes allow you to specify with attribute(s) can be used with sign in in addition to a username With Amazon Cognito, usernames are immutable, which means after the initial sign-up users are unable to change their username later. In some applications this may be undesirable, which can motivate the use of alias attributes. Alias attributes allow you to define a mutable \"preferred username\" in addition to an immutable username. Amplify Auth leverages username attributes to configure Cognito to accept an email or a phone number as the \"username\". Users will then need to verify their ownership of specified email or phone number to confirm their account. However, it is common to consider a \"username\" for display purposes. For example, you can configure your auth resource to accept a \"preferred username\" to be used as the display name: This is not a username the user will be able to sign in with, but it can be used to mask their personal information such as their email or phone number when displaying publicly. If you would like to override the default behavior and allow your users to sign up with an immutable username, you can use CDK to modify your auth resource's usernameAttributes configuration directly:"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_custom-resources.txt", "text": "Custom resources Custom resources allow you to integrate any AWS service into an Amplify backend. You are responsible for ensuring that your custom resources are secure, adhere to best practices, and work with the resources that Amplify creates for your app. With Amplify Gen 2, you can add custom AWS resources to an Amplify app using the AWS Cloud Development Kit (AWS CDK) , which is installed by default as part of the create-amplify workflow. The AWS CDK is an open source software development framework that defines your cloud application resources using familiar programming languages, such as TypeScript. The AWS CDK can be used within an Amplify app to add custom resources and configurations beyond what Amplify supports out of the box. For example, a developer could use CDK to hook up a Redis cache, implement custom security rules, deploy containers on AWS Fargate, or use any other AWS service. The infrastructure defined through the AWS CDK code is deployed along with the Amplify app backend. This provides the simplicity of Amplify combined with the flexibility of CDK for situations where you need more customization. AWS CDK apps are composed of building blocks known as constructs , which are composed together to form stacks and apps . You can learn more in the Concepts section of the AWS Cloud Development Kit (AWS CDK) v2 Developer Guide. With the Amplify code-first DX, you can add existing or custom CDK constructs to the backend of your Amplify app. The AWS CDK comes with many existing constructs that can be directly added to your Amplify backend. For example, to add an Amazon Simple Queue Service (Amazon SQS) queue and an Amazon Simple Notification Service (Amazon SNS) topic to your backend, you can add the following to your amplify/backend.ts file. Note the use of backend.createStack() . This method instructs the backend to create a new CloudFormation Stack for your custom resources to live in. You can create multiple custom stacks and you can place multiple resources in any given stack. Constructs are the basic building blocks of AWS CDK apps. A construct represents a \"cloud component\" and encapsulates everything AWS CloudFormation needs to create the component. Read more . As shown above, you can use the existing AWS CDK constructs directly in an Amplify backend. However, you may find yourself repeating some patterns of common constructs. Custom constructs allow you to encapsulate common patterns into reusable components. This helps you implement best practices, accelerate development, and maintain consistency across applications. A common use case is creating a custom notification construct that combines a Lambda function with Amazon SNS and Amazon Simple Email Service (Amazon SES). This AWS CDK construct implements a decoupled notification system using Amazon SNS and Lambda. It allows publishing notification messages to an SNS topic from one Lambda function, and processing those messages asynchronously using a separate Lambda subscribed to the topic. The key components are: An Amazon SNS topic to receive notification messages A Lambda function to publish messages to"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_custom-resources.txt", "text": "the Amazon SNS topic A second Lambda subscribed to the topic that processes the messages and sends emails through Amazon SES The publisher Lambda allows publishing a message containing the email subject, body text, and recipient address. The emailer Lambda retrieves messages from the SNS topic and handles sending the actual emails. The CustomNotifications custom CDK construct can be defined as follows: The Lambda function code for the Publisher is: The Lambda function code for the Emailer is: The CustomNotifications CDK construct can then be added to the Amplify backend one or more times, with different properties for each instance. The Construct Hub is a community-driven catalog of reusable infrastructure components. It is a place for developers to discover and share reusable patterns for AWS CDK, maintained by AWS. In addition, the example projects using the AWS CDK repository contains a number of examples of reusable CDK constructs. You can use these resources to create custom CDK constructs that can be used in your Amplify app."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_predictions_transcribe-audio.txt", "text": "Transcribe audio to text Note: Make sure to complete the getting started section first, where you will set up the IAM roles with the right policy actions You can transcribe a PCM Audio byte buffer to Text, such as a recording from microphone. To view the complete list of all the supported languages and language specific features refer to the supported languages list . The language data input type has to support streaming for it to work with Amplify Predictions."}, {"source": "data/raw_pages/react_build-a-backend_q-developer.txt", "text": "Use Amazon Q Developer with Amplify Amazon Q Developer is a generative artificial intelligence (AI) powered conversational assistant that can help you understand, build, extend, and operate AWS applications. You can ask questions about AWS architecture, your AWS resources, best practices, documentation, support, and more. Amazon Q is constantly updating its capabilities so your questions get the most contextually relevant and actionable answers. When used in an integrated development environment (IDE), Amazon Q provides software development assistance. Amazon Q can chat about code, provide inline code completions, generate net new code, scan your code for security vulnerabilities, and make code upgrades and improvements, such as language updates, debugging, and optimizations. Q Developer in the IDE provides inline code suggestions in real time. As you write code, Amazon Q automatically generates suggestions based on your existing code and comments. When you start typing out single lines of code or comments, Amazon Q makes suggestions based on your current and previous inputs. Inline suggestions are automatically enabled when you download the Amazon Q extension. Amazon Q is available as an extension in Visual Studio Code and a plugin in JetBrains. Amazon Q is also available in the AWS Toolkit for Visual Studio. To get started, please visit Install Amazon Q Developer . Amplify generates two folders in your backend directory, auth and data , which contain TypeScript AWS CDK definitions for each of these resources. We\u00e2\u0080\u0099ll build out the schema for our API through the help of Amazon Q Developer's inline code suggestion capabilities. Step 1: Open amplify/data/resource.ts and comment out the default schema for Todo provided. Step 2: In a new line below the commented schema, enter a comment to generate the schema using natural language. For example, generate a restaurant model with the following fields: id, name, description, address, image, rating, style. Rating can be a float value. Authorization should allow public. Press Enter for a new line and wait for Amazon Q Developer to generate inline code suggestion for your schema. Step 3: Select the inline code suggestion generated by Amazon Q developer. The inline code suggestion feature assists you in defining the schema and hover over the output to select from other options. Note: You can also trigger inline code suggestion feature by invoking Amazon Q Developer manually using Option+C keyboard shortcut in VS Code. For more commands, please refer to the Commands tab in the Amazon Q extension. Step 4: Make any required changes to the schema and save the amplify/data/resource.ts file. This will trigger a sandbox deployment and your new data model will be deployed Adding @workspace to your question in Amazon Q automatically incorporates the most relevant parts of your workspace code as context, using an index that updates periodically. For more information on @workspace functionality, please visit Amazon Q Developer - Workspace Context . The files below provide detailed guides that can be included as context with the @workspace command, enhancing Amazon Q's accuracy in generating AWS Amplify Gen 2 code. Download the files"}, {"source": "data/raw_pages/react_build-a-backend_q-developer.txt", "text": "and follow the steps below to use the @workspace in your Amplify project Step 1: Create a folder in the root of your project and give a descriptive name such as context . Add the files downloaded above to this folder. Step 2: Open Amazon Q Developer Chat in your IDE and type @workspace to enable workspace indexing. Follow Amazon Q's prompts to set up indexing for your project directory. Step 3: After successful indexing, reference the markdown file content in your queries to Amazon Q. Examples:"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging_integrate-application.txt", "text": "Integrate your application Although Amplify In-App Messaging can be used as a standalone JavaScript library, this guide will show you how to use it together with Amplify UI, which currently supports integration with React and React Native, to get started quickly. Amplify UI provides a Higher-Order Component for ease of integrating the In-App Messaging UI with your application. Simply wrap your application root component in, for example, App.js . Below is an example of what your entry file should look like: You can now build and run your app in your terminal. If you click on one of the buttons shown in the above example, the in-app message you defined in the Pinpoint console should be displayed in your app."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_pubsub_subscribe.txt", "text": "Subscribe and unsubscribe In order to start receiving messages from your provider, you need to subscribe to a topic as follows; Following events will be triggered with subscribe() To subscribe for multiple topics, just pass a String array including the topic names: To stop receiving messages from a topic, you can use unsubscribe() method: Now that your application is setup and using pubsub subscriptions, you may want to know when the subscription is finally established, or reflect to your users when the subscription isn't healthy. You can monitor the connection state for changes via Hub. Connection states Connected - Connected and working with no issues. ConnectedPendingDisconnect - The connection has no active subscriptions and is disconnecting. ConnectedPendingKeepAlive - The connection is open, but has missed expected keep alive messages. ConnectedPendingNetwork - The connection is open, but the network connection has been disrupted. When the network recovers, the connection will continue serving traffic. Connecting - Attempting to connect. ConnectionDisrupted - The connection is disrupted and the network is available. ConnectionDisruptedPendingNetwork - The connection is disrupted and the network connection is unavailable. Disconnected - Connection has no active subscriptions and is disconnecting. Your application can lose connectivity for any number of reasons such as network outages or when the device is put to sleep. Your subscriptions will automatically reconnect when it becomes possible to do so. While offline, your application will miss messages and will not automatically catch up when reconnection happens. Depending on your usecase, you may want take action to catch up when your app comes back online."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging_sync-messages.txt", "text": "Sync messages To trigger messages, you must sync them from your In-App Messaging campaigns to your users' devices. These messages are then triggered with an analytics or In-App Messaging event. You can control when and how often this sync is performed. Note: Syncing messages will always overwrite existing messages currently on the user's device so that they are always up to date when the sync is performed."}, {"source": "data/raw_pages/react_ai_conversation_history.txt", "text": "Conversation History The Amplify AI kit automatically and securely stores conversation history per user so you can easily resume past conversations. If you are looking for a quick way to get stared with conversation history, this example project has a similar interface to ChatGPT or Claude where users see past conversations in a sidebar they can manage. When you define a conversation route in your Amplify data schema, the Amplify AI kit turns that into 2 data models: Conversation and Message . The Conversation model functions mostly the same way as other data models defined in your schema. You can list and filter them (because they use owner-based authorization users will only see their conversations) and you can get a specific conversation by ID. Then once you have a conversation instance you can load the messages in it if there are any, send messages to it, and subscribe to the stream events being sent back. To list all the conversations a user has you can use the .list() method. It works the same way as any other Amplify data model would. You can optionally pass a limit or nextToken . The updatedAt field gets updated when new messages are sent, so you can use that to see which conversation had the most recent message. Conversations retrieved via .list() are sorted in descending order by updatedAt . The result of .list() contains a nextToken property. This can be used to retrieve subsequent pages of conversations. Conversations also have name and metadata fields you can use to more easily find and resume past conversations. name is a string and metadata is a JSON object so you can store any extra information you need. You can resume a conversation by calling the .get() method with a conversation ID. Both .create() and .get() return the a conversation instance."}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_phone.txt", "text": "Phone By default Amplify Auth is scaffolded with email as the default method for user sign-in, however this can be changed or extended to also allow your users to sign in using their phone number. This will configure the phone_number attribute that is required for sign-up and cannot be changed."}, {"source": "data/raw_pages/gen1.txt", "text": "Amplify Documentation AWS Amplify streamlines full-stack app development. With its libraries, CLI, and services, you can easily connect your frontend to the cloud for authentication, storage, APIs, and more. Build fullstack apps with your framework of choice AWS Amplify provides libraries for popular web and mobile frameworks, like JavaScript, Flutter, Swift, and React. Our guides, APIs, and other resources will help you build, connect, and host fullstack apps on AWS. Get started by selecting your preferred framework. Features for React Auth Simple configuration Easily configure auth for your app with Amplify CLI and Studio\u00e2\u0080\u0094supports login, MFA, social providers, and more. Then integrate auth with intuitive client library APIs. Pre-built UI components Quickly add polished auth and account UIs with <Authenticator> and <AccountSettings> UI components. Integrate seamlessly with minimal code. User management Manage and maintain full control of your user base in Amplify Studio, without writing code. GraphQL API Powerful data modeling Model relationships between types, customize fields, and configure validation rules using the CLI or Amplify Studio's visual editor. Amplify turns your schema into fully implemented backend and frontend code. Seamless real-time data access Fetch and mutate data through generated GraphQL queries and mutations from your frontend. Sync data in real time and integrate backend data sources easily. Granular authorization Implement fine-grained authorization to securely access data, while controlling auth at the API level or on individual fields. Manage access with AWS IAM policies or your own auth provider. Functions Flexible runtime support Write backend code in your preferred language or framework. Amplify handles deployment on AWS Lambda for serverless execution. Local testing Mock functions locally to build and test without deploying to the cloud. Debug and iterate rapidly by emulating function behavior on your local machine. Event-driven workflows Connect functions to data and auth events to trigger server-side workflows. Manage secrets, keys, and access controls. Storage Upload and Download files Upload and download files to and from cloud storage with advanced controls like pausing and resuming upload operations. Advanced file operations and access control Manage content through APIs for listing, accessing, and manipulating files. Set file permission levels, configure automatic events and triggers, and more. Cloud-connected UI components Integrate pre-built UI components to upload, display, and manage cloud-stored content with minimal coding. Focus on building your app instead of writing boilerplate UI code"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_external-identity-providers.txt", "text": "External identity providers Before you configure external sign-in with Amplify Auth you will need to set up your developer account with each provider you are using. Note: Amazon Cognito provides first class support for Facebook Login, Google Sign-In, Login with Amazon, and Sign in with Apple for seamless setup. However you can configure other Identity Providers that support SAML or OpenID Connect (OIDC). Warning: When configuring external sign-in it's important to exercise caution when designating attributes as \"required.\" Different external identity providers have varied scopes in terms of the information they respond back to Cognito with. User pool attributes that are initially set up as \"required\" cannot be changed later, and may require you to migrate the users or create a new user pool. Facebook Login Google Sign-In Login with Amazon Sign in with Apple Create a developer account with Facebook . Sign in with your Facebook credentials. Choose My Apps from the top navigation bar, and on the page that loads choose Create App . For your use case, choose Set up Facebook Login . For platform, choose Website and select No, I'm not building a game . Give your Facebook app a name and choose Create app . On the left navigation bar, choose Settings and then Basic . Note the App ID and the App Secret . You will use them in the next section in the CLI flow. Go to Google developer console . Click Select a project . Click NEW PROJECT . Type in project name and click CREATE . Once the project is created, from the left navigation menu, select APIs & Services , then select Credentials . Click CONFIGURE CONSENT SCREEN . Click CREATE . Type in App Information and Developer contact information which are required fields and click SAVE AND CONTINUE three times (OAuth consent screen -> Scopes -> Test Users) to finish setting up the consent screen. Back under the Credentials tab, Create your OAuth2.0 credentials by choosing OAuth client ID from the Create credentials drop-down list. . Choose Web application as Application type and name your OAuth Client. Click Create . Take note of Your client ID and Your Client Secret . You will need them for the next section in the CLI flow. Choose OK . Create a developer account with Amazon . Sign in with your Amazon credentials. You need to create an Amazon security profile to receive the Amazon Client ID and Client Secret. Choose Create a Security Profile . Type in a Security Profile Name , a Security Profile Description , and a Consent Privacy Notice URL . Choose Save . Choose Show Client ID and Client Secret to show the client ID and secret. You will need them for the next section in the CLI flow. Sign In with your Apple developer credentials. On the main developer portal page, select Certificates, IDs, & Profiles . On the left navigation bar, select Identifier . On the Identifiers page, select the plus icon (+) . On the"}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_external-identity-providers.txt", "text": "Register a New Identifier page, select App IDs . On the Register an App ID page, under App ID Prefix , take note of the Team ID value. Provide a description in the Description text box and provide the bundleID of the iOS app. Under Capabilities , select Sign in with Apple . Select Continue , review the configuration, and then select Register . On the Identifiers page, on the right, select App IDs , and then select Services ID . Select the plus icon (+) and, on the Register a New Identifier page, select Services IDs . Provide a description in the Description text box and provide an identifier for the Service ID. Select Continue and register the Service ID. Your developer accounts with the external providers are now set up and you can return to the Amplify specific configuration. In amplify/auth/resource.ts the external providers need to be added. The following is an example of how you would set up access to all of the external providers supported by Amplify Auth. Please note you will need to configure your callbackUrls and logoutUrls URLs for your application, which will inform your backend resources how to behave when initiating sign in and sign out operations in your app. Secrets must be created manually with ampx sandbox secret for use with cloud sandbox, or via the Amplify Console for branch environments. You need to now inform your external provider of the newly configured authentication resource and its OAuth redirect URI: Facebook Login Google Sign-In Login with Amazon Sign in with Apple Sign In to your Facebook developer account with your Facebook credentials. Choose My Apps from the top navigation bar, and on the Apps page, choose your app you created before. On the left navigation bar, choose Products . Add Facebook Login if it isn't already added. If already added, choose Settings under the Configure dropdown. Under Valid OAuth Redirect URIs type your user pool domain with the /oauth2/idpresponse endpoint. https://<your-user-pool-domain>/oauth2/idpresponse Save your changes. Go to the Google developer console . On the left navigation bar, look for APIs and Services under Pinned or under More Products if not pinned. Within the APIs and Services sub menu, choose Credentials . Select the client you created in the first step and click the Edit button. Type your user pool domain into the Authorized JavaScript origins form. Type your user pool domain with the /oauth2/idpresponse endpoint into Authorized Redirect URIs . Note: If you saw an error message Invalid Redirect: domain must be added to the authorized domains list before submitting. when adding the endpoint, please go to the Authorized Domains List and add the domain. Click Save . Sign in with your Amazon credentials. Hover over the gear and choose Web Settings associated with the security profile you created in the previous step, and then choose Edit . Type your user pool domain into Allowed Origins and type your user pool domain with the /oauth2/idpresponse endpoint into Allowed Return URLs . Choose Save"}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_external-identity-providers.txt", "text": ". Sign In with your Apple developer credentials. On the main developer portal page, select Certificates, IDs, & Profiles . On the left navigation bar, select Identifiers and then select Service IDs from the drop down list on the right. Select the Service ID created when you set up your auth provider as outlined in the section above. Enable Sign In with Apple and select Configure . Under Primary App ID select the App ID that was created before. Type your user pool domain into Domains and Subdomains . Type your user pool domain with the /oauth2/idpresponse endpoint into Return URLs . Click Next , review the information, then select Done . On Edit your Services ID Configuration click Continue , review the information, then select Save . On the main Certificates, Identifiers & Profiles , select Keys . On the Keys page, select the plus icon (+) . Provide a name for the key under Key Name . Enable Sign in with Apple and select Configure . Under Primary App ID select the App ID that was created before. Click on Save . On Register a New Key click Continue , review the information, then select Register . You will be redirected to a new page. Take note of the Key ID and download the .p8 file containing the private key. Learn more about using social identity providers with user pool You can determine the pieces of data you want to retrieve from each external provider when setting them up in the amplify/auth/resource.ts file using scopes . Identity provider (IdP) services store user attributes in different formats. When using external IdPs with Amazon Cognito user pools, attribute mapping allows you to standardize these varying formats into a consistent schema. Learn more about mapping IdP attributes to user pool profiles and tokens . Note: When a federated user signs in to your application, a mapping must be present for each attribute that your user pool requires. Additionally, you must also ensure that the target of each attribute mapping is mutable. Amazon Cognito will attempt to update each mapped attribute when a user signs in regardless of whether the latest value already matches the existing information. If these criteria are not met, Amazon Cognito will return an error and the sign in attempt will fail. Learn more about configuring the React Authenticator component for external providers To setup a OIDC provider, you can configure them in your amplify/auth/resource.ts file. For example, if you would like to setup a Microsoft EntraID provider, you can do so as follows: Use the signInWithRedirect API to initiate sign-in with an OIDC identity provider. To setup a SAML provider, you can configure them in your amplify/auth/resource.ts file. For example, if you would like to setup a Microsoft EntraID provider, you can do so as follows: Use the signInWithRedirect API to initiate sign-in with a SAML identity provider. If you are using the Authenticator component with Amplify, this feature works without any additional code. The guide below"}, {"source": "data/raw_pages/react_build-a-backend_auth_concepts_external-identity-providers.txt", "text": "is for writing your own implementation. Use the signInWithRedirect API to initiate sign-in with an external identity provider. Sign in & Sign out redirect URL(s) are used to redirect end users after the sign in or sign out operation has occurred. You may want to specify multiple URLs for various use-cases such as having different URLs for development/ production or redirect users to an intermediate URL before returning them to the app. Specifying a redirect URL on sign out If you have multiple sign out redirect URLs configured, you may choose to override the default behavior of selecting a redirect URL and provide the one of your choosing when calling signOut . The provided redirect URL should match at least one of the configured redirect URLs. If no redirect URL is provided to signOut , one will be selected based on the current app domain. If you are developing a multi-page application, and the redirected page is not the same page that initiated the sign in, you will need to add the following code to the redirected page to ensure the sign in gets completed: Note: The listener only works on the client side in the context of a SSR-enabled project, so ensure to import the listener on the client side only. For example, in a Next.js project, you should add the above import statement to a component that renders on the client side only by 'use client' . Why external Sign In needs to be explicitly handled for Multi-Page Applications When you import and use the signInWithRedirect function, it will add a listener as a side effect that will complete the external sign in when an end user is redirected back to your app. This works well in a single-page application but in a multi-page application, you might get redirected to a page that doesn't include the listener that was originally added as a side-effect. Hence you must include the specific OAuth listener on your login success page. Close accordion"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_interactions_chatbot.txt", "text": "Interact with bots You can send a text message to chatbot backend with send() command. The method returns a promise that includes the chatbot response. You can use onComplete() method to register a function to catch errors or chatbot confirmations when the session successfully ends."}, {"source": "data/raw_pages/react_build-a-backend_troubleshooting_cdktoolkit-stack.txt", "text": "Troubleshoot CDKToolkit stack issues AWS Amplify requires your AWS account and region to be bootstrapped in order to deploy resources into your account. Amplify uses the AWS Cloud Development Kit (AWS CDK) to scaffold backend resource configurations and orchestrate deployments by using the supplemental resources created by the process of bootstrapping. Bootstrapping is the process of preparing your AWS environment for usage with the AWS Cloud Development Kit (AWS CDK). Before you deploy a CDK stack into an AWS environment, the environment must first be bootstrapped. Learn more about bootstrapping by visiting the AWS documentation for AWS CDK's concept of bootstrapping . When deploying an Amplify app you may be redirected to the Amplify Console to complete bootstrapping for your current account ID and region. If this process encounters an error you will be prompted with the following message: This typically indicates one or more of the resources within the CDKToolkit stack has failed to create or update. Navigate to the AWS CloudFormation console , select your CDKToolkit stack, and select the \"Events\" tab to view resource events. Here you may identify issues with the corresponding assets bucket or issues with the IAM roles used for deployments. You can mitigate by manually updating your CDKToolkit stack using the browser-based AWS CloudShell : Or by running bootstrap using the AWS CDK CLI from your terminal: If you continue to experience this issue after applying the workaround noted above, please file an issue in the GitHub repository for Amplify Backend . If you are deploying an Amplify app for the first time and have previously bootstrapped your AWS account to work with CDK, and you encounter the following error in the Amplify Console: You can mitigate by manually updating your CDKToolkit stack using the browser-based AWS CloudShell : Or by running bootstrap using the AWS CDK CLI from your terminal: If you continue to experience this issue after applying the workaround noted above, please file an issue in the GitHub repository for Amplify Backend ."}, {"source": "data/raw_pages/react_build-a-backend_auth_manage-users_with-amplify-console.txt", "text": "Manage users with Amplify console The User management page in the Amplify console provides a user-friendly interface for managing your application's users. You can create and manage users and groups, edit user attributes, and suspend users. If you have not yet created an auth resource, visit the Auth setup guide . After you've deployed your auth resource, you can access the manager on Amplify Console. Log in to the Amplify console and choose your app. Select the branch you would like to access. Select Authentication from the left navigation bar. Then, select User management . On the User management page, select Users tab. Select Create user . In the Create user window, for Unique identifier enter a email address, username, or phone number. For Temporary password enter a password. Choose Create user. On the User management page, choose the Groups tab and then choose Create group . In the Create group window, for Title enter a name for the group. Choose Create group . On the User management page, choose the Groups tab. Select the name of the group to add users to. Choose Add users . In the Add users to group window, choose how you want to search for users to add from the Search menu. You can choose Email , Phone number , or Username . Add one user or multiple users to add to the group and then choose Add users . On the User management page, choose the Groups tab. In the Groups section, select the name of the group to delete. Choose Delete . A confirmation window is displayed. Enter Delete and choose, Confirm deletion ."}, {"source": "data/raw_pages/react_build-a-backend_functions_examples_dynamo-db-stream.txt", "text": "DynamoDB Streams With AWS Lambda, you can seamlessly integrate various event sources, such as Amazon DynamoDB, Amazon SQS, and others, to trigger Lambda functions in response to real-time events. This feature enables you to build responsive, event-driven applications that react to changes in data or system state without the need for polling services. In this guide, lets configure a Lambda function with an Amazon DynamoDB stream as an event source. The Lambda function is automatically triggered whenever an item is added, updated, or deleted from the table, enabling you to build real-time applications that react to changes in your data. In this example, we will use a Todo table created by a data model on the GraphQL API. To get started, install the AWS Lambda Powertools Logger, which provides structured logging capabilities for your Lambda function, and the aws-lambda package, which is used to define the handler type. Second, create a new directory and a resource file, amplify/functions/dynamoDB-function/resource.ts . Then, define the function with defineFunction : Third, create the corresponding handler file, amplify/functions/dynamoDB-function/handler.ts , file with the following contents: Lastly, create DynamoDB table as event source in the amplify/backend.ts file:"}, {"source": "data/raw_pages/react_build-a-backend_storage_list-files.txt", "text": "List file properties You can list files without having to download all the files. You can do this by using the list API from the Amplify Library for Storage. You can also get properties individually for a file using the getProperties API. Note the trailing slash / - if you had requested list({ path : 'album/photos' }) it would also match against files like album/photos123.jpg alongside album/photos/123.jpg . The format of the response will look similar to the below example: If the pageSize is set lower than the total file size, a single list call only returns a subset of all the files. To list all the files with multiple calls, users can use the nextToken flag: Manually created folders will show up as files with a size of 0, but you can also match keys against a regex like file.key.match(/\\.[0-9a-z]+$/i) to distinguish files from folders. Since \"folders\" are a virtual concept in Amazon S3, any file may declare any depth of folder just by having a / in its name. To access the contents and subpaths of a \"folder\", you have two options: Request the entire path and parse the contents. Use the subpathStrategy option to retrieve only the files within the specified path (i.e. exclude files under subpaths). This retrieves all files and folders under a given path. You may need to parse the result to get only the files within the specified path. If you need the files and folders in terms of a nested object instead (for example, to build an explorer UI), you could parse it recursively: This places each item's data inside a special __data key. In addition to using the list API to get all the contents of a path, you can also use it to get only the files within a path while excluding files under subpaths. For example, given the following keys in your path you may want to return only the jpg object, and not the \"vacation\" subpath and its contents: This can be accomplished with the subpathStrategy option: The response will include only the objects within the photos/ path and will also communicate any excluded subpaths: The default delimiter character is '/', but this can be changed by supplying a custom delimiter: You can also perform an copy operation to a specific bucket by providing the bucket option. This option can either be a string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console. You can also view the properties of an individual file. The properties and metadata will look similar to the below example To get the metadata in result for all APIs you have to configure user defined metadata in CORS. Learn more about how to setup an appropriate CORS Policy ."}, {"source": "data/raw_pages/react_build-a-backend_data_data-modeling_relationships.txt", "text": "Modeling relationships When modeling application data, you often need to establish relationships between different data models. In Amplify Data, you can create one-to-many, one-to-one, and many-to-many relationships in your Data schema. On the client-side, Amplify Data allows you to lazy or eager load of related data. With Amplify Data Construct @aws-amplify/data-construct@1.8.4 , an improvement was made to how relational field data is handled in subscriptions when different authorization rules apply to related models in a schema. The improvement redacts the values for the relational fields, displaying them as null or empty, to prevent unauthorized access to relational data. This redaction occurs whenever it cannot be determined that the child model will be protected by the same permissions as the parent model. Because subscriptions are tied to mutations and the selection set provided in the result of a mutation is then passed through to the subscription, relational fields in the result of mutations must be redacted. If an authorized end-user needs access to the redacted relational fields, they should perform a query to read the relational data. Additionally, subscriptions will inherit related authorization when relational fields are set as required. To better protect relational data, consider modifying the schema to use optional relational fields. Create a one-to-many relationship between two models using the hasMany() and belongsTo() method. In the example below, a Team has many Members and a Member belongs to exactly one Team. Create a reference field called teamId on the Member model. This reference field's type MUST match the type of Team 's identifier. In this case, it's an auto-generated id: a.id().required() field. Add a relationship field called team that references the teamId field. This allows you to query for the team information from the Member model. Add a relationship field called members that references the teamId field on the Member model. If your reference field is not required, then you can \"delete\" a one-to-many relationship by setting the relationship value to null . Create a one-to-one relationship between two models using the hasOne() and belongsTo() methods. In the example below, a Customer has a Cart and a Cart belongs to a Customer . Create a reference field called customerId on the Cart model. This reference field's type MUST match the type of Customer 's identifier. In this case, it's an auto-generated id: a.id().required() field. Add a relationship field called customer that references the customerId field. This allows you to query for the customer information from the Cart model. Add a relationship field called activeCart that references the customerId field on the Cart model. To create a \"has one\" relationship between records, first create the parent item and then create the child item and assign the parent. To update a \"Has One\" relationship between records, you first retrieve the child item and then update the reference to the parent to another parent. For example, to reassign a Cart to another Customer: You can set the relationship field to null to delete a \"Has One\" relationship between records. In order"}, {"source": "data/raw_pages/react_build-a-backend_data_data-modeling_relationships.txt", "text": "to create a many-to-many relationship between two models, you have to create a model that serves as a \"join table\". This \"join table\" should contain two one-to-many relationships between the two related entities. For example, to model a Post that has many Tags and a Tag has many Posts , you'll need to create a new PostTag model that represents the relationship between these two entities. const schema = a . schema ( { Copy highlighted code example postId : a . id ( ) . required ( ) , tagId : a . id ( ) . required ( ) , Copy highlighted code example post : a . belongsTo ( 'Post' , 'postId' ) , tag : a . belongsTo ( 'Tag' , 'tagId' ) , Copy highlighted code example tags : a . hasMany ( 'PostTag' , 'postId' ) , Copy highlighted code example posts : a . hasMany ( 'PostTag' , 'tagId' ) , } ) . authorization ( ( allow ) => allow . publicApiKey ( ) ) ; Relationships are defined uniquely by their reference fields. For example, a Post can have separate relationships with a Person model for author and editor . const schema = a . schema ( { title : a . string ( ) . required ( ) , content : a . string ( ) . required ( ) , Copy highlighted code example author : a . belongsTo ( 'Person' , 'authorId' ) , editor : a . belongsTo ( 'Person' , 'editorId' ) , Copy highlighted code example editedPosts : a . hasMany ( 'Post' , 'editorId' ) , authoredPosts : a . hasMany ( 'Post' , 'authorId' ) , } ) . authorization ( ( allow ) => allow . publicApiKey ( ) ) ; On the client-side, you can fetch the related data with the following code: In cases where your data model uses sort keys in the identifier, you need to also add reference fields and store the sort key fields in the related data model: const schema = a . schema ( { title : a . string ( ) . required ( ) , content : a . string ( ) . required ( ) , Copy highlighted code example author : a . belongsTo ( 'Person' , [ 'authorName' , 'authorDoB' ] ) , name : a . string ( ) . required ( ) , dateOfBirth : a . date ( ) . required ( ) , authoredPosts : a . hasMany ( 'Post' , [ 'authorName' , 'authorDoB' ] ) , Copy highlighted code example } ) . identifier ( [ 'name' , 'dateOfBirth' ] ) , } ) . authorization ( ( allow ) => allow . publicApiKey ( ) ) ; Amplify Data's relationships use reference fields to determine if a relationship is required or not. If you mark a reference field as required, then you can't \"delete\" a relationship between two models. You'd have to delete"}, {"source": "data/raw_pages/react_build-a-backend_data_data-modeling_relationships.txt", "text": "the related record as a whole. const schema = a . schema ( { title : a . string ( ) . required ( ) , content : a . string ( ) . required ( ) , Copy highlighted code example authorId : a . id ( ) . required ( ) , author : a . belongsTo ( 'Person' , 'authorId' ) , Copy highlighted code example editor : a . belongsTo ( 'Person' , 'editorId' ) , Copy highlighted code example editedPosts : a . hasMany ( 'Post' , 'editorId' ) , authoredPosts : a . hasMany ( 'Post' , 'authorId' ) , } ) . authorization ( ( allow ) => allow . publicApiKey ( ) ) ;"}, {"source": "data/raw_pages/react_how-amplify-works_faq.txt", "text": "FAQ Is there a way to upgrade an existing Amplify project from Gen 1 to Gen 2? We are still actively developing migration tooling to aid in transitioning your project from Gen 1 to Gen 2. Until then, we recommend you continue working with your Gen 1 Amplify project. We\u00e2\u0080\u0099ve put together a Gen 1 vs. Gen 2 feature support matrix here . We remain committed to supporting both Gen 1 and Gen 2 for the foreseeable future. For new projects, we recommend adopting Gen 2 to take advantage of its enhanced capabilities. Meanwhile, customers on Gen 1 will continue to receive support for high-priority bugs and essential security updates. If I have a Gen 1 app, can I use Gen 2 in it? Amplify Gen 1 and Gen 2 follow different architectural and tooling paradigms, which was necessary to address common customer feedback from Gen 1. You will need to use our upcoming migration tooling to move from a Gen 1 to Gen 2 app. You cannot use Amplify Gen 1 (Studio/CLI) in the same app as Gen 2. Should I use Amplify Gen 1 or Gen 2 in new apps? If you're building a new app, we recommend you use Amplify Gen 2. Does Amplify Gen 2 support DataStore? Amplify Gen 2 supports GraphQL APIs without DataStore. We will release migration support for moving DataStore Gen 1 apps to Gen 2. What programming languages does Amplify Gen 2 support? Amplify Gen 2 supports a wide range of programming languages for client-side development. This includes dedicated client-side libraries for JavaScript, TypeScript, Dart, Java, Kotlin, and Swift. For backend development, Amplify Gen 2 uses TypeScript. In Gen 1, Amplify offered a set of use case categories for building applications (for example, Authentication, Analytics, API, DataStore, Geo, and Predictions). Are those same categories available in Gen 2? Amplify Gen 2 offers built-in support for Auth, Data, Storage, and Functions. Other use cases can be implemented in Amplify Gen 2 as well using AWS Cloud Development Kit (AWS CDK) constructs which there is documentation for under the respective category name. Can I use Gen 2 with a JavaScript frontend that doesn't use TypeScript? Yes. Amplify Gen 2's TypeScript backend definition works with JavaScript frontends. In addition, you still get an end-to-end typed data fetching experience even with a pure JavaScript frontend. See Generate a Data client for the recommended JavaScript client code. What if we want to add a feature like AI/ML or Amazon Location Service to our application in Gen 2? Because Amplify builds on the AWS Cloud Development Kit (AWS CDK), any AWS services supported by the CDK can be added to your app using custom resources and L2/L1 AWS CDK constructs. What happens once my application grows too big and I want to do more configuration with my application (add more features, other AWS services, etc.)? You can stay with Amplify no matter how big your application grows. Amplify is layered on top of the AWS CDK and AWS"}, {"source": "data/raw_pages/react_how-amplify-works_faq.txt", "text": "CloudFormation. These provide a standardized way of interacting with AWS, so you can add any AWS service supported by CDK to your Amplify app . You can also override Amplify-generated configuration of your resources using the CDK. You can use any deployment pipeline you choose if you want more control over your CI. How much does it cost to operate Amplify Gen2? You can read all about Amplify's pricing on our pricing page . Which Amplify JavaScript version is compatible with Gen 2? Amplify JavaScript version 6.2.0 and above is compatible with backends created by Amplify Gen 2."}, {"source": "data/raw_pages/react_build-a-backend_troubleshooting_cannot-find-module-amplify-env.txt", "text": "Troubleshoot \"Cannot find module $amplify/env/<function-name>\" When deploying a Amplify Gen 2 app, you may encounter the error message Cannot find module $amplify/env/<function-name> in your frontend build on Amplify Console. This error occurs when your framework tsconfig.json configuration picks up the amplify directory and tries to resolve it as a module. This module is a placeholder for environment variables that are injected at build time by Amplify. To resolve this error, you need to exclude the amplify directory. To exclude the amplify directory in your tsconfig.json , add the following lines to the exclude section: Amplify will perform type-checking on sandbox and pipeline-deploy using the tsconfig local to the Amplify backend amplify/tsconfig.json . If you'd like to extend your base configuration you can add it to the localized tsconfig. Alternatively, if you work within a monorepo you can move your backend to its own package and export the Schema and outputs for ease of sharing with your other apps. For example, in your backend package's package.json"}, {"source": "data/raw_pages/react_build-a-backend_functions_examples_user-attribute-validation.txt", "text": "User attribute validation You can use defineAuth and defineFunction to create a Cognito pre sign-up Lambda trigger that extends the behavior of sign-up to validate attribute values. To get started, create a new directory and a resource file, amplify/auth/pre-sign-up/resource.ts . Then, define the function with defineFunction : Next, create the corresponding handler file, amplify/auth/pre-sign-up/handler.ts , file with the following contents: Lastly, set the newly created function resource on your auth resource: After deploying the changes, whenever a user attempts to sign up this handler will verify the submitter's age is above 13 years."}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_delete-user-account.txt", "text": "Delete user account Empowering users to delete their account can improve trust and transparency. You can programmatically enable self-service account deletion with Amplify Auth. If you have not yet created an Amplify Gen 2 app, visit the quickstart . You can quickly set up account deletion for your users with the Amplify Libraries. Invoking the deleteUser API to delete a user from the Auth category will also sign out your user. If your application uses a Cognito User Pool, which is the default configuration, this action will only delete the user from the Cognito User Pool. It will have no effect if you are federating with a Cognito Identity Pool alone. Before invoking the deleteUser API, you may need to first delete associated user data that is not stored in Cognito. For example, if you are using Amplify Data to persist user data, you could follow these instructions to delete associated user data. This allows you to address any guidelines (such as GDPR) that require your app to delete data associated with a user who deletes their account. You can enable account deletion using the following method: We recommend you update your UI to let your users know that their account is deleted and test the functionality with a test user. Note that your user will be signed out of your application when they delete their account."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_analytics_enable-disable.txt", "text": "Enable and disable analytics Analytics are enabled by default when you configure it in your app. To disable Analytics in your app use the disable function: To enable analytics you can use the enable function in your app:"}, {"source": "data/raw_pages/react_build-a-backend_auth.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_functions_grant-access-to-other-resources.txt", "text": "Grant access to other resources In order for Amplify Functions to interact with other resources they must be given access. There are two ways to grant Amplify Functions access to other resources: Using the access property Using the AWS Cloud Development Kit (CDK) The access property is a property found in each of the define* functions for defining Amplify resources. It allows you specify the necessary actions using common language. When you grant a function access to another resource in your Amplify backend ( such as granting access to storage ), it will configure environment variables for that function to make SDK calls to the AWS services it has access to. Those environment variables are typed and available as part of the env object. Say you have a function that generates reports each month from your Data resource and needs to store the generated reports in Storage: This access definition will add the environment variable myReports_BUCKET_NAME to the function. This environment variable can be accessed on the env object. Here's an example of how it can be used to upload some content to S3. When permissions are needed to access resources beyond the capabilities of the access property, you must use CDK. Functions are created with an execution role , which is an IAM role that contains policies that dictate what resources your Function can interact with when it executes. This role can be extended using the addToRolePolicy() method: However some constructs provide a grant* method to grant access to common policy actions. Revisiting the example above you can grant the same access with grantPublish :"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_analytics_reference.txt", "text": "Configures automatic event tracking for Pinpoint. This API will automatically transmit an analytic event when configured events are detected within your application. This can include: DOM element events (via the event tracker), session events (via the session tracker), and page view events (via the pageView tracker). service: UpdateEndpointException - Thrown when the underlying Pinpoint service returns an error. validation: AnalyticsValidationErrorCode - Thrown when the provided parameters or library configuration is incorrect."}, {"source": "data/raw_pages/react_ai_concepts_architecture.txt", "text": "Architecture The Amplify AI kit is built around the idea of routes. An AI route is like an API endpoint for interacting with backend AI functionality. AI routes are configured in an Amplify backend where you can define the authorization rules, what type of route (generation or conversation), AI model and inference configuration like temperature, what are the inputs and outputs, and what data it has access to. There are currently 2 types of AI routes: Conversation: A conversation route is an asynchronous, multi-turn API. Conversations and messages are automatically stored in DynamoDB. Examples of this are any chat-based AI experience or conversational UI. Generation: A single synchronous request-response API. A generation route is an AppSync Query that generates structured data according to your route definition. Common uses include generating structured data from unstructured input and summarization. When you create an AI route with the Amplify AI kit, it is using these services: Serverless API layer to authorize and route requests from the browser to AWS services. Serverless database for storing conversation history. Serverless execution for conversations. Serverless foundation models."}, {"source": "data/raw_pages/react_build-a-backend_storage_extend-s3-resources.txt", "text": "Extend S3 resources Amplify Storage generates Amazon S3 resources to offer storage features. You can access the underlying Amazon S3 resources to further customize your backend configuration by using the AWS Cloud Developer Kit (AWS CDK). The following is an example of how you would enable Transfer Acceleration on the bucket ( CDK documentation ). In order to enable Transfer Acceleration on the bucket, you will have to unwrap the L1 CDK construct from the L2 CDK construct like the following. Copy highlighted code example import * as s3 from 'aws-cdk-lib/aws-s3' ; import { defineBackend } from '@aws-amplify/backend' ; import { storage } from './storage/resource' ; const backend = defineBackend ( { Copy highlighted code example const s3Bucket = backend . storage . resources . bucket ; const cfnBucket = s3Bucket . node . defaultChild as s3 . CfnBucket ; cfnBucket . accelerateConfiguration = { accelerationStatus : \"Enabled\" Read more about escape hatches in the CDK . To make calls to your S3 bucket from your App, you need to set up a CORS Policy for your S3 bucket. This callout is only for manual configuration of your S3 bucket. The following steps will set up your CORS Policy: Go to Amazon S3 console and click on your project's userfiles bucket, which is normally named as [Bucket Name][Id]-dev. Click on the Permissions tab for your bucket. Click the edit button in the Cross-origin resource sharing (CORS) section. Make the Changes and click on Save Changes. You can add required metadata to be exposed in ExposeHeaders with x-amz-meta-XXXX format. Note: You can restrict the access to your bucket by updating AllowedOrigin to include individual domains."}, {"source": "data/raw_pages/react_build-a-backend_data_query-data.txt", "text": "Read application data You can read application data using the Amplify Data client. In this guide, we will review the difference between reading data and getting data, how to filter query results to get just the data you need, and how to paginate results to make your data more manageable. We will also show you how to cancel these requests when needed. Before you begin, you will need: Queries are used to read data through the API and include the list and get operations. Amplify Data automatically creates list and get queries for any a.model() type in your schema. The list query retrieves multiple items, such as Todo items, without needing to specific an identifier for a particular record. This is best suited for getting an overview or summary of items, or for enhancing the list operation to filter the items by specific criteria. When you want to query a single entry by an identifier, you would use get to retrieve a specific Todo item. Note: The cost structure of your underlying data source can impact the cost to run some queries. For example, the list operation uses Amazon DynamoDB \"scan operations,\" which can use more read request units than the get operation. You will want to review the associated costs for these operations for your data source. In our example, we are using DynamoDB. You can learn more about how DynamoDB costs are calculated by visiting Amazon DynamoDB pricing . You can list items by first generating the Data client with your backend Data schema. Then you can list items of your desired model: Troubleshoot unauthorized errors Each API request uses an authorization mode. If you get unauthorized errors, you may need to update your authorization mode. To override the default authorization mode defined in your amplify/data/resource.ts file, pass an authMode property to the request or the client. The following examples show how you can mutate data with a custom authorization mode: Close accordion As your data grows, you will need to paginate your list queries. Fortunately, this is already built in to Amplify Data. You can combine filters with and , or , and not Boolean logic. Observe that filter is recursive in respect to those fields. So if, for example, you wanted to filter for priority values of 1 or 2, you would do this: Note that querying for priority of 1 and 2 would return no results, because this is Boolean logic instead of natural language. To paginate your list query results, make a subsequent list query request with the nextToken and limit input variable set. The limit variable limits how many results are returned. The response will include a nextToken you can use to request the next page of data. A nextToken is a very long string that represents the cursor to the starting item of the next query made with these filters. If you're building a React application, you can use the usePagination hook in Amplify UI to help with managing the pagination user"}, {"source": "data/raw_pages/react_build-a-backend_data_query-data.txt", "text": "experience. A business domain model may contain many models with numerous fields. However, apps typically only need subsets of the data or fields to meet the requirements of different components or screens. It is necessary to have a mechanism to retrieve subsets of models and their relationships. This mechanism would help optimize data usage for screens and components by only transferring needed data. Having this capability would improve the app's data efficiency, latency, and the end user's perceived performance. A custom selection set allows consumers to specify, on a per-call basis, the fields the consumer wants to retrieve; this is possible for all operations that return data (CRUDL + observeQuery ). The desired fields are specified in a strongly typed way (discoverable through IntelliSense) with a \"dot notation\". When using TypeScript, you frequently need to specify data model types for type generics. For instance, with React's useState , you provide a type in TypeScript to ensure type-safety in your component code using the state. Use the Schema[\"MODEL_NAME\"][\"type\"] pattern to get TypeScript types for the shapes of data models returned from the backend API. You can combine the Schema[\"MODEL_NAME\"][\"type\"] type with the SelectionSet helper type to describe the return type of API requests using the selectionSet parameter: You can cancel any query API request by calling .cancel on the query request promise that's returned by .list(...) or .get(...) . You need to ensure that the promise returned from .list() or .get() has not been modified. Typically, async functions wrap the promise being returned into another promise. For example, the following will not work: Congratulations! You have finished the Read application data guide. In this guide, you learned how to read your data through get and list queries. Our recommended next steps include subscribing to real-time events to look for mutations in your data and continuing to build out and customize your information architecture for your data. Some resources that will help with this work include:"}, {"source": "data/raw_pages/react_build-a-backend_data_customize-authz.txt", "text": "Customize your auth rules Use the .authorization() modifier to configure authorization rules for public, signed-in user, per user, and per user group data access. Authorization rules operate on the deny-by-default principle . Meaning that if an authorization rule is not specifically configured, it is denied. In the example above, everyone ( public ) can read every Post but authenticated users ( owner ) can create, read, update, and delete their own posts. Amplify also allows you to restrict the allowed operations, combine multiple authorization rules, and apply fine-grained field-level authorization. Use the guide below to select the correct authorization strategy for your use case: Authorization rules can be applied globally across all data models in a schema, onto specific data models, and onto specific fields. Amplify will always use the most specific authorization rule that is available. For example, if there is an authorization rule for a field and an authorization rule for the model that the field belongs to, Amplify will evaluate against the field-level authorization rule. Review Field-level authorization rules to learn more. If there are multiple authorization rules present, they will be logically OR'ed. Review Configure multiple authorization rules to learn more. For userPools and oidc authorization modes, the rules are evaluated in the sequence authenticated > group(s) > owner(s)DefinedIn > group(s)DefinedIn . To help you get started, you can define an authorization rule on the data schema that will be applied to all data models that do not have a model-level authorization rule. Instead of having a global authorization rule for all production environments, we recommend creating specific authorization rules for each model or field. The global authorization rule below uses allow.publicApiKey() . This example allows anyone to create, read, update, and delete and is applied to every data model. Add an authorization rule to a model to apply the authorization rule to all fields of that model. When an authorization rule is added to a field, it will strictly define the authorization rules applied on the field. Field-level authorization rules do not inherit model-level authorization rules. Meaning, only the specified field-level authorization rule is applied. In the example below: Owners are allowed to create, read, update, and delete Employee records they own Any signed in user has read access and can read data with the exception of the ssn field Only the ssn field has owner auth applied and this field-level auth rule means that model-level auth rules are not applied Non-model types are any types added to the schema without using a.model() . These consist of modifiers such as a.customType() , a.enum() , a.query() , a.mutation() , or a.subscription() . Dynamic authorization rules such as allow.owner() , allow.ownerDefinedIn() , allow.groupDefinedIn() are not supported for non-model types. There are TS warnings and validation checks in place that will cause a sandbox deployment to fail if unsupported auth rules are defined on custom queries and mutations. When combining multiple authorization rules, they are \"logically OR\"-ed. In the following example: Any user (using Amazon Cognito identity pool's"}, {"source": "data/raw_pages/react_build-a-backend_data_customize-authz.txt", "text": "unauthenticated roles) is allowed to read all posts Owners are allowed to create, read, update, and delete their own posts On the client side, make sure to always authenticate with the corresponding authorization mode. All Amplify Gen 2 projects enable IAM authorization for data access. This ensures that the Amplify console's data manager will be able to access your API. It also allows you to authorize other administrative or machine-to-machine access using your own IAM policies. See the AWS AppSync Developer Guide for details on how AWS AppSync works with IAM. Authorization rules are only supported on data models (model-level and field-level) and custom operations (queries, mutations and subscriptions). They are not fully supported on custom types, including custom types returned by custom operations. For example, consider a custom query that returns a custom type: As you can see, the custom Counter type does not support the .authorization() modifier. Instead, behind the scenes, Amplify will add appropriate authorization rules to Counter to allow authenticated users to access it. That means that any signed-in user will be able to access the custom operation and all fields of the custom type. Note : IAM authorization is not currently supported for custom operations that return custom types if defaultAuthorizationMode is not iam . See GitHub issue #2929 for details and suggested workarounds."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_rest-api_fetch-data.txt", "text": "Fetch data To invoke an endpoint, you need to set input object with required apiName option and optional headers , queryParams , and body options. API status code response > 299 are thrown as an ApiError instance. The error instance provides name and message properties parsed from the response. The retryStrategy can be configured with: no-retry : Single attempt, fails immediately on error jittered-exponential-backoff : Default strategy that retries with increasing delays, maximum 3 attempts You can consume the response payload by accessing the body property of the response object. Depending on the use case and the content type of the body, you can consume they payload in string, blob, or JSON. You can not consume the response payload more than once. The REST API handler may throw an ApiError error instance. If the error is caused by an HTTP response with a non-2xx status code, the error instance will provide a response property. The response property contains following properties: statusCode : HTTP status code headers : HTTP response headers body : HTTP response body as a string The following example shows how to access the HTTP response from an ApiError instance, so that you can handle the error response from your REST API endpoint:"}, {"source": "data/raw_pages/react_ai_conversation_knowledge-base.txt", "text": "Knowledge Base Amazon Bedrock knowledge bases are a great way to implement Retrieval Augmented Generation, or RAG for short. RAG is a common pattern in building generative AI applications that involves storing a lot of content, like documentation, in a vector database like Postgres with pg_vector or OpenSearch. The default setup for an Amazon Bedrock knowledge base is OpenSearch Serverless which has a default cost whether or not you use it. You can get a large AWS bill if you are not careful. If you are just testing this out make sure to turn off the OpenSearch Serverless instance when you are done. To integrate Bedrock knowledge base with your conversation route, first create an Amazon Bedrock knowledge base in the console, CLI, or with CDK. Then you'll need to create a JavaScript AWS AppSync resolver to connect the query to the knowledge base. You'll need to know the ID of the knowledge base you want to use, which you can find in the Amazon Bedrock console or with the AWS CLI. Then in the amplify backend file you will need to create the data source for the knowledge base query and give it permission to call the knowledge base. That's it!"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_in-app-messaging_reference.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_connect-amazon-rekognition.txt", "text": "Connect to Amazon Rekognition for Image Analysis APIs Amazon Rekognition is an advanced machine learning service provided by Amazon Web Services (AWS), allowing developers to incorporate image and video analysis into their applications. It uses state-of-the-art machine learning models to analyze images and videos, providing valuable insights such as object and scene detection, text recognition, face analysis, and more. Key features of Amazon Rekognition include: Object and Scene Detection : Amazon Rekognition can identify thousands of objects and scenes in images and videos, providing valuable context for your media content. Text Detection and Recognition : The service can detect and recognize text within images and videos, making it an invaluable tool for applications requiring text extraction. Facial Analysis : Amazon Rekognition offers accurate facial analysis, enabling you to detect, analyze, and compare faces in images and videos. Facial Recognition : You can build applications with the capability to recognize and verify individuals using facial recognition. Content Moderation : Amazon Rekognition can analyze images and videos to identify inappropriate or objectionable content, helping you maintain safe and compliant content. In this section, you will learn how to integrate Amazon Rekognition into your application using AWS Amplify, leveraging its powerful image analysis capabilities seamlessly. Set up your project by following the instructions in the Quickstart guide . Create a new API endpoint that'll use the the AWS SDK to call the Amazon Rekognition service. To install the Amazon Rekognition SDK, run the following command in your project's root folder: Create a file named amplify/storage/resource.ts and add the following content to configure a storage resource: To use the Amazon Rekognition service, you need to add Amazon Rekognition as an HTTP Data Source and configure the proper IAM policy for Lambda to effectively utilize the desired feature and grant permission to access the storage. In this case, you can add the rekognition:DetectText and rekognition:DetectLabels actions to the policy. Update the amplify/backend.ts file as shown below. Define the function handler by creating a new file, amplify/data/identifyText.ts . This function analyzes the image and extracts text using the Amazon Rekognition DetectText service. After adding Amazon Rekognition as a data source, you can reference it in custom query using the a.handler.custom() modifier, which takes the name of the data source and an entry point for your resolvers. In your amplify/data/resource.ts file, specify RekognitionDataSource as the data source and identifyText.js as the entry point, as shown below. Customize your storage settings to manage access to various paths within your storage bucket. Modify the file amplify/storage/resource.ts as shown below. Import and load the configuration file in your app. It's recommended you add the Amplify configuration step to your app's root entry point. This code sets up a React app to upload an image to an S3 bucket and then use Amazon Rekognition to recognize the text in the uploaded image."}, {"source": "data/raw_pages/react_build-a-backend_data_customize-authz_configure-custom-identity-and-group-claim.txt", "text": "Configure custom identity and group claims Amplify Data supports using custom identity and group claims if you do not wish to use the default Amazon Cognito-provided cognito:groups or the double-colon-delimited claims, sub::username , from your JWT token. This can be helpful if you are using tokens from a 3rd party OIDC system or if you wish to populate a claim with a list of groups from an external system, such as when using a Pre Token Generation Lambda Trigger which reads from a database. To use custom claims specify identityClaim or groupClaim as appropriate. In the example below, the identityClaim is specified and the record owner will check against this user_id claim. Similarly, if the user_groups claim contains a \"Moderator\" string then access will be granted. In your application, you can perform CRUD operations against the model using client.models.<model-name> with the userPool auth mode. import { generateClient } from 'aws-amplify/data' ; import type { Schema } from '../amplify/data/resource' ; const client = generateClient < Schema > ( ) ; const { errors , data : newTodo } = await client . models . Todo . create ( content : 'My post content' , Copy highlighted code example"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-to-existing-data-sources.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_predictions_text-to-speech.txt", "text": "Text to speech Note: Make sure to complete the getting started section first, where you will set up the IAM roles with the right policy actions Generate an audio buffer for playback from a text input. To view the complete list of voiceId options refer to Voices in Amazon Polly ."}, {"source": "data/raw_pages/react_deploy-and-host_sandbox-environments_features.txt", "text": "Sandbox features Sandbox environments include additional features for managing secrets, deploying multiple sandboxes, config generation, and client codegen for your Amplify app. Secrets set in a sandbox do not show up in the Amplify Console. You can view them in the AWS Systems Manager (SSM) Parameter Store console. Amplify Gen 2 offers secure secret storage to manage sensitive data like API keys and database credentials. Secrets are similar to environment variables, but they are encrypted AWS Systems Manager Parameter Store key value pairs. Secrets are stored in AWS Parameter Store under the /amplify prefix. You can add secrets to your sandbox environment using the following command: After these commands, your sandbox will have two secrets named foo and bar . You can list all of the secret names available in your sandbox environment with the following command: Note: This will print a secret value in plain text to the terminal. Do not use this command anywhere that terminal logs may be stored (such as CI/CD jobs). To show the value of a secret, run the following command. To remove a secret from from the sandbox, run the following command in your terminal: To remove all secrets from the sandbox, run the following command in your terminal: Once you have set a secret, you can reference the secret in your backend definition using the secret() function. The following example shows how to set up social sign-in with authentication in your app. Depending on your environment, Amplify will automatically load the correct secret value. import { defineAuth , secret } from '@aws-amplify/backend' ; export const auth = defineAuth ( { Copy highlighted code example clientSecret : secret ( 'bar' ) The secret() function does NOT retrieve the value of the secret. It places a reference to the secret value in the backend definition. The secret value is only resolved during deployment of your backend. The secret() function can only be used in specific places in your backend definition such as configuring auth providers and function secrets . Sometimes you might have multiple AWS profiles set up locally. To run ampx sandbox secret commands, use the --profile flag to deploy to a specific profile. For example, let's say you have two AWS profiles set up locally\u00e2\u0080\u0094 default and work . To add secrets to the sandbox in the work profile, run the following command in your terminal: Provisioning multiple sandboxes per app is possible but not recommended because managing multiple ephemeral environments for a single developer introduces complexity. With multiple sandboxes, it can be difficult to keep track of what code version or configuration is deployed where. Sticking to a single sandbox per developer keeps your workflows simpler. You can create multiple sandboxes if you want to have different features or test environments available in different sandboxes. By default, your sandbox is named based on the local machine username. To override this name, use the --identifier option: This will start a sandbox named feature1sandbox . Once the deployment completes, exit sandbox and run"}, {"source": "data/raw_pages/react_deploy-and-host_sandbox-environments_features.txt", "text": "the following command in the terminal: After successful deployment, you will have two sandboxes feature1sandbox and feature2sandbox . You can switch between them but only one can be running at a time. When working with multiple sandboxes, secrets must be configured for each one. All of the sandbox secret commands accept the --identifier argument to manage secrets for named sandboxes. For example, to add a secret to feature1sandbox , use: Amplify offers the ability to stream function logs directly to your terminal or a file. Learn more about streaming function logs . The client config, or amplify_outputs.json file, contains the configuration strings for interacting with AWS resources specific to an environment. The Amplify client libraries need the client config in order to use the library APIs to connect to backend resources. By default, the cloud sandbox generates the client configuration file at the root of the project (such as @/amplify_outputs.json ). If you want to place the file at a different path (such as for a monorepo or Android app), run the following command in the terminal: Alternatively, if you want to generate the config for a branch environment to test against, run the following command in the terminal. Alternatively, if you want to generate the config for a branch environment to test against, you can run the following command below in the terminal: For Web and React Native, generating the config with the default format and output directory. Amplify Gen 2 introduces a fully typed experience for data that no longer requires an explicit codegen step, unlike in Amplify Gen 1. You will only need this command if you are building a mobile app or have Gen 1 requirements. Codegen generates native code for Swift (iOS), Java (Android), and JavaScript that represents your GraphQL API's data models. It can also generate GraphQL statements (queries, mutations, and subscriptions) so that you don't have to manually code them. Once your sandbox completes a deployment, you can run the following command in the terminal to generate client code that is specific to your needs: You can delete a cloud sandbox environment in several ways: Ctrl+C your sandbox and choose to delete resources. Run npx ampx sandbox delete or npx ampx sandbox delete --name Visit the Amplify console and delete sandboxes ."}, {"source": "data/raw_pages/react_build-a-backend_data_connect-to-API.txt", "text": "Connect your app code to API In this guide, you will connect your application code to the backend API using the Amplify Libraries. Before you begin, you will need: Your cloud sandbox with an Amplify Data resource up and running ( npx ampx sandbox ) A frontend application set up with the Amplify library installed npm installed When you deploy you're iterating on your backend ( npx ampx sandbox ), an amplify_outputs.json file is generated for you. This file contains your API's endpoint information and auth configurations. Add the following code to your app's entrypoint to initialize and configure the Amplify client library: Once the Amplify library is configured, you can generate a \"Data client\" for your frontend code to make fully-typed API requests to your backend. If you're using Amplify with a JavaScript-only frontend (i.e. not TypeScript), then you can still get a fully-typed data fetching experience by annotating the generated client with a JSDoc comment . Select the JavaScript in the code block below to see how. To generate a new Data client, use the following code: The Authorization Mode determines how a request should be authorized with the backend. By default, Amplify Data uses the \"userPool\" authorization which uses the signed-in user credentials to sign an API request. If you use a allow.publicApiKey() authorization rules for your data models, you need to use \"apiKey\" as an authorization mode. Review Customize your auth rules to learn more about which authorization modes to choose for which type of request. A Default Authorization Mode is provided as part of the amplify_outputs.json that is generated upon a successful deployment. You can generate different Data clients with different authorization modes or pass in the authorization mode at the request time. To apply the same authorization mode on all requests from a Data client, specify the authMode parameter on the generateClient function. API Key Amazon Cognito user pool AWS IAM (including Amazon Cognito identity pool roles) OpenID Connect (OIDC) Lambda Authorizer Use \"API Key\" as your authorization mode when if defined the allow.publicApiKey() authorization rule. Use \"userPool\" as your authorization mode when using Amazon Cognito user pool-based authorization rules, such as allow.authenticated() , allow.owner() , allow.ownerDefinedIn() , allow.groupsDefinedIn() , or allow.groups() . Use \"identityPool\" as your authorization mode when using Amazon Cognito identity pool-based authorization rules, such as allow.guest() or allow.authenticated('identityPool') . Use \"oidc\" as your authorization mode when connecting applications to a trusted identity provider. Private, owner, and group authorization can be configured with an OIDC authorization mode. Review the OIDC authorization docs to learn more. Use \"Lambda Authorizer\" when using your own custom authorization logic via allow.custom() . Review Customize your auth rules to learn more about how to implement your authorization protocol. You can also specify the authorization mode on each individual API request. This is useful if your application typically only uses one authorization mode with a small number of exceptions. API Key Amazon Cognito user pool AWS IAM (including Amazon Cognito identity pool roles) OpenID Connect (OIDC) Lambda"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-to-API.txt", "text": "Authorizer You can implement your own custom API authorization logic using a AWS Lambda function. Review Customize your auth rules to learn more about how to implement your authorization protocol with AWS Lambda. When working with the Amplify Data endpoint, you may need to set request headers for authorization purposes or to pass additional metadata from your frontend to the backend API. This is done by specifying a headers parameter into the configuration. You can define headers either on a per Data client-level or on a per-request level: The examples above show you how to set static headers but you can also programmatically set headers by specifying an async function for headers : If you have an additional Data endpoint that you're managing with a different Amplify project or through other means, this section will show you how to utilize that endpoint in your frontend code. This is done by specifying the endpoint parameter on the generateClient function. If this Data endpoint shares its authorization configuration (for example, both endpoints share the same user pool and/or identity pool as the one in your amplify_outputs.json file), you can specify the authMode parameter on generateClient . If the endpoint uses API Key authorization, you can pass in the apiKey parameter on generateClient . If the endpoint uses a different authorization configuration, you can manually pass in the authorization header using the instructions in the Set custom request headers section."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_predictions_interpret-sentiment.txt", "text": "Interpret sentiment Note: Make sure to complete the getting started section first, where you will set up the IAM roles with the right policy actions Analyze text to find key phrases, sentiment (positive, negative, neutral), or the syntax (pronouns, verbs, etc.). You can also find entities in the text such as names or places, or perform language detection."}, {"source": "data/raw_pages/react_ai_concepts_inference-configuration.txt", "text": "Inference Configuration LLMs have parameters that can be configured to change how the model behaves. This is called inference configuration or inference parameters. LLMs are actually predicting text based on the text input. This prediction is probabilistic, and can be tweaked by adjusting the inference configuration to allow for more creative or deterministic outputs. The proper configuration will depend on your use case. Bedrock documentation on inference configuration Inference refers to the process of using a model to generate or predict output based on input data. Inference is using a model after it has been trained on a data set. Close accordion All generative AI routes in Amplify accept inference configuration as optional parameters. If you do not provide any inference configuration options, Bedrock will use default ones for that particular model . Affects the shape of the probability distribution for the predicted output and influences the likelihood of the model selecting lower-probability outputs. Temperature is usually* number from 0 to 1, where a lower value will influence the model to select higher-probability options. Another way to think about temperature is to think about creativity. A low number (close to zero) would produce the least creative and most deterministic response. -* AI21 Labs Jamba models use a temperature range of 0 \u00e2\u0080\u0093 2.0 Top p refers to the percentage of token candidates the model can choose from for the next token in the response. A lower value will decrease the size of the pool and limit the options to more likely outputs. A higher value will increase the size of the pool and allow for lower-probability tokens. This parameter is used to limit the maximum response a model can give. Bedrock documentation on model default inference configuration -* AI21 Labs Jamba models use a temperature range of 0 \u00e2\u0080\u0093 2.0"}, {"source": "data/raw_pages/react_build-a-backend_data_data-modeling_disable-operations.txt", "text": "Disable Operations The disableOperations method allows you to selectively disable specific GraphQL operations for a model in your Amplify application. This can be useful for implementing specialized API designs and reduce the number of resources being deployed. You can disable operations by adding the disableOperations method to your model definition: The disableOperations method accepts an array of operation types that you want to disable: mutations : Disables all mutation operations (create, update, delete) subscriptions : Disables all real-time subscription operations (onCreate, onUpdate, onDelete) queries : Disables all query operations (get, list) You can also disable more granular operations: Query Operations get : Disables the ability to fetch a single item by ID list : Disables the ability to fetch multiple items create : Disables the ability to create new items update : Disables the ability to update existing items delete : Disables the ability to delete items onCreate : Disables real-time notifications when items are created onUpdate : Disables real-time notifications when items are updated onDelete : Disables real-time notifications when items are deleted You can specify one or more operation types in the array to disable them:"}, {"source": "data/raw_pages/react_build-a-backend_auth_manage-users_manage-webauthn-credentials.txt", "text": "Manage WebAuthn credentials Amplify Auth enables your users to associate, keep track of, and delete passkeys. Note that users must be authenticated to register a passkey. That also means users cannot create a passkey during sign up; consequently, they must have at least one other first factor authentication mechanism associated with their account to use WebAuthn. You can associate a passkey using the following API: The user will be prompted to register a passkey using their local authenticator. Amplify will then associate that passkey with Cognito. You can list registered passkeys using the following API: You can delete a passkey with the following API: Here is a code example that uses the list and delete APIs together. In this example, the user has 3 passkeys registered. They want to list all passkeys while using a pageSize of 2 as well as delete the first passkey in the list."}, {"source": "data/raw_pages/react_build-a-backend_data_connect-to-existing-data-sources_connect-postgres-mysql-database.txt", "text": "Connect your app to existing MySQL and PostgreSQL database Amplify's native integration supports any MySQL or Postgres database, no matter if they're hosted on AWS within a VPC or outside of AWS with a 3rd party hosted database provider. You must create a connection string using the following database information to get started: Database hostname Database port Database username Database user password Database name First, provide all the database connection information as secrets, you can use the Amplify sandbox's secret functionality to set them or go to the Amplify console to set secrets in a shared environment: MySQL PostgreSQL Connection string format for MySQL mysql://user:password@hostname:port/db-name Connection string format for PostgreSQL postgres://user:password@hostname:port/db-name Run the following command to generate the Data schema with your database connection information. It'll infer an a.model() representation for all database tables that have primary key specified . Connecting to a database behind the VPC If your RDS database exists within a VPC, it must be configured to be Publicly accessible . This does not mean the instance needs to be accessible from all IP addresses on the Internet, but this flag is required to allow your local machine to connect via an Inbound Rule rather than being inside your VPC or connected to the VPC via VPN. To generate the TypeScript representation of your database schema: If your database is protected by a VPC, you will need to add an Inbound Rule for the database port from your local IP address. The npx ampx generate schema-from-database command connects to your database from your local workstation to read schema information. If you are connecting to an RDS Proxy, the machine you run the generate schema-from-database command must be in the same VPC as the proxy itself, or you must connect to it via VPN. Simply opening an Inbound Rule for the database port is not sufficient. To connect to your database during runtime: When you specify connection information, we'll compare the hostname you supply against a list of RDS instances, clusters, and proxies in your account in the same region as your project. If we find a match, we'll automatically detect the VPC configuration for your database and provision a SQL Lambda function to connect to the database and retrieve the schema. The VPC security group in which your database instance, cluster, or proxy is installed must have Inbound rules that allow traffic from the following TCP ports: The specified database port (e.g., 3306 for MySQL databases or 5432 for Postgres databases). Port 443 (HTTPS) to allow the Lambda function to connect to AWS Systems Manager to retrieve configuration parameters. Finally, the security group must have Outbound rules that allow traffic on those same ports from the security group itself. Close accordion Handling of implicit fields (id, createdAt, updatedAt) When creating new DynamoDB-backed data models via a.model() , a set of a implicit fields, such as id , createdAt , and updatedAt are added by default. When connecting to an existing SQL database, these fields are not implicitly"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-to-existing-data-sources_connect-postgres-mysql-database.txt", "text": "added as they are not part of the underlying data source. If createdAt and updatedAt are valid columns in the underlying database table, then Amplify Data will automatically populate those fields with their respective values upon create and update mutations. Close accordion RDS Proxy for improved connectivity Consider adding an RDS Proxy in front of the cluster to manage database connections. When using Amplify GraphQL API with a relational database like Amazon RDS, each query from your application needs to open a separate connection to the database. If there are a large number of queries occurring concurrently, it can exceed the connection limit on the database and result in errors like \"Too many connections\". To avoid this, Amplify can use an RDS Proxy when connecting your GraphQL API to a database. The RDS Proxy acts as an intermediary sitting in front of your database. Instead of each application query opening a direct connection to the database, they will connect through the Proxy. The Proxy helps manage and pool these connections to avoid overwhelming your database cluster. This improves the availability of your API, allowing more queries to execute concurrently without hitting connection limits. However, there is a tradeoff of increased latency - queries may take slightly longer as they wait for an available connection from the Proxy pool. There are also additional costs associated with using RDS Proxy. Please refer to the pricing page for RDS Proxy to learn more. Close accordion Connecting to a database with a custom SSL certificate Amplify creates an AWS Lambda function using a Node.js runtime to connect your AppSync API to your SQL database. The Lambda function connects to the database using Secure Socket Layer (SSL) or Transport Layer Security (TLS) to protect data in transit. Amplify automatically uses the correct root certificate authority (CA) certificates for Amazon RDS databases, and the Node.js runtime includes root CAs from well-known certificate providers to connect to non-RDS databases. However, if your database uses a custom or self-signed SSL certificate, you can upload the PEM-encoded public CA certificate of 4 KB or less to your Amplify project as a secret when you generate the database configuration, and specify that secret when generating the schema from your database: The Lambda function will then use the specified root CA to validate connections to the database. When deploying your app to production, you need to add the PEM-encoded public CA certificate as a secret . Make sure to add the certificate with the same secret name you used in the sandbox environment. For example, we used CUSTOM_SSL_CERT above. Make sure to preserve the newlines and the ------BEGIN CERTIFICATE------ and ------END CERTIFICATE------ delimiters in the value. Finally, make sure the size of the entire value does not exceed 4KB. Close accordion This creates a new schema.sql.ts with a schema reflecting the types of your database. Do not edit the schema.sql.ts file directly . Import the schema to your amplify/data/resource.ts file and apply any additive changes there. This ensures that you can"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-to-existing-data-sources_connect-postgres-mysql-database.txt", "text": "continuously regenerate the TypeScript schema representation of your SQL database without losing any additive changes that you apply out-of-band. import { type ClientSchema , a , defineData } from '@aws-amplify/backend' ; Copy highlighted code example import { schema as generatedSqlSchema } from './schema.sql' ; const sqlSchema = generatedSqlSchema . authorization ( allow => allow . guest ( ) ) const schema = a . schema ( { } ) . authorization ( allow => [ allow . guest ( ) ] ) Copy highlighted code example const combinedSchema = a . combine ( [ schema , sqlSchema ] ) ; Copy highlighted code example export type Schema = ClientSchema < typeof combinedSchema > ; export const data = defineData ( { Copy highlighted code example Use the .setAuthorization() modifier to set model-level and field-level authorization rules for the SQL-backed data models. Review Customize your auth rules for detailed authorization rule options. Copy highlighted code example const sqlSchema = generatedSqlSchema . setAuthorization ( ( models ) => [ models . event . authorization ( ( allow ) => [ allow . publicApiKey ( ) ] ) , models . event . fields . id . authorization ( allow => [ allow . publicApiKey ( ) , allow . guest ( ) ] ) , models . event . fields . created_at . authorization ( allow => [ allow . publicApiKey ( ) , allow . guest ( ) ] ) Finally, you can now validate your Data resources with your cloud sandbox: On the client side, you can now make create, read, update, delete, and subscribe to your SQL-backed data models: When deploying your app to production, you need to add the database connection string as a secret . Make sure to add the appropriate database connection string with the same secret name you used in the sandbox environment. For example, we used SQL_CONNECTION_STRING above. To improve the ergonomics of your API, you might want to rename the generate fields or types to better accommodate your use case. Use the renameModels() modifier to rename the auto-inferred data models. const sqlSchema = generatedSqlSchema . authorization ( allow => allow . guest ( ) ) Copy highlighted code example const sqlSchema = generatedSqlSchema . authorization ( allow => allow . guest ( ) ) Copy highlighted code example . setRelationships ( ( models ) => [ models . Note . relationships ( { comments : a . hasMany ( \"Comment\" , \"note_id\" ) , models . Comment . relationships ( { note : a . belongsTo ( \"Note\" , \"note_id\" ) Use the .addToSchema(...) to add in additional queries, mutations, and subscriptions to your auto-generated SQL data schema. Note: you can't add additional data models via a.model() . They should be exclusively generated via npx ampx generate schema-from-database . const sqlSchema = generatedSqlSchema . authorization ( allow => allow . guest ( ) ) Copy highlighted code example listEventsWithDecodedLatLong : a . query ( ) . returns ( a . ref ( \"EventWithDecodedCoord\""}, {"source": "data/raw_pages/react_build-a-backend_data_connect-to-existing-data-sources_connect-postgres-mysql-database.txt", "text": ") . array ( ) ) . handler ( a . handler . inlineSql ( . authorization ( allow => [ allow . guest ( ) ] ) , EventWithDecodedCoord : a . customType ( { You can define the different SQL handlers in separate .sql files and reference them in your custom queries / mutations. First, configure the custom query or mutation in your amplify/data/resource.ts file: const sqlSchema = generatedSqlSchema . authorization ( allow => allow . guest ( ) ) createNewLocationWithLongLat : a . mutation ( ) lat : a . float ( ) . required ( ) , long : a . float ( ) . required ( ) , name : a . string ( ) . required ( ) , address : a . string ( ) . required ( ) . returns ( a . json ( ) . array ( ) ) . authorization ( allow => allow . authenticated ( ) ) Copy highlighted code example . handler ( a . handler . sqlReference ( './createNewLocationWithLongLat.sql' ) ) Next, add a corresponding sql file to handle the request: The return type for custom queries and mutations expecting to return row data from SQL statements must be an array of the corresponding model. This is true even for custom get queries, where a single row is expected. Example slug : a . string ( ) . required ( ) , Copy highlighted code example . returns ( a . ref ( \"Post\" ) . array ( ) ) SELECT id, title, slug, content, created_at, updated_at The Amplify uses AWS Lambda functions to enable features like querying data from your database. To work properly, these Lambda functions need access to common logic and dependencies. Amplify provides this shared code in the form of Lambda Layers. You can think of Lambda Layers as a package of reusable runtime code that Lambda functions can reference. When you deploy an Amplify API, it will create two Lambda functions: This allows you to query and write data to your database from your API. NOTE: If the database is in a VPC, this Lambda function will be deployed in the same VPC as the database. The usage of Amazon Virtual Private Cloud (VPC) or VPC peering, with AWS Lambda functions will incur additional charges as explained, this comes with an additional cost as explained on the Amazon Elastic Compute Cloud (EC2) on-demand pricing page . This automatically keeps the SQL Lambda up-to-date by managing its Lambda Layers. A Lambda layer that includes all the core SQL connection logic lives within the AWS Amplify service account but is executed within your AWS account, when invoked by the SQL Lambda. This allows the Amplify service team to own the ongoing maintenance and security enhancements of the SQL connection logic. This allows the Amplify team to maintain and enhance the SQL Layer without needing direct access to your Lambdas. If updates to the Layer are needed, the Updater Lambda will receive a"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-to-existing-data-sources_connect-postgres-mysql-database.txt", "text": "signal from Amplify and automatically update the SQL Lambda with the latest Layer. Note: MySQL does not support time zone offsets in date time or timestamp fields. Instead, we will convert these values to datetime , without the offset. Unlike MySQL, PostgreSQL does support date time or timestamp values with an offset. To return the actual SQL error instead of a generic error from underlying API responses, an environment variable DEBUG_MODE can be set to true on the Amplify-generated SQL Lambda function. You can find this Lambda function in the AWS Lambda console with the naming convention of: <stack-name>-<api-name>-SQLLambdaFunction<hash> . This is likely because the table doesn't have a designated primary key. A primary key is required for npx ampx generate schema-from-database to infer the table structure and create a create, read, update, and delete API."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_rest-api_test-api.txt", "text": "Test the REST API If unauthenticated guest users have access to your REST API you can test it from the terminal using curl . curl is a command-line tool that lets you transfer data to and from a server using various protocols. Curl is available in many distributions including Mac, Windows and Linux. Follow the install instructions in the docs . Let's test your new REST API using the route below with HTTP Method GET and path /items?limit=10 which includes a limit query string parameter. Sign in to the API Gateway console Choose the myRestApi REST API In the Resources pane, choose the method you want to test. Select GET right under /items . In the Method Execution pane, select TEST . Choose the GET method and add limit=10 to the query string {items} field. Choose Test to run the test for GET /items?limit=10 . The following information will be displayed: request, status, latency, response body, response headers and logs."}, {"source": "data/raw_pages/react_build-a-backend_data_customize-authz_custom-data-access-patterns.txt", "text": "Custom data access using Lambda functions You can define your own custom authorization rule with a Lambda function. In your application, you can perform CRUD operations against the model using client.models.<model-name> with the lambda auth mode. import { generateClient } from 'aws-amplify/data' ; import type { Schema } from '../amplify/data/resource' ; const client = generateClient < Schema > ( ) ; const { errors , data : newTodo } = await client . models . Todo . create ( Copy highlighted code example The Lambda function of choice will receive an authorization token from the client and execute the desired authorization logic. The AppSync GraphQL API will receive a payload from Lambda after invocation to allow or deny the API call accordingly. To configure a Lambda function as the authorization mode, create a new file amplify/data/custom-authorizer.ts . You can use this Lambda function code template as a starting point for your authorization handler code: You can use the template above as a starting point for your custom authorization rule. The authorization Lambda function receives the following event: Your Lambda authorization function needs to return the following JSON: Review the Amplify documentation to set the custom authorization token for the Data client ."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_analytics_identify-user.txt", "text": "Identify user This API sends information about the current user to Amazon Pinpoint. Additional information such as the user's name, email, location, and device can be included by specifying the UserProfile . Custom attributes can also be included by setting UserProfile.customProperties . If the user was signed in through signIn you can retrieve the current user's ID as shown below: Sending user information allows you to associate a user to their user profile and activities or actions in your app. The user's actions and attributes can also tracked across devices and platforms by using the same userId . Some scenarios for identifying a user and their associated app activities are: When a user completes app sign up When a user completes sign in process When a user launches your app When a user modifies or updates their user profile"}, {"source": "data/raw_pages/react_start_connect-to-aws-resources.txt", "text": "Connect to AWS resources Amplify client libraries provide you with the flexibility to directly connect your application to AWS resources such as AWS AppSync, Amazon Cognito, Amazon S3, and more. To get started, client libraries must be configured . This is typically done by using the amplify_outputs.json file generated by the Amplify backend tooling, however using the client libraries does not require backend resources to be created by Amplify. For JavaScript-based applications, the client library can be configured by using the generated outputs file: Or by configuring the library directly by passing a ResourcesConfig object. For example, to configure the client library for use with Amazon Cognito, specify the Auth configuration: By configuring the client library, Amplify automates the communication with the underlying AWS resources, and provides a friendly API to author your business logic. In the snippet below, the signIn function does not require passing information from your Cognito resource to initiate the sign-in flow. For more information about how to use the Amplify client libraries with existing AWS resources, visit the guides:"}, {"source": "data/raw_pages/react_build-a-backend_data_connect-from-server-runtime_nextjs-server-runtime.txt", "text": "Next.js server runtime This guide walks through how you can connect to Amplify Data from Next.js Server-side Runtimes (SSR). For Next.js applications, Amplify provides first-class support for the App Router (React Server Components, Route Handlers, and Server Actions) , the Pages Router (Components, API Routes) , and Middleware . Before you begin, you will need: Connecting to Amplify Data will include choosing the correct data client for Next.js server runtimes, generating the data client, and then calling the API. Amplify offers two specialized data clients for Next.js server runtimes (from @aws-amplify/adapter-nextjs/data ) that you should use depending whether you retrieve the user tokens using cookies or NextRequest and NextResponse : generateServerClientUsingCookies() \u00f0\u009f\u008d\u00aa generates a data client with the Next.js cookies function from next/headers . Each API request dynamically refetches the cookies at runtime. generateServerClientUsingReqRes() \u00f0\u009f\u008c\u0090 generates a data client requiring NextRequest and NextResponse provided to an runWithAmplifyServerContext function to prevent token contamination. Choose the correct data client based on your Next.js Router (App or Pages) and then the use case: generateServerClientUsingCookies() \u00f0\u009f\u008d\u00aa generateServerClientUsingReqRes() \u00f0\u009f\u008c\u0090 To generate a Data client for the Next.js server runtime using cookies, you need to provide both your Amplify configuration and the cookies function from Next.js. We recommend you generate Amplify Data's server client in a utility file. Then, import the generated client in your Next.js React Server Components, Server Actions, or Route Handlers. To generate a data client for the Next.js server runtime using NextRequest and NextResponse , you only need to provide your Amplify configuration. When making the individual API requests, you will need to pass the config to the runWithAmplifyServerContext function to pass in the cookies from request and response variables. We recommend you generate the server Data client in a utility file. Then, import the generated client in your Next.js Middleware, component's server runtime code, and API Routes. You can make any available query or mutation request with the generated server data clients; however, note that subscriptions are not available within server runtimes. generateServerClientUsingCookies() \u00f0\u009f\u008d\u00aa generateServerClientUsingReqRes() \u00f0\u009f\u008c\u0090 Import the cookie-based server Data client in your Next.js React Server Component code and make your API requests. Import the NextRequest/NextResponse-based server Data client in your Next.js server runtime code and make your API requests within the runWithAmplifyServerContext function. Review Server-side Rendering to learn more about creating an Amplify server context. For example, in a Next.js Pages Router API route, use the req and res parameters from the handler function with runWithAmplifyServerContext :"}, {"source": "data/raw_pages/react_build-a-backend_data_data-modeling_add-fields.txt", "text": "Add fields to data model Amplify Data supports all AWS AppSync scalar types as field types. The following scalar types are available: Sometimes, the built-in types do not meet the needs of your application. In those cases, you can specify custom types. You can either define the custom types inline or explicitly define the custom type in the schema. Inline definition: The \"location\" field will become a new non-model type that uses PascalCase, a naming convention in which the first letter of each word in a compound word is capitalized. If there are conflicts with another schema-level definition (model, custom type, enum), you will receive a Type error with a warning that you need to sift the value out as a separate item and use a \"ref\". Explicit definition: Specify the \"Location\" as a.customType() in your schema. To use the custom type, reference it through a.ref() in the respective field definitions. To set or read the location field on the client side, you can expand a nested object and the type system will auto-infer the allowed values. Enum has a similar developer experience as custom types: short-hand and long-form approaches. Short-hand approach Long-form approach When creating a new item client-side, the enums are also type-enforced: You can list available enum values client-side using the client.enums.<ENUM_NAME>.values() API. For example, this allows you to display the available enum values within a dropdown UI. By default, fields are optional. To mark a field as required, use the .required() modifier. Any field can be modified to be an array using the .array() modifier. You can use the .default(...) modifier to specify a default value for optional scalar type fields . The .default(...) modifier is not available for custom types, arrays, or relationships. Note: The .default(...) modifier can't be applied to required fields."}, {"source": "data/raw_pages/react_build-ui_formbuilder.txt", "text": "Connected forms Connected Forms are bound to a model in your app's data schema. Whenever a connected form is submitted, a record is automatically created or updated in the bound data model, with some or all of the form's input fields mapping to fields in the data model. Connected forms automatically work with any Amplify GraphQL API, and no onSubmit handling is required. First, install the Amplify UI library. To use connected forms, you first need to deploy a data model from your sandbox environment. We will use the same example as in the getting started tutorial . To get started run the following command from your project root: This will generate create and update forms for each model defined in your schema in a folder called ui-components . In Gen 2, we automatically generate the form UI for you, which you can then customize and manage. If you decide to update your data model and need to regenerate the forms, please ensure you back up the original ui-components folder before executing the npx ampx generate forms command again. In your application's entrypoint file (e.g. src/index.js for create-react-app or src/main.jsx for Vite), add the following imports and configuration Copy highlighted code example import '@aws-amplify/ui-react/styles.css' ; import { ThemeProvider } from '@aws-amplify/ui-react' ; import { Amplify } from 'aws-amplify' ; import outputs from './amplify_outputs.json' ; Amplify . configure ( outputs ) ; In your application's entrypoint file (e.g. src/main.jsx for Vite), wrap the <App /> component with the following: Import your form by name. For a form named TodoCreateForm , you would use the following code: Place your form in code. For a form named ProductCreateForm in a React project, you could use the following App code: All connected and unconnected forms are either a Create form or an Update form. Create forms render a form with empty inputs. If a create form is connected to a data model, will always generate a new record upon submission. Update forms expect an input value in order to pre-populate the form. For update forms that are connected to a data model, you can use the id prop, or the model prop: id prop: id string of the record you want to update. For example: Model prop: if your form is bound to a data model named Author , your form will have a prop named author as well, which can receive a record. For example:"}, {"source": "data/raw_pages/react_deploy-and-host_fullstack-branching.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_analytics_personalize-recommendations.txt", "text": "Personalized recommendations Amazon Personalize can create recommendations by using event data, historical data, or a combination of both. The event data can then be used to create recommendations. To record event data, you need the following: For more information, see Record Events . After creating the Amazon Personalize dataset group, you need to add the personalize:PutEvents permission to your AWS Identity and Access Management (IAM) user roles. An example IAM policy: You need the tracking ID of your event tracker. For more information, see Get a Tracking ID . Configure Amazon Personalize: You can use the Identify event type to track a user identity. This lets you connect a user to their actions and record traits about them. To identify a user, specify a unique identifier for the userId property. Consider the following user interactions when choosing when and how often to call record with the Identify eventType: After a user registers. After a user logs in. When a user updates their information (For example, changing or adding a new address). Upon loading any pages that are accessible by a logged-in user (optional). You can send events to Amazon Personalize by calling the record operation. If you already use Identify to track end-user data, you can skip the userId, the SDK will fetch the userId based on current browser session. For information about the properties field, see Put Events . You can track iframe and HTML5 media types by using the MediaAutoTrack event type. MediaAutoTrack tracks all media events of the media DOM element that you bind to. MediaAutoTracker will automatically track Play , Pause , Ended , TimeWatched , and Resume in eventType . The duration of the event compared to the total length of the media is stored as a percentage value in eventValue . The recorded events are saved in a buffer and sent to the remote server periodically (You can tune it with the flushInterval option) . If needed, you have the option to manually clear all the events from the buffer by using the 'flushEvents' API."}, {"source": "data/raw_pages/react_build-a-backend_auth_examples_microsoft-entra-id-saml.txt", "text": "Microsoft Entra ID (SAML) Microsoft Entra ID can be configured as a SAML provider for use with Amazon Cognito. Integrating Entra ID enables you to sign in with your existing enterprise users, and maintain profiles unique to the Amplify Auth resource for use within your Amplify app. To learn more, visit the Azure documentation for SAML authentication with Microsoft Entra ID . Note: the following guidance showcases configuration with your personal cloud sandbox . You will need to repeat the configuration steps for branch deployments after confirming functionality against your sandbox. To get started, define your auth resource with the appropriate redirect URIs: Deploy to your personal cloud sandbox with npx ampx sandbox . This will generate a domain you can use to configure your new Entra ID App. After deploying your changes successfully, copy the generated domain value from amplify_outputs.json Next, navigate to portal.azure.com , select Entra ID . In your default directory, or company's existing directory, under Manage , select Enterprise Applications Afterwards, select New application , then select Create your own application . Specify a name for the application and choose Integrate any other application you don't find in the gallery (Non-gallery) Now that you have created the new enterprise application you can begin to configure Single Sign-on with SAML. Select Single sign-on Then select SAML You will be directed to a page to set up single sign-on with SAML, which needs a few pieces of information from your Amplify Auth resource. In the Basic SAML Configuration step, select Edit and populate with the appropriate values. Warning: there is a known limitation where upstream sign-out functionality successfully signs out of Entra ID, but fails to redirect back to the user app. This behavior is disabled by default with SAML integrations in Amplify Auth. Save the configuration and proceed to Step 3's SAML Certificates section. Copy the App Federation Metadata Url Now that you've configured your SAML provider with Microsoft Entra ID and copied the App Federation Metadata Url , configure your auth resource with the new SAML provider and paste the URL value into the metadataContent property: User attributes can be found in Step 2's Attributes & Claims section, and are prefixed with a namespace by default. The example above shows mapping the default claim for the SAML user's email address, however additional attributes can be mapped. In the AWS Console, navigate to your Cognito User Pool. Select the identity provider, MicrosoftEntraIDSAML , created after configuring Amplify Auth with the Entra ID SAML provider. Select View signing certificate and download as .crt Rename the file extension to .cer in order to upload to Azure. On the Single sign-on page, scroll down to Step 3 ( SAML Certificates ), and under Verification Certificates (optional) , select Edit . Select Require verification certificates and upload the certificate. Save your changes, and now requests to Entra ID from your Cognito User Pool will be verified. Now your users are ready to sign in with Microsoft Entra ID. To sign in"}, {"source": "data/raw_pages/react_build-a-backend_auth_examples_microsoft-entra-id-saml.txt", "text": "with this custom provider, specify the provider name as the name specified in your auth resource definition: MicrosoftEntraIDSAML"}, {"source": "data/raw_pages/react_build-ui_formbuilder_validations.txt", "text": "Validate form data Sanitize user input by adding validation rules to your form. By default, Amplify generated forms infers a range of validation rules based on the data model. For example, given a data model with an AWSEmail field, the generated form input will automatically run an email validation rule. By default, the following validation rules are available for you to configure: For the types below, we automatically apply validation rules on form inputs: AWSIPAddress : input value must be a valid IPv4 or IPv6 address. AWSURL : input value must consist of a schema ( http , mailto ) and a path part. Path part can't contain two forward slashes (//). AWSEmail : input value must be an email address in the format <local-part>@<domain-part> . AWSJSON : input value must be a valid JSON. AWSPhone : input value must be a phone number that can contain either spaces or hyphens to separate digit groups. Every form provides an onValidate event handler to provide additional validation rules via code. Return an object with validation functions for the fields you want to validate. In the example below, address must start with a number, otherwise return any existing auto-generated validation responses. Note: the validation function must return a validation response of the following shape: Amplify generated forms can also produce nested JSON object. For example, you can create a new ProductForm component based on the following JSON object: To add validation rules to the nested objects, pass in validation functions in the same nested structure as the data: Sometimes your form needs to asynchronously validate an input with an external API or database before the form data is submitted. Return a Promise in the onValidate prop to run an asynchronous validation rule. In the following example, we check with an external API if a real estate agent exist based on a given license number:"}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_sign-out.txt", "text": "Sign-out Amplify provides a client library that enables you to interact with backend resources such as Amplify Auth. The quickest way to get started with Amplify Auth in your frontend application is with the Authenticator component , which provides a customizable UI and complete authentication flows. To sign a user out of your application use the signOut API. You can also sign out users from all devices by performing a global sign-out. This will also invalidate all refresh tokens issued to a user. The user's current access and ID tokens will remain valid on other devices until the refresh token expires (access and ID tokens expire one hour after they are issued)."}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_connect-bedrock.txt", "text": "Connect to Amazon Bedrock for generative AI use cases Amazon Bedrock is a fully managed service that removes the complexity of using foundation models (FMs) for generative AI development. It acts as a central hub, offering a curated selection of high-performing FMs from leading AI companies like Anthropic, AI21 Labs, Cohere, and Amazon itself. Amazon Bedrock streamlines generative AI development by providing: Choice and Flexibility : Experiment and evaluate a wide range of FMs to find the perfect fit for your use case. Simplified Integration : Access and use FMs through a single, unified API, reducing development time. Enhanced Security and Privacy : Benefit from built-in safeguards to protect your data and prevent misuse. Responsible AI Features : Implement guardrails to control outputs and mitigate bias. In the following sections, we walk through the steps to add Amazon Bedrock to your API as a data source and connect to it from your Amplify app: Add Amazon Bedrock as a data source Define a custom query Configure custom business logic handler code Invoke a custom query to prompt a generative AI model To connect to Amazon Bedrock as a data source, you can choose between two methods - using a Lambda function or a custom resolver powered by AppSync JavaScript resolvers. The following steps demonstrate both methods: Function Custom resolver powered by AppSync JavaScript resolvers In your amplify/backend.ts file, replace the content with the following code to add a lambda function to your backend and grant it permission to invoke a generative AI model in Amazon Bedrock. The generateHaikuFunction lambda function will be defined in and exported from the amplify/data/resource.ts file in the next steps: In your amplify/backend.ts file, replace the content with the following code to add an HTTP data source for Amazon Bedrock to your API and grant it permissions to invoke a generative AI model: For the purpose of this guide, we will use Anthropic's Claude 3 Haiku to generate content. If you want to use a different model, you can find the ID for your model of choice in the Amazon Bedrock documentation's list of model IDs or the Amazon Bedrock console and replace the value of MODEL_ID . The availability of Amazon Bedrock and its foundation models may vary by region. The policy statement in the code above assumes that your Amplify app is deployed in a region supported by Amazon Bedrock and the Claude 3 Haiku model. If you are deploying your app in a region where Amazon Bedrock is not available, update the code above accordingly. For a list of supported regions please refer to the Amazon Bedrock documentation . Function Custom resolver powered by AppSync JavaScript resolvers Next, replace the contents of your amplify/data/resource.ts file with the following code. This will define and export a lambda function that was granted permission to invoke a generative AI model in Amazon Bedrock in the previous step. A custom query named generateHaiku is added to the schema with the generateHaikuFunction as the handler using the a.handler.function()"}, {"source": "data/raw_pages/react_build-a-backend_data_custom-business-logic_connect-bedrock.txt", "text": "modifier: With Amazon Bedrock added as a data source, you can reference it in custom queries using the a.handler.custom() modifier which accepts the name of the data source and an entry point for your resolvers. Replace the contents of your amplify/data/resource.ts file with the following code to define a custom query named generateHaiku in the schema: Function Custom resolver powered by AppSync JavaScript resolvers Next, create a generateHaiku.ts file in your amplify/data folder and use the following code to define a custom resolver for the custom query added to your schema in the previous step: The following code uses the BedrockRuntimeClient from the @aws-sdk/client-bedrock-runtime package to invoke the generative AI model in Amazon Bedrock. The handler function takes the user prompt as an argument, invokes the model, and returns the generated haiku. Next, create a generateHaiku.js file in your amplify/data folder and use the following code to define a custom resolver for the custom query added to your schema in the previous step: The following code defines a request function that constructs the HTTP request to invoke the generative AI model in Amazon Bedrock. The response function parses the response and returns the generated haiku. The code above uses the Messages API , which is supported by chat models such as Anthropic's Claude 3 Haiku. The system prompt is used to give the model a persona or directives to follow, and the messages array can contain a history of messages. The max_tokens parameter controls the maximum number of tokens the model can generate, and the temperature parameter determines the randomness, or creativity, of the generated response. From your generated Data client, you can find all your custom queries and mutations under the client.queries and client.mutations APIs respectively. The custom query below will prompt a generative AI model to create a haiku based on the given prompt. Replace the prompt value with your desired prompt text or user input and invoke the query as shown below: Here's an example of a simple UI that prompts a generative AI model to create a haiku based on user input: In this guide, you learned how to connect to Amazon Bedrock from your Amplify app. By adding Bedrock as a data source, defining a custom query, configuring custom business logic handler code, and invoking custom queries, you can leverage the power of generative AI models in your application. To clean up, you can delete your sandbox by accepting the prompt when terminating the sandbox process in your terminal. Alternatively, you can also use the AWS Amplify console to manage and delete sandbox environments."}, {"source": "data/raw_pages/vue_start_quickstart.txt", "text": "Quickstart \u00f0\u009f\u0091\u008b Welcome to AWS Amplify! In this quickstart guide, you will: Deploy a Vue.js app Build and connect to a database with real-time data updates Configure authentication and authorization rules We've created a starter \"To-do\" application to help get started faster. First, you will create a repository in your GitHub account using our starter Vue template. Use our starter template to create a repository in your GitHub account. This template scaffolds create-vite-app with Amplify backend capabilities. Create repository from template Use the form in GitHub to finalize your repo's creation. Now that the repository has been created, deploy it with Amplify. Deploy to AWS Select GitHub . After you give Amplify access to your GitHub account via the popup window, pick the repository and main branch to deploy. Make no other changes and click through the flow to Save and deploy . While you are waiting for your app to deploy (~5 mins) Learn about the project structure Let's take a tour of the project structure in this starter repository by opening it on GitHub. The starter application has pre-written code for a to-do list app. It gives you a real-time database with a feed of all to-do list items and the ability to add new items. Close accordion When the build completes, visit the newly deployed branch by selecting \"Visit deployed URL\". Since the build deployed an API, database, and authentication backend, you will be able to create new to-do items. In the Amplify console, click into the deployment branch (in this case main ) > select Data in the left-hand menu > Data manager to see the data entered in your database. Let's learn how to enhance the app functionality by creating a delete flow for to-do list items. Now let's set up our local development environment to add features to the frontend. Click on your deployed branch and you will land on the Deployments page which shows you your build history and a list of deployed backend resources. At the bottom of the page you will see a tab for Deployed backend resources . Click on the tab and then click the Download amplify_outputs.json file button. Clone the repository locally. Now move the amplify_outputs.json file you downloaded above to the root of your project. The amplify_outputs.json file contains backend endpoint information, publicly-viewable API keys, authentication flow information, and more. The Amplify client library uses this outputs file to connect to your Amplify Backend. You can review how the outputs file is imported within the main.tsx file and then passed into the Amplify.configure(...) function of the Amplify client library. Close accordion Go to the components/Todos.vue file and add in a new deleteTodo functionality and pass function into the <li> element's onClick handler. Try out the deletion functionality now by starting the local dev server: This should start a local dev server at http://localhost:5173 . The starter application already has a pre-configured auth backend defined in the amplify/auth/resource.ts file. We've configured it to support email and password login"}, {"source": "data/raw_pages/vue_start_quickstart.txt", "text": "but you can extend it to support a variety of login mechanisms, including Google, Amazon, Sign In With Apple, and Facebook. The fastest way to get your login experience up and running is to use our Authenticator UI component. In your src/App.vue file, import the Authenticator UI component and wrap your <main> template. The Authenticator component auto-detects your auth backend settings and renders the correct UI state based on the auth backend's authentication flow. Try out your application in your localhost environment again. You should be presented with a login experience now. To get these changes to the cloud, commit them to git and push the changes upstream. Amplify automatically deploys the latest version of your app based on your git commits. In just a few minutes, when the application rebuilds, the hosted app will be updated to support the deletion functionality. Let's update our backend to implement per-user authorization rules, allowing each user to only access their own to-dos. To make backend updates, we are going to require AWS credentials to deploy backend updates from our local machine. Skip ahead to step 8 , if you already have an AWS profile with credentials on your local machine, and your AWS profile has the AmplifyBackendDeployFullAccess permission policy. Otherwise, set up local AWS credentials that grant Amplify permissions to deploy backend updates from your local machine. To update your backend without affecting the production branch, use Amplify's cloud sandbox. This feature provides a separate backend environment for each developer on a team, ideal for local development and testing. To start your cloud sandbox, run the following command in a new Terminal window : Once the cloud sandbox has been fully deployed (~5 min), you'll see the amplify_outputs.json file updated with connection information to a new isolated authentication and data backend. The npx ampx sandbox command should run concurrently to your npm run dev . You can think of the cloud sandbox as the \"localhost-equivalent for your app backend\". The to-do items in the starter are currently shared across all users, but, in most cases, you want data to be isolated on a per-user basis. To isolate the data on a per-user basis, you can use an \"owner-based authorization rule\". Let's apply the owner-based authorization rule to your to-do items: In the application client code, let's also render the username to distinguish different users once they're logged in. Now, let's go back to your local application and test out the user isolation of the to-do items. You will need to sign up new users again because now you're working with the cloud sandbox instead of your production backend. To get these changes to the cloud, commit them to git and push the changes upstream. Once your build completes in the Amplify Console, the main backend will update to support the changes made within the cloud sandbox. The data in the cloud sandbox is fully isolated and won't pollute your production database. That's it! You have successfully built a fullstack app on AWS"}, {"source": "data/raw_pages/vue_start_quickstart.txt", "text": "Amplify. If you want to learn more about how to work with Amplify, here's the conceptual guide for how Amplify works ."}, {"source": "data/raw_pages/react_build-a-backend_data_customize-authz_user-group-based-data-access.txt", "text": "User group-based data access You can use the group authorization strategy to restrict access based on user groups. The user group authorization strategy allows restricting data access to specific user groups or groups defined dynamically on each data record. When you want to restrict access to a specific set of user groups, provide the group names in the groups parameter. In the example below, only users that are part of the \"Admin\" user group are granted access to the Salary model. In your application, you can perform CRUD operations against the model using client.models.<model-name> with the userPool auth mode. import { generateClient } from 'aws-amplify/data' ; import type { Schema } from '../amplify/data/resource' ; const client = generateClient < Schema > ( ) ; const { errors , data : newSalary } = await client . models . Salary . create ( Copy highlighted code example This can then be updated to allow access to multiple defined groups; in this example below we added access for \"Leadership\". With dynamic group authorization, each record contains an attribute specifying what Cognito groups should be able to access it. Use the first argument to specify which attribute in the underlying data store holds this group information. To specify that a single group should have access, use a field of type a.string() . To specify that multiple groups should have access, use a field of type a.string().array() . By default, group authorization leverages Amazon Cognito user pool groups but you can also use OpenID Connect with group authorization. See OpenID Connect as an authorization provider . Known limitations for real-time subscriptions when using dynamic group authorization: If you authorize based on a single group per record, then subscriptions are only supported if the user is part of 5 or fewer user groups If you authorize via an array of groups ( groups: a.string().array() used in the example above), subscriptions are only supported if the user is part of 20 or fewer groups you can only authorize 20 or fewer user groups per record You can access a user's groups from their session using the Auth category:"}, {"source": "data/raw_pages/react_build-a-backend_auth_customize-auth-lifecycle_email-customization.txt", "text": "Email customization By default, Amplify Auth resources are scaffolded with email as the default method for your users to sign in. When you users sign up they receive a verification email to confirm their ownership of the email they specified during sign-up. Emails such as the verification email can be customized with your app's brand identity. To get started, change the email attribute of loginWith from true to an object to begin customizing its default behavior: In some cases, you may set up a user account on behalf of a user in the Amplify console . In this case, Amplify Auth will send an invitation email to the user welcoming them to your application. This email includes a brief welcome message, along with the email address they can log in with and the temporary password you've set up for them. If you'd like to customize that email, you can override the userInvitation attribute of the email object: Note that when using the user and code arguments of the emailBody function, user and code are functions which must be called. Failure to call them will result in an error when your sandbox deploys."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_predictions_identify-text.txt", "text": "Identify text Note: Make sure to complete the getting started section first, where you will set up the IAM roles with the right policy actions Detect text in an input image. Input can be sent directly from the browser or an Amazon S3 key from project bucket. The following options are independent of which source is specified. For demonstration purposes we will reference a file but it can be an S3 Key as well. Predictions.identify({text : {...}}) can detect unstructured text PLAIN , structured text from tables TABLE or text from forms FORM . For detecting plain text, you can see the whole detected text, the lines detected, the position of each line of text, and each word. For detecting structured forms (documents, tables, etc.) from an image, keyValues will return a string of the entity found in the image as well as metadata such as selected checkboxes or the relative location in the image using a boundingBox . For example the below image would return keyValues with \"Test\" or \"Checked\" as a key, and true since they are selected. The location of these elements would be returned in the boundingBox value. For detecting structured tables from an image For detecting tables and forms on the image just select format \"ALL\""}, {"source": "data/raw_pages/react_ai_conversation_tools.txt", "text": "Tools Tools allow LLMs to query information to respond with current and relevant information. They are invoked only if the LLM requests to use one based on the user's message and the tool's description. There are a few different ways to define LLM tools in the Amplify AI kit. Model tools Query tools Lambda tools The easiest way to define tools for your conversation route is with a.ai.dataTool() for data models and custom queries in your data schema. When you define a tool for a conversation route, Amplify takes care of the heavy lifting: Describing the tools to the LLM: Each tool definition is an Amplify model query or custom query that is defined in the schema. Amplify knows the input parameters needed for that tool and describes them to the LLM. Invoking the tool with the right parameters: After the LLM requests to use a tool with necessary input parameters, the conversation handler Lambda function invokes the tool, returns the result to the LLM, and continues the conversation. Maintaining the caller identity and authorization: Through tools, the LLM can only access data that the application user has access to. When the LLM requests to invoke a tool, we will call it with the user's identity. For example, if the LLM wanted to invoke a query to list Todos, it would only return the todos that user has access to. You can give the LLM access to your data models by referencing them in an a.ai.dataTool() with a reference to a model in your data schema. This requires that the model uses at least one of the following authorization strategies: Per user data access owner() ownerDefinedIn() ownersDefinedIn() Any signed-in user data access Per user group data access group() groupsDefinedIn() groups() groupsDefinedIn() This will let the LLM list and filter Post records. Because the data schema has all the information about the shape of a Post record, the data tool will provide that information to the LLM so you don't have to. Also, the Amplify AI kit handles authorizing the tool use requests based on the caller's identity. This means if you have an owner-based model, the LLM will only be able to query the user's records. The only supported model operation is 'list' . You can also give the LLM access to custom queries defined in your data schema. To do so, define a custom query with a function or custom handler and then reference that custom query as a tool. This requires that the custom query uses the allow.authenticated() authorization strategy. The Amplify data tool takes care of specifying the necessary input parameters to the LLM based on the query definition. Below is an illustrative example of a Lambda function handler for the getWeather query. Lastly, you will need to update your amplify/backend.ts file to include the newly defined getWeather function. You can connect to any AWS service by defining a custom query and calling that service in the function handler. To properly authorize the custom query function to"}, {"source": "data/raw_pages/react_ai_conversation_tools.txt", "text": "call the AWS service, you will need to provide the Lambda with the proper permissions. You can also define a tool that executes in the conversation handler AWS Lambda function. This is useful if you want to define a tool that is not related to your data schema or that does simple tasks within the Lambda function runtime. First install the @aws-amplify/backend-ai package. Define a custom conversation handler function in your data schema and reference the function in the handler property of the a.conversation() definition. Define the executable tool(s) and handler. Below is an illustrative example of a custom conversation handler function that defines a calculator tool. Note that we throw an error in the calculator tool example above if the input is invalid. This error is surfaced to the LLM by the conversation handler function. Depending on the error message, the LLM may try to use the tool again with different input or completing its response with test for the user. Lastly, update your backend definition to include the newly defined chatHandler function. Validate and sanitize any input from the LLM before using it in your application, e.g. don't use it directly in a database query or use eval() to execute it. Handle errors gracefully and provide meaningful error messages. Log and monitor tool usage to detect potential misuse or issues."}, {"source": "data/raw_pages/flutter_start_quickstart.txt", "text": "Quickstart Before you get started, make sure you have the following installed: Once you have installed Flutter, you can create a new Flutter project using the following command: In this Quickstart guide, you will build the application for web. However, if you want to run the application on other platforms, be sure to follow the required setup guide here . The easiest way to get started with AWS Amplify is through npm with create-amplify command. You can run it from your base project directory. First, go to the base project directory with the following command: After that, run the following to create an Amplify project: Running this command will scaffold Amplify backend files in your current project with the following files added: To deploy your backend use Amplify's per-developer cloud sandbox. This feature provides a separate backend environment for every developer on a team, ideal for local development and testing. To run your application with a sandbox environment, you can run the following command: The initial scaffolding already has a pre-configured auth backend defined in the amplify/auth/resource.ts file. We've configured it to support email and password login but you can extend it to support a variety of login mechanisms, including Google, Amazon, Sign In With Apple, and Facebook. The fastest way to get your login experience up and running is to use our Authenticator UI component available in the Amplify UI library. To use the Authenticator, you need to add the following dependencies to your project: You will add: amplify_flutter to connect your application with the Amplify resources. amplify_auth_cognito to connect your application with the Amplify Cognito resources. amplify_authenticator to use the Amplify UI components. After adding the dependencies, you need to run the following command to install the dependencies: Lastly update your main.dart file to use the Amplify UI components: The Authenticator component auto-detects your auth backend settings and renders the correct UI state based on the auth backend's authentication flow. Run your application in your local environment again. You should be presented with a login experience now. The initial scaffolding already has a pre-configured data backend defined in the amplify/data/resource.ts file. The default example will create a Todo model with content field. Let's modify this to add the following: A boolean isDone field. An authorization rules specifying owners, authenticated via your Auth resource can \"create\", \"read\", \"update\", and \"delete\" their own records. Update the defaultAuthorizationMode to sign API requests with the user authentication token. Next, let's implement UI to create, list, and delete the to-do items. Amplify can automatically generate code for interacting with the backend API. Run the command in the terminal to generate dart model classes from the Data schema under lib/models : Once you are done, add the API dependencies to your project. You will add amplify_api to connect your application with the Amplify API. After adding the dependencies, update the _configureAmplify method in your main.dart file to use the Amplify API: Next create a new widget called TodoScreen and add the following code to"}, {"source": "data/raw_pages/flutter_start_quickstart.txt", "text": "the end of the main.dart file: This will create a random Todo every time a user clicks on the floating action button. You can see the ModelMutations.create method is used to create a new Todo. And update the MyApp widget in your main.dart file like the following: Next add a _todos list in _TodoScreenState to add the results from the API and call the refresh function: and create a new function called _refreshTodos : and update the build function like the following: Now let's add the update and delete functionality. For update, add the following code to the onChanged method of the CheckboxListTile.adaptive widget: This will call the ModelMutations.update method to update the Todo with a copied/updated version of the todo item. So now the checkbox will get an update as well. For delete functionality, add the following code to the confirmDismiss method of the Dismissible widget: This will delete the Todo item when the user swipes the item from right to left. Now if you run the application you should see the following flow. You can terminate the sandbox environment now to clean up the project. Publishing changes to the cloud requires a remote git repository. Amplify offers fullstack branch deployments that allow you to automatically deploy infrastructure and application code changes from feature branches. To learn more, visit the fullstack branch deployments guide . That's it! You have successfully built a fullstack app on AWS Amplify. If you want to learn more about how to work with Amplify, here's the conceptual guide for how Amplify works ."}, {"source": "data/raw_pages/react.txt", "text": "Amplify Documentation for React AWS Amplify is everything you need to build web and mobile apps. Easy to start, easy to scale. You can build a fullstack app using Amplify backend building capabilities, or you can deploy your web app using Amplify Hosting. How Amplify works > Build fullstack apps with your framework of choice You can use AWS Amplify with popular web and mobile frameworks like JavaScript, Flutter, Swift, and React. Build, connect, and host fullstack apps on AWS. Get started by selecting your preferred framework. Features Code-first DX The fullstack TypeScript developer experience lets you focus on your app code instead of infrastructure. Fullstack Git deployments Deploy your frontend and backend together on every code commit. Your Git branch is the source of truth. Faster local development Per-developer cloud sandbox environments let you quickly iterate during development."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_geofences.txt", "text": "Work with geofences First, make sure you've provisioned a geofence collection resource and configured your app using the instructions in either Configure a geofence collection or Use existing Amazon Location Service resources and you have already setup displaying a map in your application. To add a geofence management component to your map, you can use the amplify-geofence-control . Install the necessary dependencies with the following command: Note: Make sure that aws-amplify @aws-amplify/geo version 6.0.0 or above are installed. First, create a map onto which you want to add the geofence management component. See the guide on creating and displaying maps . Then, import AmplifyGeofenceControl from \"maplibre-gl-js-amplify\", create a new instance of this control and add it to your MapLibre map instance. Notes: To use Geofence Controls the user will need to be authenticated with the administrative Cognito user associated with the Geofence Collection you created. Below is an example using React and the Amplify Authenticator . Javascript React Note: When using the existing maps implementation you can add the Geofence control to an existing map Note: Ensure that your package bundler (webpack, rollup, etc) is configured to handle css files. Check out the webpack documentation here . If you are using a different mapping library or need a programmatic approach to managing geofences, the @aws-amplify/geo package provides methods for managing geofences, but not geofence collections. First, you need to import Geo from the @aws-amplify/geo package. saveGeofences is used to save geofences to your collection. It can take a single geofence or an array of geofences. API Parameters geofences - can be a single geofence object, or an array of geofence objects to save to a collection. options - optional options object for saving geofences collectionName - the name of the collection to save geofences to. Defaults to the default collection listed in your amplify_outputs.json file after provisioning a geofence collection resource. Geofence objects must have the following properties: geofenceId - a opaque and unique identifier for the geofence. geometry - a geometry object that defines the geofence. polygon - an array of arrays with [Longitude, Latitude] coordinates. NOTE: Polygon arrays have a few requirements: must have at least 4 vertices (i.e. 4 coordinate points) the first and last point must be the same in order to complete the polygonal loop vertices must be in counter-clockwise order Return The return from saveGeofences is a Promise that resolves to SaveGeofenceResults which contains both successes and errors for geofences that were successfully created or failed. Each success object has the following properties: geofenceId - the geofenceId of the geofence that was saved. createTime - the time the geofence was created. updateTime - the time the geofence was last updated. Each error object has the following properties: geofenceId - the geofenceId of the geofence that failed to be saved. error - an error object Example geoGeofence is used to get a single geofence from a collection. API Parameters geofenceId - the id of the geofence to get. options - optional options object for getting"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_geofences.txt", "text": "a geofence collectionName - the name of the collection to get geofence from. Defaults to the default collection listed in your amplify_outputs.json file after provisioning a geofence collection resource. Return The return from getGeofence is a Promise that resolves to a geofence object. Example listGeofences is used to get a list of geofences from a collection. It has pagination built in and will return 100 geofences per page. API Parameters options - optional options object for saving geofences nextToken - the pagination token for the next page of geofences. if no token is given, it will return the first page of geofences. collectionName - the name of the collection to save geofences to. Defaults to the default collection listed in your amplify_outputs.json file after provisioning a geofence collection resource. Return Returns a Promise that resolves to an object with the following properties: entries - an array of geofences nextToken - the pagination token for the next page of geofences Example deleteGeofences is used to delete a geofences from a collection. It can delete a single or multiple geofences at once. API Parameters geofenceIds - a single geofenceId or array of geofenceIds to delete options - optional options object for saving geofences collectionName - the name of the collection to save geofences to. Defaults to the default collection listed in your amplify_outputs.json file after provisioning a geofence collection resource. Return The return from deleteGeofences is a Promise that resolves to an object with both successes and errors for geofences that were successfully deleted or not. The success object is an array of geofenceIds that were successfully deleted. The error object is an array of error objects that include the following properties: geofenceId - the geofenceId of the geofence that failed to be deleted. error - an error object Example"}, {"source": "data/raw_pages/react-native_start_quickstart.txt", "text": "Quickstart Before you get started, make sure you have the following installed: This Quickstart guide will walk you through how to build a Todo application for Android or iOS using Expo 's TypeScript template. Warning: React Native for Web is not officially supported yet, but we are working towards official support. We are tracking the progress in issue #13918 on GitHub For calling native libraries and platform dependencies, you need to have run the prebuild command for generating the folders for depending platforms. The easiest way to get started with AWS Amplify is through npm with create-amplify command. You can run it from your base project directory. Running this command will scaffold Amplify backend files in your current project with the following files added: To deploy your backend use Amplify's per-developer cloud sandbox. This feature provides a separate backend environment for every developer on a team, ideal for local development and testing. To run your application with a sandbox environment, you can run the following command: The initial scaffolding already has a pre-configured auth backend defined in the amplify/auth/resource .ts file. We've configured it to support email and password login but you can extend it to support a variety of login mechanisms, including Google, Amazon, Sign In With Apple, and Facebook. The fastest way to get your login experience up and running is to use our Authenticator UI component available in the Amplify UI library. To use the Authenticator, you need to add the following dependencies to your project: Then install the iOS cocoapods for targeting iOS by running: Next, update the App.tsx file with the following: The Authenticator component auto-detects your auth backend settings and renders the correct UI state based on the auth backend's authentication flow. Run your application in your local environment again. You should be presented with a login experience now. The initial scaffolding already has a pre-configured data backend defined in the amplify/data/resource.ts file. The default example will create a Todo model with content field. Let's modify this to add the following: A boolean isDone field. An authorization rules specifying owners, authenticated via your Auth resource can \"create\", \"read\", \"update\", and \"delete\" their own records. Update the defaultAuthorizationMode to sign API requests with the user authentication token. Next, let's implement UI to create, list, and delete the to-do items. Create a src folder, and within the folder, create a new file called TodoList.tsx . This page will contain information about creating, reading, updating, and deleting Todo items. Copy and paste the following code into the file: With the code above, you can create a random todo item and display todo items in a list. You can mark them as done, update the list, or revert that operation. You can also delete the items. Each change in the list is listened to with the subscription and immediately shown on the screen. If we take a closer look at the code: generateClient generates the necessary files and folders for models. TodoList component includes the subscription, creation operations,"}, {"source": "data/raw_pages/react-native_start_quickstart.txt", "text": "and a list to hold created items. TodoItem holds the information about each todo item. Lastly, update the App component in App.tsx as follows: If you run the application now, you should see the following behavior: You can terminate the sandbox environment now to clean up the project. Publishing changes to the cloud requires a remote git repository. Amplify offers fullstack branch deployments that allow you to automatically deploy infrastructure and application code changes from feature branches. To learn more, visit the fullstack branch deployments guide . That's it! You have successfully built a fullstack app on AWS Amplify. If you want to learn more about how to work with Amplify, here's the conceptual guide for how Amplify works ."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_interactions_set-up-interactions.txt", "text": "Set up Amplify Interactions AWS Amplify Interactions enables AI-powered chatbots in your web or mobile apps. You can use Interactions to configure your backend chatbot provider and to integrate a chatbot UI into your app with just a single line of code. AWS Amplify supports Amazon Lex as the default chatbots service. Amazon Lex supports creating conversational bots with the same deep learning technologies that power Amazon Alexa. You can create an Amazon Lex V2 chatbot in Amazon Lex console. To create your bot, follow the steps shown in Amazon Lex V2 Developer Guide . Amazon Lex service requires an IAM policy in order to use the interactions APIs ( remember to replace the template with real value ): Add the aws-amplify and interactions package to your project: Make sure that the @aws-amplify/interactions package has the same version number as the aws-amplify package in your package.json file. Import and load the configuration file in your app. It's recommended you add the Amplify configuration step to your app's root entry point. For example, App.js (Expo) or index.js (React Native CLI). Make sure you call Amplify.configure as early as possible in your application\u00e2\u0080\u0099s life-cycle. A missing configuration or NoCredentials error is thrown if Amplify.configure has not been called before other Amplify JavaScript APIs. Review the Library Not Configured Troubleshooting guide for possible causes of this issue."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_pubsub.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_start_manual-installation.txt", "text": "Manual installation To get started with AWS Amplify we recommend that you use our quickstart starter template. However, for some use cases, it may be preferable to start from scratch, either with a brand new directory or an existing frontend app. In that case we recommend to use npm with create-amplify . Running this command will scaffold a lightweight Amplify project in your current project with the following files: If needed, you can manually install AWS Amplify without using create-amplify or the starter template. This guide will walk you through how to initialize your project, install dependencies, and author your first backend. First, if your frontend framework of choice doesn't have it already, create your project's package.json with npm init -y . Then, install the Amplify dependencies for building a backend: Note : TypeScript is not a requirement but is recommended for an optimal experience. Next, create the entry point for your backend, amplify/backend.ts , with the following code: Now you can run npx ampx sandbox to create your first backend! Amplify Gen 2 requires your backend to be configured for use with ECMAScript modules (ESM) . If you encounter the following error during ampx sandbox , consider modifying your package.json with \"type\": \"module\" : Or, you can create a local file in the Amplify backend directory, amplify/package.json : You can use define* functions to define your resources. For example, you can define authentication: Or define your data resource: Each of these newly defined resources are then imported and set in the backend definition: You can also update an existing frontend app. To upgrade existing Amplify code-first DX (Gen 2) apps, use your Node.js package manager (for example, npm ) to update relevant backend packages: We recommend the following next steps:"}, {"source": "data/raw_pages/react_start_quickstart.txt", "text": "Quickstart \u00f0\u009f\u0091\u008b Welcome to AWS Amplify! In this quickstart guide, you will: Deploy a React and Vite app Build and connect to a database with real-time data updates Configure authentication and authorization rules We've created a starter \"To-do\" application to help get started faster. First, you will create a repository in your GitHub account using our starter React template. Use our starter template to create a repository in your GitHub account. This template scaffolds create-vite-app with Amplify backend capabilities. Create repository from template Use the form in GitHub to finalize your repo's creation. Now that the repository has been created, deploy it with Amplify. Deploy to AWS Select GitHub . After you give Amplify access to your GitHub account via the popup window, pick the repository and main branch to deploy. Make no other changes and click through the flow to Save and deploy . While you are waiting for your app to deploy (~5 mins) Learn about the project structure Let's take a tour of the project structure in this starter repository by opening it on GitHub. The starter application has pre-written code for a to-do list app. It gives you a real-time database with a feed of all to-do list items and the ability to add new items. Close accordion When the build completes, visit the newly deployed branch by selecting \"Visit deployed URL\". Since the build deployed an API, database, and authentication backend, you will be able to create new to-do items. In the Amplify console, click into the deployment branch (in this case main ) > select Data in the left-hand menu > Data manager to see the data entered in your database. Let's learn how to enhance the app functionality by creating a delete flow for to-do list items. Now let's set up our local development environment to add features to the frontend. Click on your deployed branch and you will land on the Deployments page which shows you your build history and a list of deployed backend resources. At the bottom of the page you will see a tab for Deployed backend resources . Click on the tab and then click the Download amplify_outputs.json file button. Clone the repository locally. Now move the amplify_outputs.json file you downloaded above to the root of your project. The amplify_outputs.json file contains backend endpoint information, publicly-viewable API keys, authentication flow information, and more. The Amplify client library uses this outputs file to connect to your Amplify Backend. You can review how the outputs file is imported within the main.tsx file and then passed into the Amplify.configure(...) function of the Amplify client library. Close accordion Go to the src/App.tsx file and add in a new deleteTodo functionality and pass function into the <li> element's onClick handler. Try out the deletion functionality now by starting the local dev server: This should start a local dev server at http://localhost:5173 . The starter application already has a pre-configured auth backend defined in the amplify/auth/resource.ts file. We've configured it to support email and"}, {"source": "data/raw_pages/react_start_quickstart.txt", "text": "password login but you can extend it to support a variety of login mechanisms, including Google, Amazon, Sign In With Apple, and Facebook. The fastest way to get your login experience up and running is to use our Authenticator UI component. In your src/main.tsx file, import the Authenticator UI component and wrap your <App> component. The Authenticator component auto-detects your auth backend settings and renders the correct UI state based on the auth backend's authentication flow. In your src/App.tsx file, add a button to enable users to sign out of the application. Import the useAuthenticator hook from the Amplify UI library to hook into the state of the Authenticator. Try out your application in your localhost environment again. You should be presented with a login experience now. To get these changes to the cloud, commit them to git and push the changes upstream. Amplify automatically deploys the latest version of your app based on your git commits. In just a few minutes, when the application rebuilds, the hosted app will be updated to support the deletion functionality. Let's update our backend to implement per-user authorization rules, allowing each user to only access their own to-dos. To make backend updates, we are going to require AWS credentials to deploy backend updates from our local machine. Skip ahead to step 8 , if you already have an AWS profile with credentials on your local machine, and your AWS profile has the AmplifyBackendDeployFullAccess permission policy. Otherwise, set up local AWS credentials that grant Amplify permissions to deploy backend updates from your local machine. To update your backend without affecting the production branch, use Amplify's cloud sandbox. This feature provides a separate backend environment for each developer on a team, ideal for local development and testing. To start your cloud sandbox, run the following command in a new Terminal window : Once the cloud sandbox has been fully deployed (~5 min), you'll see the amplify_outputs.json file updated with connection information to a new isolated authentication and data backend. The npx ampx sandbox command should run concurrently to your npm run dev . You can think of the cloud sandbox as the \"localhost-equivalent for your app backend\". The to-do items in the starter are currently shared across all users, but, in most cases, you want data to be isolated on a per-user basis. To isolate the data on a per-user basis, you can use an \"owner-based authorization rule\". Let's apply the owner-based authorization rule to your to-do items: In the application client code, let's also render the username to distinguish different users once they're logged in. Go to your src/App.tsx file and render the user property from the useAuthenticator hook. Now, let's go back to your local application and test out the user isolation of the to-do items. You will need to sign up new users again because now you're working with the cloud sandbox instead of your production backend. To get these changes to the cloud, commit them to git and push the changes"}, {"source": "data/raw_pages/react_start_quickstart.txt", "text": "upstream. Once your build completes in the Amplify Console, the main backend will update to support the changes made within the cloud sandbox. The data in the cloud sandbox is fully isolated and won't pollute your production database. That's it! You have successfully built a fullstack app on AWS Amplify. If you want to learn more about how to work with Amplify, here's the conceptual guide for how Amplify works ."}, {"source": "data/raw_pages/react_build-a-backend_functions_examples_custom-message.txt", "text": "Custom message You can use defineAuth and defineFunction to create an Amazon Cognito custom message AWS Lambda trigger thats sends an custom email or phone verification message, or a multi-factor authentication (MFA) code. To get started, install @types/aws-lambda package that will be used to define the type of the handler: Next, create a new directory and a resource file, amplify/auth/custom-message/resource.ts . Then, define the function with defineFunction : Next, create the corresponding handler file, amplify/auth/custom-message/handler.ts , file with the following contents: The input event for the CustomMessage_AdminCreateUser trigger source includes both a username and verification code. Admin-created users must receive both their username and code in order to sign in and thus you must include both the usernameParameter and codeParameter in your message template. Lastly, set the newly created function resource on your auth resource: After deploying the changes, whenever a user with user attribute locale set to es attempts to reset a password they will receive an email with a one-time code in Spanish."}, {"source": "data/raw_pages/react_build-a-backend_functions_streaming-logs.txt", "text": "Streaming logs Amplify enables you to stream logs from your AWS Lambda functions directly to your terminal while running ampx sandbox . To get started, specify the --stream-function-logs option when starting sandbox: Note : this feature is only available for Sandbox Streaming function logs directly to your terminal enable faster debug iterations, and greater insight into your functions' executions. By default, Amplify will stream all of your functions' logs. If you wish to only stream a subset of functions you can specify a filter by function name or a regular expression for function names. For example, if you have a collection of Auth triggers where the function names include \"auth\". When working with more than 5 functions, we recommend using the --logs-filter flag to filter the log output to specific functions. After you successfully deploy your personal cloud sandbox, start your frontend application, and sign up for the first time, you will see logs from your triggers' executions printed to the terminal where sandbox is running. By default, Amplify will print logs to the terminal where sandbox is running, however you can alternatively write logs to a file by specifying --logs-out-file : This can be combined with --logs-filter to create a log file of just your Auth-related functions, for example: However it cannot be combined multiple times to write logs to multiple files."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_predictions.txt", "text": "AI/ML Predictions Amplify provides provides a solution for using AI and ML cloud services to enhance your application. Some supported use cases: Predictions is broadly organized into 3 key use cases - Identify, Convert, and Interpret - which are available in the client API as well as CLI workflows. Identify will find text (words, tables, pages from a book), entities (faces and/or celebrities) from images. You can also identify real world landmarks or objects such as chairs, desks, etc. which are referred to as \u00e2\u0080\u009clabels\u00e2\u0080\u009d from images. Convert allows you to translate text from one source language to a target language. You can also generate speech audio from text input. Lastly, you can take an audio input and transcribe it using a websocket stream. Interpret allows you to analyze text for language, entities (places, people), key phrases, sentiment (positive, neutral, negative), and syntax (pronouns, verbs, adjectives). Some common use cases are listed below, as well as an advanced workflow which allows you to perform dynamic image indexing from a connected s3 bucket. Predictions comes with built-in support for Amazon Translate , Amazon Polly , Amazon Transcribe , Amazon Rekognition , Amazon Textract , and Amazon Comprehend ."}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_listen-to-auth-events.txt", "text": "Listen to auth events Amplify Auth emits events during authentication flows, which enables you to react to user flows in real time and trigger custom business logic. For example, you may want to capture data, synchronize your app's state, and personalize the user's experience. You can listen to and respond to events across the Auth lifecycle such as sign-in and sign-out. You can use Amplify Hub with its built in Amplify Auth events to subscribe a listener using a publish-subscribe pattern and capture events between different parts of your application. The Amplify Auth category publishes in the auth channel when auth events such as signedIn or signedOut happen independent from your app code. You can review the Amplify Hub guide to learn more . Channels are logical group names that help you organize dispatching and listening. However, some channels are protected and cannot be used to publish custom events, and auth is one of these channels. Sending unexpected payloads to protected channels can have undesirable side effects such as impacting authentication flows. See the Amplify Hub guide for more protected channels. Here is a basic example of setting up a listener that logs an event emitted through the auth channel: Once your app is set up to subscribe and listen to specific event types from the auth channel, the listeners will be notified asynchronously when an event occurs. This pattern allows for a one-to-many relationship where one auth event can be shared with many different listeners that have been subscribed. This lets your app react based on the event rather than proactively poll for information. Additionally, you can set up your listener to extract data from the event payload and execute a callback that you define. For example, you might update UI elements in your app to reflect your user's authenticated state after the signedIn or signedOut events. One of the most common workflows will be to log events. In this example you can see how you can listen and target specific auth events using a switch to log your own messages. You can also stop listening for messages by calling the result of the Hub.listen() function. This may be useful if you no longer need to receive messages in your application flow. This can also help you avoid any memory leaks on low powered devices when you are sending large amounts of data through Amplify Hub on multiple channels. To stop listening to a certain event, you need to wrap the listener function with a variable and call it once you no longer need it: You now have a few use cases and examples for listening to and responding to auth events."}, {"source": "data/raw_pages/react_deploy-and-host.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_reference_cdk-constructs.txt", "text": "CDK constructs Constructs\u00e2\u0080\u0094the basic building blocks of AWS Cloud Development Kit (AWS CDK) apps\u00e2\u0080\u0094abstract away the complexity of configuring cloud resources, so you can concentrate on your application code. In the following sections, we summarize the available Amplify backend constructs. The official AmplifyData construct can be found on Construct Hub . This package provides a Level 3 (L3) CDK construct wrapping the behavior of the Amplify GraphQL API. This enables quick development and iteration of AppSync APIs that support the Amplify GraphQL directives. For more information on data modeling, visit the data-modeling documentation . The official AmplifyAuth construct can be found on the npm registry ."}, {"source": "data/raw_pages/react_build-a-backend_functions_examples_create-user-profile-record.txt", "text": "Create a user profile record You can use defineAuth and defineFunction to create a Cognito post confirmation Lambda trigger to create a profile record when a user is confirmed. A user is \"confirmed\" when they verify their account. Typically this happens when the user confirms their email via the verification email. The post confirmation handler will not be triggered for federated sign-ins (i.e. social sign-in). To get started, install the aws-lambda package, which is used to define the handler type. Update the amplify/data/resource.ts file to define a data model for the user's profile: Create a new directory and a resource file, amplify/auth/post-confirmation/resource.ts . Then, define the Function with defineFunction : Then, create the corresponding handler file, amplify/auth/post-confirmation/handler.ts , file with the following contents: When configuring Amplify with getAmplifyDataClientConfig , your function consumes schema information from an S3 bucket created during backend deployment with grants for the access your function need to use it. Any changes to this bucket outside of backend deployment may break your function. Lastly, set the newly created Function resource on your auth resource: After deploying the changes, whenever a user signs up and verifies their account a profile record is automatically created."}, {"source": "data/raw_pages/react_build-a-backend_functions_custom-functions.txt", "text": "Custom functions AWS Amplify Gen 2 functions are AWS Lambda functions that can be used to perform tasks and customize workflows in your Amplify app. Functions can be written in Node.js, Python, Go, or any other language supported by AWS Lambda . Note: The following options in defineFunction are not supported for Custom Functions: You'll need to configure these options directly in your CDK Function definition instead. However, resourceGroupName property is supported and can be used to group related resources together in your defineFunction definition. In this guide, you will learn how to create Python and Go functions with Amplify functions. The examples shown in this guide do not use Docker to build functions. Instead, the examples use commands that run on your host system to build, and as such require the necessary tooling for the language you are using for your functions. To get started, create a new directory and a resource file, amplify/functions/say-hello/resource.ts . Then, define the function with defineFunction : Next, create the corresponding handler file at amplify/functions/say-hello/index.py . This is where your function code will go. The handler file must export a function named \"handler\". This is the entry point to your function. For more information on writing functions, refer to the AWS documentation for Lambda function handlers using Python . If you need Python packages, you can add them to a requirements.txt file in the same directory as your handler file. The bundling option in the Code.fromAsset method will install these packages for you. Create a requirements.txt file in the same directory as your handler file. This file should contain the names of the packages you want to install. For example: You're now ready to deploy your python function. Next is the same process as the Node.js/TypeScript function. Go to Common steps for all languages to continue. To get started, Create a new directory and a resource file, amplify/functions/say-hello/resource.ts . Then, define the function with defineFunction : Next, create the corresponding handler file at amplify/functions/say-hello/main.go . This is where your function code will go. Then you should run the following command to build the go function: then run to install the dependencies. You're now ready to deploy your golang function. Next is the same process as the Node.js/TypeScript function. Regardless of the language used, your function needs to be added to your backend. Now when you run npx ampx sandbox or deploy your app on Amplify, it will include your function. To invoke your function, we recommend adding your function as a handler for a custom query with your Amplify Data resource . To get started, open your amplify/data/resource.ts file and specify a new query in your schema: Custom function may require Docker in order to build and bundle function's code. A deployment failing with CustomFunctionProviderDockerError error indicates that a custom function requires Docker but the Docker daemon was not found. In that case you need to provide a working Docker installation at runtime. Ensure that Docker is installed on your computer and that Docker"}, {"source": "data/raw_pages/react_build-a-backend_functions_custom-functions.txt", "text": "daemon is running. You can check if Docker daemon is running using the following command: Amplify does not provide Docker daemon out of the box in branch deployments. However, you have an option to provide your own image that meets Amplify requirements and includes a Docker installation. For example, the aws/codebuild/amazonlinux-x86_64-standard:5.0 image ( see definition ) meets Amplify requirements and includes Docker installation."}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_google-migration.txt", "text": "Migrate from Google Maps Are you using Google Maps or another similar Map Provider and would like to switch over to using Amplify Geo or Amazon Location Service? This tutorial will show you how to take your existing Google Maps APIs and switch over to using Amplify Geo. Amplify Geo provides APIs for using location based functionality. Under the hood Amplify uses Amazon Location Service and is designed to work with open source mapping library MapLibre . This guide assumes that you are already familiar with the Google Maps JavaScript API and with front-end web development concepts including HTML, CSS, and JavaScript. To complete this tutorial, you will need: Amplify Geo A text editor A key difference to notice between using Amplify Geo and Google Maps is with Google Maps Platform their convention for specifying coordinates is [lat, lng] . When migrating over to Amplify Geo the order is swapped to be [lng, lat] . This was done to match the geojson spec which is also used by MapLibre. When using Google Maps Platform or other similar services like Mapbox you will first be prompted to go to the Google Cloud Console to set up APIs and create an API key where you will then use the API key when requesting the Google Maps JS API. With Amplify Geo you will instead setup Amplify Auth and the MapView component will read the auth configuration from the amplify_outputs.json file. Behind the scenes Amplify Auth uses Amazon Cognito to set up client credentials with access to Location Service and Geo will use those credentials when making any location related API calls. More information on setting Amplify Auth and Geo can be found below in the Setting Up Amplify section. Open your text editor and create a new file called index.html . Paste the following code into the file to set up the framework for a webpage with a map. This code imports the MapLibre GL JS library and CSS, one of the popular options for map rendering we recommend for use with Amplify Geo. In the HTML body you create a <div> element with an id of 'map' that will be the map's container. Finally in the script section you'll setup some Amplify configuration that is required for Amplify Geo to understand what Amplify AWS resources have been created. You will need to setup a Geo Map resources . Follow instructions for creating a map. Once the workflow has completed you should have an amplify_outputs.json file in the same directory as your index.html file. Save your index.html file. In this step we will show you how to add code to display a map in your application. Amplify Google Maps With Amplify Geo and MapLibre you can add the following code to your index.html file inside the <script> tags, after the Amplify.configure command: Save your HTML file and open it in a web browser to see your rendered map. With the Google Maps JS API you can display a map like so. Here"}, {"source": "data/raw_pages/react_build-a-backend_add-aws-services_geo_google-migration.txt", "text": "you will add a marker to your map Amplify Google Maps With Amplify Geo and MapLibre you can do the following. Save your changes and refresh your page and you should see a default blue marker icon on your map. Using the Google Maps JS API you would add a marker as show below. Now you can add a popup that displays information when a user clicks on a marker. Amplify Google Maps With Amplify Geo and MapLibre you can do the following. Save your changes and refresh your page and now when you click on the icon a popup should appear on the screen. Using the Google Maps JS API you would add a marker as shown below. Now we can try adding a search bar to your map which can return results and place markers on a map based on those results. Amplify Google Maps With Amplify Geo and MapLibre you can do the following. Save your changes and refresh your page and now when you should see a maplibre-gl-geocoder control in the top right corner of your map. This example uses the MapLibre's geocoder component to create a search component. To see more options for our createAmplifyGeocoder utility function check out the docs here . Using the Google Places JS API you would add a search bar as shown below. Now we can try adding a search bar without adding it to a map which can return results that you can use. Amplify Google Maps With Amplify Geo and MapLibre you can do the following. Save your changes and refresh your page and now when you should see a maplibre-gl-geocoder control in the div you created. This example uses the MapLibre's geocoder component to create a search component. To see more options for our createAmplifyGeocoder utility function check out the docs here . Using the Google Places JS API you would add a stand alone search bar as shown below. Some lines omitted for brevity, see the Google Maps Platform Places Search Box example for the full application"}, {"source": "data/raw_pages/nextjs_start_quickstart.txt", "text": "Quickstart \u00f0\u009f\u0091\u008b Welcome to AWS Amplify! In this quickstart guide, you will: Deploy a Next.js app Build and connect to a database with real-time data updates Configure authentication and authorization rules We have two Quickstart guides you can follow:"}, {"source": "data/raw_pages/react_build-a-backend_auth_examples.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_build-a-backend_auth_manage-users.txt", "text": "Fullstack TypeScript Write your app's data model, auth, storage, and functions in TypeScript; Amplify will do the rest."}, {"source": "data/raw_pages/react_ai_generation.txt", "text": "Generation AI generation routes are a request-response API used to generate structured output from AI models. Examples of generation routes include: generated structured data from unstructured input summarization Under the hood, a generation route is an AWS AppSync query that ensures the AI model responds with the response type defined for the route. You can influence response generation by setting inference parameters for the AI model. This ability to control the randomness and diversity of responses is useful for generating responses that are tailored to your needs. More information about inference parameters . For example, the following schema defines a Recipe model, but this model cannot be used as the return type of a generation route. You can, however, reference custom types. Here's an example of a custom type that can be used as the return type of a generation route. The following AppSync scalar types are not supported as required fields in response types: AWSEmail AWSDate AWSTime AWSDateTime AWSTimestamp AWSPhone AWSURL AWSIPAddress"}, {"source": "data/raw_pages/react_build-a-backend_storage_upload-files.txt", "text": "Upload files You can implement upload functionality in your app by either using the File Uploader UI component or further customizing the upload experience using the upload API. Upload files from your app in minutes by using the cloud-connected File Uploader UI Component. Then, use the component in your app. Learn more about how you can further customize the UI component by referring to the File Uploader documentation . The following is an example of how you would upload a file from a file object, this could be retrieved from the local machine or a different source. You can follow this example if you have data saved in memory and would like to upload this data to the cloud. You can also perform an upload operation to a specific bucket by providing the bucket option. You can pass in a string representing the target bucket's assigned name in Amplify Backend. import { uploadData } from 'aws-amplify/storage' ; const result = await uploadData ( { path : 'album/2024/1.jpg' , Copy highlighted code example bucket : 'assignedNameInAmplifyBackend' Alternatively, you can also pass in an object by specifying the bucket name and region from the console. import { uploadData } from 'aws-amplify/storage' ; const result = await uploadData ( { path : 'album/2024/1.jpg' , Copy highlighted code example bucketName : 'bucket-name-from-console' , Monitor progress of upload by using the onProgress option. We have callback functions that support resuming, pausing, and cancelling uploadData requests. Custom metadata can be associated with your uploaded object by passing the metadata option. The behavior of uploadData and properties of the uploaded object can be customized by passing in additional options. Uploads that were initiated over one hour ago will be cancelled automatically. There are instances (e.g. device went offline, user logs out) where the incomplete file remains in your Amazon S3 account. It is recommended to setup a S3 lifecycle rule to automatically cleanup incomplete upload requests. Amplify will automatically perform an Amazon S3 multipart upload for objects that are larger than 5MB. For more information about S3's multipart upload, see Uploading and copying objects using multipart upload"}, {"source": "data/raw_pages/react_build-a-backend_auth_connect-your-frontend_manage-user-sessions.txt", "text": "Manage user sessions Amplify Auth provides access to current user sessions and tokens to help you retrieve your user's information to determine if they are signed in with a valid session and control their access to your app. You can use the getCurrentUser API to get information about the currently authenticated user including the username , userId and signInDetails . This method can be used to check if a user is signed in. It throws an error if the user is not authenticated. The user's signInDetails are not supported when using the Hosted UI or the signInWithRedirect API. Your user's session is their signed-in state, which grants them access to your app. When your users sign in, their credentials are exchanged for temporary access tokens. You can get session details to access these tokens and use this information to validate user access or perform actions unique to that user. If you only need the session details, you can use the fetchAuthSession API which returns a tokens object containing the JSON Web Tokens (JWT). The fetchAuthSession API automatically refreshes the user's session when the authentication tokens have expired and a valid refreshToken is present. Additionally, you can also refresh the session explicitly by calling the fetchAuthSession API with the forceRefresh flag enabled. Warning: by default, sessions from external identity providers cannot be refreshed."}]